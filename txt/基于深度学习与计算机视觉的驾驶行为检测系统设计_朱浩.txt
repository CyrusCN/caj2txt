 单位代码：  10293   密  级：              专 业 学 位 硕 士 论 文        论文题目：     基于深度学习与计算机视觉的                            驾驶行为检测系统设计                                1220045430                                   朱浩                                      沈澍                                  电子信息硕士          申请                   全日制             申请                 计算机技术                    2023年6月             学号 姓名 导师 专业学位类别 类型 专业（领域） 论文提交日期    Design of driving behavior detection system based on deep learning and computer vision      Thesis Submitted to Nanjing University of Posts and Telecommunications for the Degree of  Master of Engineering    By Hao Zhu Supervisor: Prof. Shu Shen June  2023      I 摘要 我国的汽车保有量已跃居世界第一，汽车驾驶人数增长十分迅速，而根据世界卫生组织于2020年的道路交通事故报告显示，全球每年约有130万人死于交通事故，道路交通事故造成的经济损失约占其生产总值的3%，而驾驶行为分心驾驶和疲劳驾驶造成的交通事故已经超过了全部事故的35%。因此针对分心驾驶和疲劳驾驶的识别检测工作十分必要，本文基于多视图学习和多任务学习建立了多视图分心驾驶行为识别模型和多任务分心驾驶行为和疲劳驾驶特征联合识别模型，并基于疲劳驾驶的疲劳特征确认了疲劳驾驶判定策略，以此实现驾驶行为检测系统，为提高我国道路交通安全贡献自己的一份力量。本文主要工作如下： 首先，本文建立了多视角驾驶行为图像数据集NMDA数据集并提出了多视图分心驾驶行为识别模型MMobNet。基于分心驾驶行为识别和疲劳驾驶行为识别两种任务的需求视角不同建立了多视角驾驶行为图像数据集NMDA数据集，并进行了数据集有效性实验，确认了NMDA数据集的有效性和可靠性。以多视图模型MVCNN为模型基础框架，并提出视图注意力机制VAM模块，建立了多视图分心驾驶行为识别模型MMobNet。通过实验后，确认了MMobNet模型对单任务模型的优越性，准确率高出10%以上，同时使用了VAM模块的MMobeNet模型较同类型多视图模型准确率高1.02%，并进行MMobNet的视角组合对比确认视角1、2、3的组合可以在略微降低模型准确率的同时大幅降低数据需求规模。 然后，本文确认了疲劳驾驶行为识别方案并提出了多任务分心驾驶行为和疲劳特征联合识别多任务模型MTDFNet。在疲劳驾驶行为识别方案中使用RetinaFace作为人脸检测器，采集人脸区域图像分为上下眼部和嘴部图像建立疲劳特征数据集EMFD，建立疲劳特征识别模型MEMNet。以分心驾驶行为识别模型MMobNet和疲劳特征识别模型MEMNet为基础结构，利用多任务学习建立多任务模型MTDFNet。通过实验，确认了训练MTDFNet模型使用的损失函数和loss组合算法，得到了较为优秀的模型准确率，分心驾驶行为识别和疲劳特征识别两个子任务的准确率分别为91.22%和93.30%，较与对应的单任务模型，疲劳特征识别任务虽然降低了0.47%，但更为困难的分心驾驶行为识别任务准确率提高了3.7%，证明了多任务学习可以提高部分子任务性能的特点。 最后，本文完成了驾驶行为检测系统的设计与实现工作。基于疲劳特征识别模型MEMNet确认了疲劳驾驶的疲劳特征判定策略并进行了测试，得到了关键参数闭眼帧数占比上限和张嘴时间占比上限。使用Pyqt5作为系统设计框架，建立了可视化界面和包含界面的驾驶行为检测系统并进行了系统测试，结果显示，该系统能够有效地运行。 关键词:  多视角驾驶行为图像数据集，多视图学习，深度神经网络 ，多任务学习   II Abstract China’s automobile ownership has jumped to the world’s first place, and the number of drivers has grown rapidly. However, according to the World Health Organization’s 2020 report on road traffic accidents, about 1.3 million people die in traffic accidents worldwide each year, and the economic losses caused by road traffic accidents account for about 3% of their gross domestic product. Distracted driving and fatigued driving have caused more than 35% of all accidents. Therefore, it is necessary to identify and detect distracted driving and fatigued driving. This paper establishes a multi-view distracted driving behavior recognition model and a multi-task distracted driving behavior and fatigue driving feature joint recognition model based on multi-view learning and multi-task learning. Based on the fatigue characteristics of fatigue driving, this paper confirms the fatigue driving judgment strategy to realize the driving behavior detection system and contribute to improving China’s road traffic safety. Firstly, this paper establishes a multi-view driving behavior image dataset NMDA dataset and proposes a multi-view distracted driving behavior recognition model MMobNet. Based on the different requirements of the two tasks of distracted driving behavior recognition and fatigue driving behavior recognition, a multi-view driving behavior image dataset NMDA dataset was established from different perspectives, and the effectiveness of the dataset was experimentally confirmed. Based on the multi-view model MVCNN as the model basic framework and the proposed view attention mechanism VAM module, a multi-view distracted driving behavior recognition model MMobNet was established. After experiments, it was confirmed that the MMobNet model has superior performance to single-task models, with an accuracy rate of more than 10% higher. The MMobeNet model using the VAM module has an accuracy rate 1.02% higher than that of other similar multi-view models. The combination of views 1, 2, and 3 in MMobNet’s view combination comparison can greatly reduce the data demand scale while slightly reducing the model accuracy. Secondly, this paper confirms the fatigue driving behavior recognition scheme and proposes a multi-task distracted driving behavior and fatigue feature joint recognition multi-task model MTDFNet. In the fatigue driving behavior recognition scheme, RetinaFace is used as the face detector to collect face area images divided into upper and lower eye and mouth images to establish a fatigue feature dataset EMFD and establish a fatigue feature recognition model MEMNet. Based  III on the distracted driving behavior recognition model MMobNet and the fatigue feature recognition model MEMNet as the basic structure, a multi-task model MTDFNet is established using multi-task learning. Through experiments, it was confirmed that the loss function and loss combination algorithm used to train the MTDFNet model obtained excellent model accuracy. The accuracy rates of the two sub-tasks of distracted driving behavior recognition and fatigue feature recognition were 91.22% and 93.30%, respectively. Compared with the corresponding single-task models, although the accuracy rate of the more difficult distracted driving behavior recognition task increased by 3.7%, the accuracy rate of the fatigue feature recognition task decreased by 0.47%, which proves that multi-task learning can improve the performance of some sub-tasks. Finally, this paper completed the design and implementation of the driving behavior detection system. Based on the fatigue feature recognition model MEMNet, the fatigue feature determination strategy for fatigue driving was confirmed and tested, and the upper limit of the number of frames with closed eyes and the upper limit of the time with open mouth were obtained as key parameters. Using Pyqt5 as the system design framework, a visual interface and a driving behavior detection system containing the interface were established and system testing was performed. The results showed that the system can run effectively. Key words: multi-view driving behavior image dataset，multi-view learning，deep neural network，multi-task learning 目  录 第一章 绪论............................................................................................... 1 1.1 研究背景及意义 ............................................................................... 1 1.2 国内外现状 ....................................................................................... 2 1.2.1 驾驶行为评判标准研究 ............................................................. 2 1.2.2 分心驾驶行为识别与疲劳驾驶行为识别的研究 .................... 3 1.3 论文主要研究及结构安排 .............................................................. 5 第二章 相关背景知识介绍 ...................................................................... 7 2.1 深度学习相关知识 ........................................................................... 7 2.1.1 迁移学习 ..................................................................................... 7 2.1.2 卷积神经网络 ............................................................................. 7 2.1.3 注意力机制 ............................................................................... 12 2.2 多视图学习相关介绍 ..................................................................... 13 2.3 多任务学习相关介绍 ..................................................................... 14 2.4 本章小结 ......................................................................................... 17 第三章 基于多视图学习的分心驾驶行为识别研究 ............................ 18 3.1 分心驾驶行为图像数据集 ............................................................ 18 3.1.1 公开数据集 ............................................................................... 18 3.1.2 NJUPT多视角驾驶行为图像数据集....................................... 19 3.1.3 NMDA数据集有效性证明 ...................................................... 23 3.2 多视图模型设计研究 ..................................................................... 25 3.2.1 特征提取模块 ........................................................................... 26 3.2.2 特征融合模块 ........................................................................... 28 3.2.3 分类模块 ................................................................................... 29 3.3 模型训练 ......................................................................................... 30 3.3.1 数据预处理 ............................................................................... 30 3.3.2 参数设置及相关函数选择 ....................................................... 32 3.4 比较与讨论 ..................................................................................... 34 3.4.1 MMobNet模型与单视图模型对比 .......................................... 34 3.4.2 MMobNet模型与多视图模型对比 .......................................... 35 3.4.3 MMobNet模型视角选择对比 .................................................. 35 3.5 本章小结 ......................................................................................... 36 第四章 基于多任务学习的联合驾驶行为识别研究 ............................ 38 4.1 疲劳驾驶行为识别方案研究 ........................................................ 38 4.1.1 疲劳驾驶行为图像数据集 ....................................................... 38 4.1.2 人脸检测算法研究 ................................................................... 39 4.1.3 疲劳特征识别研究 ................................................................... 44 4.2 多任务模型设计研究 ..................................................................... 46 4.2.1 模型结构 ................................................................................... 46 4.2.2 模型训练参数处理 ................................................................... 47 4.3 比较与讨论 ..................................................................................... 49 4.3.1 损失函数选择对比 ................................................................... 49 4.3.2 损失函数权重对比 ................................................................... 50 4.3.3 多任务模型与单任务模型对比 ............................................... 51 4.4 本章小结 ......................................................................................... 52 第五章 驾驶行为检测系统设计与实现 ................................................ 53 5.1 系统结构 ......................................................................................... 53 5.2 疲劳驾驶行为检测方案................................................................. 53 5.2.1 眼睛疲劳特征判定策略 ........................................................... 54 5.2.2 嘴巴疲劳特征判定策略 ........................................................... 55 5.2.3 疲劳驾驶检测测试 ................................................................... 55 5.3 系统实现与测试 ............................................................................. 56 5.4 本章小结 ......................................................................................... 57 第六章 总结与展望 ................................................................................ 58 6.1 总结 ................................................................................................. 58 6.2 展望 ................................................................................................. 59 参考文献 ................................................................................................... 60 南京邮电大学专业学位硕士研究生学位论文 第一章 绪论  1 第一章 绪论  研究背景及意义 现阶段，我国已建成全球规模最大、品类最全、配套完整的汽车工业体系[1]，与此相应的是我国的汽车保有量已跃居世界第一，汽车驾驶人数增长也十分迅速。而根据世界卫生组织（World Health Organization，WHO）于2020年的道路交通事故报告[2]显示，全球每年约有130万人死于交通事故，在大多数国家中，道路交通事故造成的经济损失约占其生产总值的3%，因此在交通道路上汽车行驶愈发增多的现实情况下如何尽量减少交通安全事故，避免伤亡损失与经济损失俨然成为我国社会发展亟需解决的问题。 在造成道路交通事故发生的因素中，人为因素占据了绝大部分，即驾驶员的驾驶行为极大地影响了汽车的行驶安全。即使目前世界许多大企业与机构都在大力发展自动驾驶技术，但在完全实现道路全自动驾驶之前，通过构建一种检测系统来检测驾驶员的非正常驾驶行为并及时修正行为已然成了一种必然的趋势。目前针对辅助驾驶员驾驶的系统有其专有名词，即高级驾驶辅助系统[3]（Advanced Driver Assistance System，ADAS），该系统涉及汽车多种传感设备、通信设备等，通过车载计算机进行实时计算模拟，辅助驾驶员安全驾驶，有效提高汽车行驶安全。 驾驶行为检测实际上属于人体行为识别，不过区别于常规的人体行为识别，驾驶行为识别拥有三个特点，一是场景单一，大多受限于一辆汽车内；二是识别人数有限，正常只包含驾驶员一人；三是驾驶行为与车辆状态紧密相关，可以通过检测车辆状态信息来反映驾驶行为，从而有效识别。因此目前在ADAS中，检测驾驶员的驾驶行为是否安全有三种途径[4]，包括生理学信息、行为学信息以及基于车辆自身的信息，这些信息都通过对应的传感设备来感知并收集， 利用这些信息单独或组合起来进行数据处理与分析最终实现驾驶行为识别。 驾驶行为包含正常的驾驶与非正常的驾驶，非正常的驾驶在剔除汽车本身的问题外，主要分为三大类[5]，分心驾驶、疲劳驾驶和危险驾驶。分心驾驶就是指驾驶员无意识地做出一些影响驾驶注意力集中的行为[6]，如收听电话、触摸中控屏幕、与乘客交流等，即驾驶员不认为这些行为会影响驾驶安全，但客观上驾驶员的注意力没有集中在驾驶上。疲劳驾驶相比于分心驾驶，则更加容易判别出来，由于汽车驾驶[7]既是体力劳动，又是脑力劳动，身心负荷大，非常容易在驾驶过程中，尤其是长途驾驶中产生困倦，造成疲劳驾驶。危险驾驶相较于前两者，驾驶员自身的素质占据了很大原因，例如故意频繁别车，陡然刹车等，常说的路南京邮电大学专业学位硕士研究生学位论文 第一章 绪论  2 怒症就属于危险驾驶行为，驾驶员有意识地做出一些伤害他人生命财产的驾驶行为，这种行为已经在中国刑法中列作一种刑事犯罪。 分心驾驶和疲劳驾驶造成的交通事故已经超过了全部事故的35%[8]，且由于是驾驶员的非故意行为，可以通过汽车内部的检测系统及时检测出来并提醒驾驶员修正行为或停止驾驶进行休息，所以针对分心驾驶和疲劳驾驶的识别工作在学术界已经发展了很多年，部分研究已经进入了实际商用环节。而危险驾驶由于属于驾驶员的主观行为[9]，辅助检测系统很难进行修正,这种行为更多是需要社会中的宣传与教育工作。 基于上述分析，本文针对分心驾驶和疲劳驾驶两种行为的识别工作做了一部分的研究，针对分心驾驶行为识别和疲劳驾驶行为识别两种任务的需求视角不同建立多视角驾驶行为图像数据集，并基于此数据集建立多视图分心驾驶行为识别模型，同时结合疲劳驾驶的疲劳特征识别模型提出一种基于多任务学习机制的联合驾驶行为识别模型，最终设计并实现驾驶行为检测系统，为汽车驾驶的安全性做出一些贡献，希望能在今后的道路行驶中减少驾驶员的非正常驾驶，实现道路千万条，安全第一条的目标。  国内外现状 1.2.1 驾驶行为评判标准研究 驾驶行为识别在进行识别正常驾驶与非正常驾驶前，首先需要确认的是什么行为下是正常驾驶，什么行为下是非正常驾驶，即驾驶行为判别需要有一套明确的评判标准。 对于分心驾驶，早在1967年,Senders等人[10]就通过遮挡方式检测与特定任务相关的视觉需求，来评估该任务中的行为是否会影响正常驾驶, 1999年van Winsum等人[11]提出的外围检测任务（Peripheral Detection Task，PDT）则更为简易实用。Stojmenova在2016年[12]和2018年[13]均对基于听觉和触觉的PDT做出一些研究，并得出基于触觉刺激的PDT对分心驾驶敏感度更高的结论。Mattes[14]于2003年研究的变道测试（Lane Change Test ，LCT）更基于驾驶行为本身。最后较为新颖的方法是利用盒任务法[15]（Box Task，BT），与DRT结合使用，Daniel Trommler等人[16]用该方法来评估各种的行为对驾驶注意力的影响，并且该方法对于大多数分心行为均可评判。 对于疲劳驾驶的评判方法上则更加精确一些，主要是依据驾驶员的生理数据，比如眨眼时间[17]、头部运动[18]、瞳孔反射[19]等，在驾驶员的众多生理数据中评判疲劳驾驶的最佳数据为驾驶员的脑电数据[20]。 南京邮电大学专业学位硕士研究生学位论文 第一章 绪论  3 1.2.2 分心驾驶行为识别与疲劳驾驶行为识别的研究 通过评判分心驾驶和疲劳驾驶的研究，得出了一些驾驶行为的类型，基于此，针对驾驶行为的识别也有了很大的进展。由于疲劳驾驶与分心驾驶存在一定程度的识别区别，所以对于驾驶行为识别工作，也与评判工作一样，分为分心驾驶识别和疲劳驾驶识别两个方面来研究。 （1）分心驾驶行为识别 分心驾驶识别的数据来源基本上分为三大类，汽车行驶状态数据、驾驶员图像数据和驾驶员生理数据，分别来自于车载传感器[21][22]、车载摄像头[23]和人体生理信号采集装置[24]。其中，车载传感器和车载摄像头属于是非侵入性传感器[25]，即不影响驾驶正常驾驶，而人体生理信号采集装置如脑电检测设备、心电检测设备等属于侵入式传感器[26],容易影响驾驶员驾驶状态，使得这方面主要应用于评判研究和评估驾驶员精神状态等。 基于车载传感器的分心驾驶行为识别研究由于依靠部署在汽车上的传感器来实现，属于间接性识别，且需要汽车厂商的合作，所以学术界对此种方法只做了一定的研究，如使用方向盘位置传感器[27]，通过计算方向盘转向角和观测的转向角的误差进行二阶泰勒展开，当数值偏差于一定值时识别为分心驾驶。还有文献[28]使用车速、侧向加速度、方向盘转速、方向盘转向角来作为识别指标，提出了一种基于逻辑回归和Fisher判别的机器学习模型，识别准确度为94.79%。 基于驾驶员图像数据的驾驶行为识别工作则更加普遍且易于实现，比较偏受研究者青睐[29]。获取驾驶员图像数据的设备通常为车载单目摄像头，早先的研究者通过图像处理和传统机器学习的方法采集特征进行识别驾驶员行为。Billah等人 [30]通过车载摄像头提取的驾驶员视频数据，检测和跟踪驾驶员的基准身体部位，根据跟踪轨迹之间的相对距离来表示驾驶员的动作特征，利用KSVM提取的身体部位特征以识别驾驶员的分心驾驶行为。ZHAO等人[31]使用随机森林、多层感知机和K近邻算法对拍摄到的抓方向盘、打电话、吃东西和调收音机图像数据进行分类识别，准确率达到90.5%。 随着深度学习在计算机视觉领域的不断发展，卷积神经网络(Convolutional Neural Networks, CNN)、循环神经网络(Recurrent Neural Network, RNN)和长短期记忆网络(Long Short-Term Memory，LSTM)以其能够从训练数据中自动学习特征的优势，被广泛应用于分心驾驶行为识别领域。Eraqi等人 [32]提出了一种可靠的基于深度学习的司机分心检测与识别解决方案。他们从仪表盘上方的摄像机获取RGB图像，并对原始图像、皮肤分割图像、人脸图像、手图像、脸结合手图像训练多个卷积神经网络架构。最后对所有网络的输出进行加权和南京邮电大学专业学位硕士研究生学位论文 第一章 绪论  4 评估，并使用遗传算法进行最终的类分布。Yehya [33]将遗传算法和卷积神经网络相结合，使用卷积神经网络来训练驾驶员姿势的各部分图像，最后再对其输出的加权和进行评估，得到最终训练结果，虽然这种方法在一定程度上提高了识别精度，但无法及时识别驾驶员是否注意力分散的动态信息。Alotaibia和Alotaibi [34]开发了一个检测分心驾驶员姿态的系统，他们将带有残差块的inception模块和层次递归神经网络(HRNN)进行组合形成一个新的深度学习模型，从而提高了识别分心驾驶员行为的性能。Behera等 [35]提出了一种新的多流长短期记忆(M-LSTM)网络，用于识别难以区分的细粒度驾驶员分心活动，M-LSTM融合了LSTM和CNN的概念，用于识别司机的分心活动，如发短信、打电话、吃东西和喝水。夏瀚笙等人[36]提出一种使用驾驶员的人体关键点位置信息来帮助卷积神经网络识别驾驶员是否分心驾驶的方法，通过加入人体关键点的位置信息, 减少背景信息的干扰。该方法在State Farm数据集上达到了94.934%的准确率。 （2）疲劳驾驶行为识别 与分心驾驶行为识别相比，疲劳驾驶行为识别在基于驾驶员图像数据上都集中在驾驶员面部区域以及头部运动等。其中驾驶员头部运动检测方法是最早开始的，1994年Wierwille、1996年Knipling、1998年Dinges、1999年Carroll、2001年 Philip W. Kithil 等众多研究者进行了一系列的研究，总体研究方案可概括为采用驾驶舱上方的传感器装置，确定并跟踪驾驶员的头部位置，依据不同时刻下驾驶员头部产生的位移变化判断其是否处于疲劳状态 [37] [38]。眼部检测是一种更加精确直观检测方法，PERCLOS(Percentage of Eyelid Closure over the Pulil, over Time)检测手段最具代表性，其原理是根据单位时间内眼睑闭合一定比例所占的时间，检测识别驾驶员否处于疲劳状态的效果[39]。PERCLOS检测方法具有检测精确、无侵入的优点，研究表明其检测精度可以达88.6%，但同时，在实际应用中，对此方法的及时性、实时性有着极高要求[40]。 Zhang[41]等采用快速、鲁棒的人脸检测算法对人脸表情图像进行描述和归一化，并采用Adaboost从一个大的LBP特征池中学习最具判别性的疲劳面部LBP特征，并结合支持向量机(SVM)分类器对驾驶员疲劳状态进行了有效的判别。AkrOut[42]等提出了一种基于眨眼测量和三维头部姿态估计的睡意检测融合系统，通过分析一个非平稳的非线性信号来研究驾驶员的眼动行为，同时利用人脸的三个关键点来估计头部在偏航、俯仰和滚转三个方向上的转动，该系统由DEAP和MiraclHB两个数据库进行评估，取得了较好的识别准确率。Mbouna[43]利用人眼指数(EI)、瞳孔活动(PA)和头部姿态(HP)等视觉特征提取车辆驾驶员非警惕性的关键信息，然后利用SVM对驾驶员疲劳状态进行分类。Deng[44]等提出了一个名为DriCare的系统，综合考虑打哈欠和睁闭眼时间两项指标来评估驾驶员状态，同时引入了新的人脸检测与跟踪南京邮电大学专业学位硕士研究生学位论文 第一章 绪论  5 算法来提高检测精度，取得了较高的判别准确率。郑伟成等人[45]利用MTCNN模型检测驾驶员人脸图像,使用PFLD深度学习模型进行人脸关键点检测以定位眼部、嘴部和头部位置,从中提取眨眼频率、嘴巴张开程度和点头频率等特征参数,通过多特征融合策略检测疲劳驾驶。 对于分心驾驶和疲劳驾驶的联合检测，即完整的驾驶行为检测，2019年Kim等[46]利用眼睛注视方向检测分心驾驶以及眼部和嘴部的图像数据检测疲劳驾驶，具体是使用MobileNet作为基础模型结构和多任务学习机制建立识别驾驶员面部状态的深度神经网络模型MT-Mobilenets。2022年Fan等[47]对列车驾驶员进行了分心驾驶和疲劳驾驶的研究，使用放置在驾驶员前额的脑电信号记录装置采集脑电信号，利用该信号提出了基于时间序列的集成学习方法，优于其他主流的机器学习算法。  论文主要研究及结构安排 本文旨在通过联合分心驾驶和疲劳驾驶识别，完成完整的驾驶行为识别，建立一个基于深度学习与计算机视觉的驾驶行为检测边缘系统。首先利用分心驾驶和疲劳驾驶各自需求的摄像头视角不同，采用多个摄像头采集驾驶员图像数据，并以此使用多视图学习机制，以驾驶员的多角度图像，建立基于多视角深度神经网络的驾驶行为识别模型，该模型在多视角图像处理的数据基础上引入注意力机制，完成分心驾驶的识别，以实现更高的检测准确度。其次，使用RetinaFace人脸检测模型检测驾驶员人脸，针对驾驶员面部区域的图像建立深度神经网络进行闭眼和打哈欠的识别，利用多任务学习机制修改模型决策层，实现分心驾驶和疲劳驾驶两个识别任务。最后，本文基于疲劳驾驶检测机制，建立基于视频数据的驾驶行为检测系统，检测视频数据的分心驾驶和疲劳驾驶，已达到实际使用要求。 本文内容主要分为六章，具体安排如下： 第一章首先介绍了驾驶行为检测与它的两个核心检测任务分心驾驶检测、疲劳驾驶检测的研究背景及意义。然后，针对驾驶行为评判标准的国内外研究和分心驾驶、疲劳驾驶识别工作的国内外研究分别进行了描述。最后，总结了本文的主要工作与文章结构。 第二章对本文所涉及到的相关技术与背景知识进行了介绍。首先在深度学习领域分别介绍了迁移学习、卷积神经网络和注意力机制。然后介绍了多视图学习的机制与相关网络模型。最后介绍了多任务学习及相关框架和具体模型结构。 第三章介绍了本文提出的多视图分心驾驶行为识别模型。首先介绍基于分心驾驶和疲劳驾驶图像识别所需的视角不同而建立的多视角驾驶行为图像数据集，并对此数据集进行了有效性证明实验。然后介绍了多视图分心驾驶行为识别模型的模型建立过程和模型训练相关函南京邮电大学专业学位硕士研究生学位论文 第一章 绪论  6 数与参数的设计。最后将提出的模型与单视图模型、同类型的多视图模型进行对比，并针对多个视角进行视角对比，确认了更为高效的视角数据。 第四章介绍了疲劳驾驶行为识别方案和本文提出的多任务分心驾驶与疲劳驾驶特征联合识别模型。首先介绍了疲劳驾驶行为识别的流程，包括人脸检测算法和人脸区域的疲劳驾驶特征识别模型。然后针对本文提出的多任务模型进行模型结构和训练相关函数进行了介绍。最后，设计了多任务模型的训练相关函数对比实验和对单任务模型的对比实验。 第五章介绍了驾驶行为检测系统的设计流程。首先介绍了检测系统的总体结构。然后介绍了基于特定疲劳驾驶特征的疲劳驾驶判定策略，并对视频形式的驾驶行为数据集进行了测试，得到合适的策略参数。最后，介绍了基于多任务驾驶行为识别模型的检测系统的设计流程与系统的测试结果。 第六章主要对本文所做工作的总结与展望。总结并分析本文工作不足之处，基于不足之处提出了可以改进或完善的想法，并对未来的驾驶行为检测技术进行了展望。    南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  7 第二章 相关背景知识介绍 本文主要针对基于深度学习和计算机视觉的驾驶行为检测进行研究，因此本章节针对深度学习技术和计算机视觉技术进行介绍，包括迁移学习、卷积神经网络、多视图学习、注意力机制等。同时为后文中的多任务学习机制也做一定的相关知识介绍。  深度学习相关知识 2.1.1 迁移学习 迁移学习（Transfer Learning）属于机器学习（Machine Learning）的一种方法，应用于跨领域的知识迁移[48]。迁移学习的方法有多种，其区别主要在于使用迁移学习的基础不同，包括样本、特征、模型和关系。在深度学习中，迁移学习使用得也越来越普遍[49]，通常是比较知名的学者或团队提出一种通用的模型，该模型通常在大型或超大型数据集上，如COCO、ImageNet等，使用大量算力进行训练，其任务可以是图像分类、目标检测、图像分割等等，然后各领域的研究者使用该模型基于特定领域对部分参数重新训练或修改部分结构以达到特定任务效果。深度学习上的这种迁移学习方法也可称为归纳迁移[50]，使用归纳迁移可以在较小规模数据集和有限算力上“借用”在大规模数据集和大量算力训练得到的模型，使用其部分训练好的部分神经网络参数和结构，得到更为高效的特定任务模型，因此基于本文的驾驶行为图像分类任务，后文选取了较为知名且有效的图像识别通用模型作为实验的参考和新模型的基础框架。 2.1.2 卷积神经网络 卷积神经网络（Convolutional Neural Networks, CNN）是深度学习技术的代表算法之一，如其名，CNN包含大量卷积计算，在处理2D和3D图像的结构上进行了高度优化[51]，对学习和提取2D特征方面非常有效，因此本文主要以CNN为主要模型结构，以下为第三章和第四章所使用的CNN模型。 （1）VGG VGG模型[52]是牛津大学Visual Geometry Group团队于2014年提出的卷积深度神经网络模型。VGG模型在2014年的ILSVRC竞赛分类任务中获得第二名，且在多种迁移学习任务南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  8 中效果优异。VGG模型主要贡献在于提出加深神经网络深度可以有效提高准确率，且使用了小卷积核来代替大卷积核。VGG模型在[52]中使用了多种版本，其区别主要为网络层数不同和使用了一些特别的方法。本文主要以其中的VGG16作为实验的基础模型框架之一。而VGG16中的16指模型中卷积层和全连接层加起来一共16层，不包含池化层，具体结构如图2.1。  图2.1 VGG16模型结构  VGG16一共有13层卷积层，因此卷积层是该模型，同时也是众多CNN模型的重点。卷积层进行的处理为卷积计算，卷积计算的主要操作是使用过滤器（卷积核）在输入特征图上进行滑动计算，得到输出特征图。过滤器是建立该卷积层时设计好的数量为N，尺寸为F*F*C的矩阵，C为通道数，需要与输入数据的维度一致。滑动计算是指过滤器在输入数据上以一定步幅S进行元素累乘相加计算，计算过程如下图2.2。  图2.2 卷积计算过程图 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  9 由于边缘端的元素比中间的元素的卷积操作次数少，因此需要在进行卷积操作前进行Padding，即在输入特征图的四周补零，其补零的大小为P。由于卷积操作为累乘相加，因此补零不会带来额外的噪声，同时又给边缘元素带来了类似中间元素的计算待遇。 输出特征图的通道数为过滤器的数量N，而通道方向上的输出特征图尺寸计算具体如下： 𝑂=𝐼−𝐹+2𝑃𝑆+1                              （2.1） O为输出特征图的高度/宽度，I为输入特征图的高度/宽度。 池化层在CNN中通常位于每一卷积层的后面，它的主要作用是下采样，可以有效减少网络参数，达到加快计算速度和防止过拟合的作用。池化层主要有最大池化（Max pooling）和平均池化（average pooling）两种，而Max pooling是最为常见的，在VGG16模型中的池化层就采用的Max pooling。Max pooling的操作就是将特征图划分为若干个矩阵，对每个矩阵输出其最大值。 除了卷积层外，全连接层（fully connected layers，FC）也是VGG16模型中较为关键的部分，它起到“分类器”的作用，将卷积输出的特征空间映射到样本标记空间，即模型卷积层提取出的特征通过FC整合成一个值，有利于减少特征位置对于分类结果的影响，提高整个模型的鲁棒性。 （2）GoogLeNet GoogLeNet模型[53]是Google团队于2014年提出，在当年的ILSVRC竞赛分类任务获得第一名。GoogLeNet模型的重点主要在网络宽度方面，相对于专注于网络深度的VGG模型来说，GoogLeNet在加宽网络宽度方面提出了一种新的基础结构，即Inception结构，因此GoogLeNet模型也被叫做InceptionNet。Inception结构是一种并行结构，如图2.3，将输入特征图同时输入到四个分支中进行处理，再将四个分支处理得到的特征图在通道方向上进行拼接，同时使用1*1卷积核构成的卷积层来进行降维，减少参数量。  图2.3 Inception结构+1*1卷积核降维 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  10 GoogLeNet模型除了Inception结构外，还使用了两个辅助分类器来来提高准确率，主要原因是神经网络的中间层也具有一定的识别能力，也为了防止梯度消失。这两个辅助分类器在总损失中的权重为0.3，当然这部分核心是为了提高网络性能，在进行推理工作时两个辅助分类器就会被抛弃。 （3）ResNet ResNet模型[54]是微软实验室中的何恺明等几位学者于2015年提出的，在当年的ILSVRC竞赛分类任务获得第一名，目标检测任务第一名，在COCO数据集中目标检测第一名，图像分割第一名。ResNet模型也叫残差网络模型，在网络深度方面近一步发展，它的初始网络深度达到152层。但ResNet模型不只是单单堆砌层数，在[54]中也提出当网络深度到几十层后，网络性能不会再提高，反而越来越差，即深度网络的退化问题，所以何恺明等学者就此问题设计出了新的网络结构，残差结构（residual结构）。 残差结构的基本结构如图2.4，多个类似的残差结构组成了ResNet模型。残差结构使用了shortcut连接方式，即输入特征图在进行一定的卷积层后得到的特征图再与输入特征图进行直接的相加，再得到最终的输出特征图。所以在进行卷积操作后一定要维持特征图的尺寸与输入特征图一致，否则无法进行shortcut操作。  图2.4 残差结构图 （4）MobileNet MobileNet模型[55]是Google团队于2017年提出的，是一款轻量级CNN模型，也是近年来轻量级网络的代表之一，它的参数量是VGG16的2%左右，GoogLeNet的一半不到，因此MobileNet在移动端的使用和部署是比较可行的。 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  11 MobileNet的低参数量主要原因是将常规的卷积操作换成了深度可分离卷积（depthwise separable convolution）。深度可分离卷积的具体计算流程如图2.5，深度可分离卷积由逐通道卷积（depthwise convolution）和逐点卷积（pointwise convolution）组成，深度卷积是指卷积核的通道数为1，而数量为输入特征图的通道数，这样输出的特征图的通道数与输入特征图的通道数一致，只在单通道特征图上完成了特征提取。逐点卷积类似于常规卷积，只不过它的卷积核的尺寸为1*1，通道数为输入特征图的通道数，达到升维或降维的作用，同时也是在通道方向上达到了特征提取效果。  图2.5 深度可分离卷积  常规卷积的参数量主要是由多个卷积核构成，设卷积核尺寸为K*K*M，数量为N，因此参数量为K*K*M*N。而深度可分离卷积的参数量由逐通道卷积的卷积核跟逐点卷积的卷积核共同组成，逐通道卷积核尺寸为K*K*1，数量为M，参数量为K*K*M，而逐点卷积核尺寸为1*1*M，数量为N，参数量为M*N，因此深度可分离卷积总参数量为K*K*M+M*N。深度可分离卷积的参数量与常规卷积的参数量比值如下： 𝐾∗𝐾∗𝑀+𝑀∗𝑁𝐾∗𝐾∗𝑀∗𝑁=1𝑁+1𝐾2                          （2.2） N为输出特征图的通道数，而K为卷积核尺寸，所以很明显深度可分离卷积的参数量要远远小于常规卷积的参数量，且能达到一样的特征提取效果。 （5）ShuffleNet ShuffleNet模型[56]是旷视科技于2017提出，与MobileNet模型相同，都是针对移动端的使用和部署，属于一种轻量型网络模型。ShuffleNet模型的核心方法是对分组卷积进行channel 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  12 shuffle，即将分组卷积中的不同组得出的特征图进行重新排列，如图2.6所示，使得分组卷积组间的信息得到分享，提高网络的特征提取能力。分组卷积（group convolution），是把特征图按通道方向分为不同组，分别进行卷积计算，MobileNet的深度可分离卷积就属于分组卷积，只不过深度可分离卷积的分组数就是特征图的通道数。     图2.6 使用channel shuffle的分组卷积 2.1.3 注意力机制 注意力机制（Attention Mechanism）在上世纪九十年就有相关研究，主要针对语言翻译这一领域，直到2014年Volodymyr等人的《Recurrent Models of Visual Attention》[57]将注意力机制应用于视觉领域，再在2017年Ashish Vaswani的《Attention is all you need》[58]中提出transformer结构之后，注意力机制就在多个领域中发展得越来越迅速了。由于本文以CNN为模型基础框架，所以主要介绍的是基于神经网络的注意力机制。 神经网络中的注意力机制就是将人的感知方式与注意力的使用应用于神经网络里，将神经网络提取的特征进行重要性判定，将更加关键的特征提高权重，不重要的降低权重，以此来提高网络性能。2017年Hu Jie等人提出的SE-Net模型[59]就引入了注意力机制的思想，该模型获得了2017年最后一届ILSVRC竞赛分类任务的第一名。SE-Net模型的核心是SE（Squeeze-and-Excitation）模块，该模块基于注意力机制思想，利用loss对特征各个通道进行权重划分，具体流程如图2.7，使有效的特征图权重增大，效果小甚至无效果的特征图权重减小，由此训练模型达到更好的结果，其中Fsq函数为基于通道方向对特征图进行压缩，Fex函数为对得到的压缩通道特征进行权重学习，里面包含两个全连接层，最后的Fscale函数是对压缩前的特征图与得到的通道权重相乘，最终达到经过通道权重调整的特征图。 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  13   图2.7 SE模块框架  多视图学习相关介绍 多视图学习（Multi-view learning）是机器学习中的一个发展方向[60]，多视图学习是从多个不同特征集的数据中进行机器学习，其优势是通过事物的多个角度进行学习，让模型从多方面更好地理解事物，从而实现模型的更优解。 多视图学习需要的不同的角度或特征集需要符合两个准则，即共识准则（consensus principle）和互补准则（complementary principle）。因此多视图的来源通常有两种，一种是事物的多个特征源，如人物识别可以通过脸部特征、步态特征等多方面特征进行多视图学习。另一种是事物一个特征源的多种表现，比如图像识别可以通过普通RGB图像、红外图像、深度图像等图像的多种表现进行多视图学习。 应用多视图学习的领域有多种[61],无监督学习中的降维、半监督学习、监督学习、主动学习、集成学习和迁移学习等等。 而近年来随着深度学习的不断发展，多视图学习在深度学习上也得到越来越多的关注，所以针对多视图学习的神经网络模型在学界提出的比较多。 MVCNN模型[62]是Su等人于2015年提出的一项具有开创性的研究，基于多视图学习使用二维图像来识别三维物体。MVCNN将12个视角的图像输入网络，使用二维卷积核对数据进行特征提取，再利用全连接层作为分类器得到识别结果。MVCNN的模型框架是基于常规的VGG16、ResNet等CNN模型，而核心在于在特征提取层和分类器中间设置了一个View-pooling层，起到“聚合”作用，即把多个视角图像获得的特征进行融合，模型框架如图2.8。View-pooling层采用的计算方法是把从特征提取层得到的12个特征图逐元素取最大值，类似于最大池化层的操作，只不过MVCNN是在特征图方向上进行取最大值。 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  14  图2.8 MVCNN模型框架 GVCNN模型是基于MVCNN的研究基础上，Feng Y等人[63]提出的一种使用视图分组聚合方法的多视图模型，该论文认为MVCNN没有将多视角下图像特征之间的关系进行有效利用，这会限制最后形成的聚合特征的效果。因此，Feng Y的团队先使用特征提取的的一部分FCN进行特征提取，然后使用一个group网络对各视角图像提取的特征进行评判打分，将得分接近的视角特征进行组合，由此划分出多个特征组，最终将各个特征组进行全局池化（view pooling）聚合一起，接上分类层进行分类。 View-GCN模型是2020年Wei X团队[64]提出的一种应用多视图学习进行图像分类新方案，提出View-GCN的论文中指出图像的视角位置也是多视图学习可以使用的特征之一，因此论文使用图卷积神经网络进行图像分类，提出view-Graph来提供图像的特征和图像的视角位置，以此来表示物体的3D形状。  多任务学习相关介绍 多任务学习（Multitask learning）[65]是也属于机器学习，是把多个相关的任务共享模型的一种方法，前文描述的多种模型都是深度学习中的单任务学习，单任务学习是针对某一特定任务建立模型，如人体动作分类识别、目标检测、物体识别等等，它们只对现实中的一个问题进行分析，即使问题很复杂，也是划分为一个个子问题再单独分析，从而忽略了现实中的联系性原理，即现实世界中复杂的问题的子问题之间存在着丰富的关联信息。因此多任务学习的优势就在于把多个相关联的任务结合共同分析，共享部分信息，以此相互促进任务的完成效果，同时也提高了模型的泛化效果。 多任务学习使用的前提是多个任务是相关联的，而任务的关联性可以有以下四种解释。 （1） 需求的特征一致或接近； 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  15 （2） 任务之间的区别界限不大； （3） 任务需要解决的问题主体一致或接近； （4） 任务之间的联系性不明显，但可以提高其中某一个任务的泛化效果。 基于多任务学习的深度神经网络模型较于单任务模型，其优势可以有三点，一是多任务 模型的不同任务在共享层里的局部极小值位置是不同的，二是通过多任务之间不相关的部分的相互作用，有助于逃离局部极小值点，三是多任务之前相关的部分有利于底部共享层对通用特征表示的学习。 多任务学习的核心在于多任务的共享机制，共享的形式有两种，一种是基于参数的共享，另一种是基于约束的共享[66]。基于参数的共享是指多个任务共同使用神经网络的隐藏层，对各自任务建立独立的输出层，在CNN中就是指任务共同使用特征提取层，分类层是各自建立的。而基于约束的共享是指各个任务拥有各自的模型结构，通过对任务间的参数差异添加约束，如使用L2正则化。两者的差异如图2.9所示。  图2.9 多任务学习的共享形式 随着近些年深度学习的迅速发展，原本属于机器学习的多任务学习也逐渐进入深度学习领域中。目前在深度学习领域，基于约束的共享形式是研究的重点倾向，主要原因是基于参数的共享形式在任务的数量增多的情况下，模型提取到能表示所有任务的特征的能越来越弱，模型的性能也越来越差。尽管完全的参数共享形式不可取，但在特征提取层中部分使用参数共享，可以有效利用参数共享能够降低过拟合风险的能力。 在如何实现共享的参数和不参与共享的参数融合起来，一起送入到各自任务的分类层中，Google的团队在2017年提出的MoE（Mixture of Experts）结构[67]以及次年提出的MMoE（Multi-gate Mixture-of-Experts）模型[68]有效解决了上述问题。 MoE结构一开始是应用于语言建模和机器翻译，主要目的是缩小模型规模，其结构如图2.10所示，由多个Expert网络组成，每个Expert网络都是较为简单的前馈神经网络，此外还南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  16 包含一个可训练参数的Gating网络，Gating网络经过训练后得到各个Expert网络的输出权重，以此得到更高效的特征表达。  图2.10 MoE结构 对给定的输入x，用𝐺(𝑥)和𝐸𝑖(𝑥)表示Gating网络和第i个Expert网络的输出，MoE模块的输出y可以用以下公式表达： 𝑦=∑𝐺(𝑥)𝑖𝐸𝑖(𝑥)𝑛𝑖=1                          （2.3） 由公式（2.3）可知，MoE结构就是将多个子网络经过加权再输出，而加权则通过Gating网络进行训练得到。 MMoE模型是基于MoE结构的多任务学习模型，它的核心思想是将shared-bottom结构替换成MoE结构，而shared-bottom结构实际上就是参数共享的部分神经网络层，基于shared-bottom结构的模型框架如图2.11所示，文献[69]把该框架作为多任务学习模型的baseline。  图2.11 基于shared-bottom结构的模型框架 南京邮电大学专业学位硕士研究生学位论文 第二章 相关背景知识介绍  17 MMoE模型框架如图2.12，把Expert网络的数量设为3个，在每个任务都设置了Gating网络，以此实现不同任务可以自由的共享参数，从而发挥多任务学习的核心思想，即多个相关联的任务的解决可以相互依靠，而不是单独解决。  图2.12 MMoE模型框架 在基于shared-bottom结构的多任务学习模型中，shared-bottom结构的输出是固定的𝑓(𝑥)，第k个任务的分类层输出则是ℎ𝑘(𝑓(𝑥))，以此第k个任务得到的输出为： 𝑦𝑘=ℎ 𝑘(𝑓(𝑥))                        （2.4） 而MMoE模型中，第i个Expert网络的输出为𝑓𝑖(𝑥)，对应的Gating网络在第k个任务下的表示为𝑔𝑘(𝑥)𝑖，由此得到第k个任务得到的输出为： 𝑓𝑘(𝑥)=∑𝑔𝑘(𝑥)𝑖𝑓𝑖(𝑥)3𝑖=1                    （2.5） （2.4）式和（2.5）式的对比得出在相同参数规模上MMoE模型进行了更加自由的参数共享，每个任务都可以在训练中得到最高效的Expert网络输出权重分配。  本章小结 本章首先引出深度学习中的卷积神经网络，对VGG、GoogLeNet等5种卷积神经网络模型进行了系统概述，介绍了如Inception结构、残差结构、深度可分离卷积等模块和机制，同时也介绍了可以提高模型性能的注意力机制。然后介绍了多视图学习以及相关模型MVCNN，在第三章中将基于MVCNN模型进行分心驾驶识别的实验研究。最后介绍了多任务学习的任务关联性研究、共享机制和基于深度学习的多任务学习模型MMoE，第四章中将基于多任务学习把分心驾驶和疲劳驾驶两个分类任务联合起来，实现统一的驾驶行为识别。 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  18 第三章  基于多视图学习的分心驾驶行为识别研究  分心驾驶行为图像数据集 3.1.1 公开数据集 公开的分心驾驶行为图像数据集目前是比较多的，比如2016年Kaggle比赛使用的State Farm数据集[70]、2017年Yehya团队提出的“AUC Distracted Driver” 数据集[71]、2022年Magic Data发布的开源DMS驾驶员行为数据集[72]等。由于近年来针对图像的分心驾驶行为识别研究大多以State Farm数据集作为参考数据集，因此本文将以State Farm数据集作为公开数据集的代表进行实验研究，并以相关论文的实验数据与本文数据进行对比，得到较为客观的分析。 State Farm数据集是2016年Kaggle的一项比赛中提出的数据集，其旨在使用驾驶员的驾驶行为图像来有效识别是否是分心驾驶，以此提高交通道路安全。State Farm数据集的图像为RGB图像，像素大小为640x480，视角为汽车副驾驶位置右上方，分心驾驶行为分为10类，如图3.1所示，分别为C0安全驾驶（Safe Driving）、C1右手发短信（Texting using Right Hand）、C2右手打电话（Talking on the Phone using Right Hand）、C3左手发短信（ Texting using Left Hand）、C4左手打电话（Talking on the Phone using Left Hand）、C5收音机操作（Operating the Radio）、C6喝水（Drinking）、C7伸手至后座（Reaching behind）、C8梳头发和化妆（Doing Hair and Makeup）、C9与乘客交谈（Talking to Passengers）。  图3.1 State Farm数据集行为类别 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  19 State Farm数据集在Kaggle官方网站中包含两个文件夹，分别是训练（train）集和验证（test）集，训练集中对图片进行了标签处理，总共22424张图片，而验证集中的图片没有标签，数量为79726张。为便于后续实验，本文对State Farm数据集进行了部分整理与清洗。首先使用训练集，划分为训练集和测试集，划分比例为4：1。使用VGG16模型进行训练，利用得到训练的模型对验证集进行初步分类。然后再人工进行细致分类，剔除模型分类的错误结果。最终得到全新的State Farm数据集，如图3.2所示。  图3.2 经过整理与清洗的State Farm数据集 经过统计，新的State Farm数据集参与人数为55人，图片总数为97858张，划分为训练（train）集、测试（val）集和验证（test）集，划分比例为6：2：2。 3.1.2 NJUPT多视角驾驶行为图像数据集 State Farm数据集是针对分心驾驶所设计的，所以在图像视角上为了便于清晰有效的显示驾驶员的肢体行为，采用了汽车副驾驶位置右上方的视角。但由于分心驾驶行为只是驾驶行为的一部分，此外还有疲劳驾驶和危险驾驶，其中危险驾驶视角要求与分心驾驶差不多，都需要图像尽可能完整的展示驾驶员的肢体行为，而疲劳驾驶则不同，它对驾驶员的图像视角要求是针对驾驶员上半身，尤其是头部面部区域，因为驾驶员的疲劳在头部运动、眼睛的闭合、嘴巴的动作等方面更加明显，导致疲劳驾驶识别模型更加聚焦于此部分。 综上所述，State Farm数据集并不能作为完整的驾驶行为图像数据集，因此本文结合分心驾驶和疲劳驾驶的图像视角需求，制作了多视角驾驶行为图像数据集（NJUPT Multi-view Driver Action Dataset），可以简称其为NMDA数据集。考虑数据集的真实性以及行车的安全性的问题，本文在真实汽车内模仿不同的驾驶行为，而不在道路上进行实际的多种驾驶行为南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  20 以避免发生交通事故。 NMDA数据集采用了文献[71]数据集制作的流程与方法，使用的摄像头均为智能手机的主摄像头，设备包含一台小米Mi8、两台iphone12和一台iPhone12Pro，共4台采集设备，采集了四种视角的图像数据。虽然四个视角的采集设备并不一致，但录制的视频参数均一致，像素大小为1280x720，30fps，所以采集设备的不同并不影响图像质量的一致性。四个视角参考相关驾驶行为数据集和市场上的新能源汽车内部摄像头位置，分别为驾驶员左侧的A柱方向、驾驶员的正前方、驾驶员右前方，即后视镜下方位置以及驾驶员右侧方，其位于副驾驶位车门上方。具体视角图像如图3.3所示，其中红色框为摄像头所在位置。  图3.3 驾驶行为图像视角 NMDA数据集包含分心驾驶和疲劳驾驶两部分，此处只涉及分心驾驶部分，疲劳驾驶部分将于第四章进行介绍。NMDA数据集分心驾驶部分参考State Farm数据集和其他驾驶行为图像数据集，将分心驾驶行为分为12类，其中包含正常的驾驶行为，具体如图3.4和图3.5所示，分别为01正常驾驶、02左手打电话、03右手打电话、04左手发消息、05右手发消息、06与乘客交谈、07操作中控屏、08喝水、09吃食物、10伸手至后座、11座位下捡拾、12双手离开方向盘。 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  21  图3.4 NMDA数据集行为类别（a） 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  22  图3.5 NMDA数据集行为类别（b）  NMDA数据集参与人数19人，16位男性和3位女性，年龄分布在20岁至28岁，没有佩戴帽子或太阳镜等遮挡物。 NMDA数据集首先以视频的形式进行驾驶行为数据采集，每个动作持续30秒，动作速南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  23 度适中，尽量拟合真实驾驶环境。采集得到视频通过Premiere剪辑并同步时间，分为12类各19个短视频。然后使用Python语言编写脚本将视频进行抽帧操作，以此得到格式为.jpg的图像数据集，抽帧的频率为每秒3帧，该频率下得到的图像清晰稳定且无大量重复相似。最后对整理的图像数据通过Python脚本进行自动标签化处理，同时依据标签人工检查图像准确性，以防出现无效行为导致数据集噪声过大。最终得到的数据集如图3.6所示，有效图片77388张，单时间下行为图片为19347张，即总图片数除以四个视角得到的行为数，由于数据量有限，实验人员不多且行为分类数较大，故不划分验证集进行验证使用，将总图片划分为训练集和测试集，划分比例为8：2，划分后图片数据为训练集图片60228张，测试集12340张。  图3.6 NMDA数据集最终文件 3.1.3 NMDA数据集有效性证明 NMDA数据集制作完成后，需要验证其有效性，即需要实验证明NMDA数据集是否可靠，能否与目前公开的驾驶行为图像数据集进行客观的比较。由此本文进行了以下实验。 NMDA数据集有效性证明实验的核心思想是通过应用于图像识别分类的CNN模型，分别在NMDA数据集和一个具有代表性的公开驾驶行为数据集上进行训练，当两者测试集准确率在一定的参照下差距不明显时即可证明NMDA数据集的有效性。 本小节的实验使用的CNN模型为VGG16、GoogLeNet、ResNet、MobileNet-V2和南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  24 ShuffleNet，其中ResNet依据层数分为ResNet34和ResNet50两个版本，总计6种模型。由于目前国内外暂无公开的多视角驾驶行为图像数据集，且NMDA数据集的建立借鉴了State Farm数据集，其中一个视角与State Farm数据集一致，所以在本实验中采用该视角来与State Farm进行比较。尽管NMDA数据集在本次实验中仅使用了一个视角，但由于NMDA数据集多视角图像具有时间同步性，在验证该视角后，即可证明NMDA数据集多视角图像均具有有效性。 本小节的NMDA数据集有效性实验采用的模型均为模型初始结构，部分参数数据如表3.1所示。 表3.1 模型参数数据表 batchsize 32 损失函数 CrossEntropyLoss epochs 50 动态调整学习率 ReduceLR0Plateau LR 0.0001 数据预处理 RandomResized CropColorJitter 优化器 Adam  上述6种模型在State Farm数据集和NMDA数据集上分别进行训练，得到各自测试集的准确度，如图3.7所示，其中SF10为State Farm的10分类数据集，NMDA12为NMDA的12分类数据集。  图3.7 模型在两种数据集上训练得到的结果 图3.7显示SF10上训练的模型准确率在80%至90%之间，平均准确率为86.78%，而在NMDA12上训练的模型准确率较差，最低为61.1%，平均准确率为67.3%。两者差距为25.68%，差距因素主要是任务分类数量不同和数据集的数据规模不同，此外模型没有进行适当的调参也是准确率相对较低的原因之一。新的State Farm数据集分类为10种，人数55人，总图片数达78387，而NMDA数据集的单一视角分类为12种，人数19人，总图片数只有18142，所以6种模型在State Farm数据集上训练得到的准确率高于NMDA数据集的结论并不能表明南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  25 NMDA数据集是不可靠的。而NMDA数据集想要到达State Farm数据规模，需要更多志愿者进行采集数据任务，但由于采集数据时间处于特殊防控阶段，无法再采集30多人的图像数据，且添加小规模数据依然没有达到同等的数据规模，因此本小节的实验采用了新的对比方案。 新方案尽量考虑了分类数一致性和数据规模的大致相等的原则，将State Farm数据集的C8类别和NMDA数据集的09、11、12类别去除，使得两个数据集的分类一致。此外使用Python脚本将9分类的新State Farm数据集随机取5分之一，使得最后的State Farm数据集数据规模和新NMDA数据集大致相等。 新方案的两种数据集经过6种模型训练得到的结果如图3.8所示，其中SF9为State Farm的9分类数据集，NMDA9为NMDA的9分类数据集。  图3.8 新方案的数据集有效性对比结果 图3.8显示SF9和NMDA9两者的模型准确率差距缩小了很多，平均值差距为7.7%。尽管两者仍然有差距，但由于两者仍存在参与人数的不同，SF9拥有50多人的图片，而NMDA9只有19人，参与人数在模型的泛化性上也影响了准确率的高低，所以SF9和NMDA9的对比实验已经证明了NMDA9数据集具有相对有效性，从而可推广证明NMDA10及NMDA完整的数据集具有较为严谨的有效性。后文可以使用该数据集进行相应的实验，从而得出较为客观严谨的实验结论。  多视图模型设计研究 由于完整的驾驶行为图像视角需求，本文基于多视图学习设计了多视图分心驾驶行为识别模型，以此有效利用全部的视角图像，同时也为第四章联合疲劳驾驶行为识别任务奠定了南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  26 模型基础。 图像识别任务通常可以归类于图像分类任务，而CNN模型在此领域应用十分广泛[73-74]。基于多视角图像的数据输入需求和分类任务，本文的模型基础框架将使用MVCNN模型，MVCNN模型原本是用于3D物体的识别任务，但使用MVCNN基础框架完成多视角驾驶行为分类任务，既可以有效解决多视角模型的数据输入问题，又能使新模型有效利用多个视角图像特征，分类结果更加准确。 新模型名称为MMobNet，由特征提取模块、特征融合模块和分类模块组成。 3.2.1 特征提取模块 特征提取模块以MobileNetV3[75]的图像特征提取方案为骨架，如图3.9所示，MobileNetV3的特征提取方案采用的是模块化设计，由11个bneck组成，每个bneck都包含倒残差结构和SE模块，由输入参数列表确认是否使用它们。在该模块由于使用的是已有的模块，未进行大规模改动，因此采用迁移学习方法进行预训练，使用已经在超大规模数据集上训练得到的参数作为模型初始值，以此达到减少训练时间和提高结果准确率的目的。  图3.9 MobileNet-V3特征提取层结构，引用自文献[6] MobileNetV3模型的特征提取模块具有4个特点： （1） 深度可分离卷积，有效减少模型参数量； （2） 倒残差结构(the inverted residual with linear bottleneck)，提升梯度在卷积层之间的传播能力，内存使用效率更高； （3） SE模块（squeeze and excitation），通道注意力机制，提高特征图通道间的联系； （4） h-swish替代swish激活函数，降低计算成本。 深度可分离卷积已经在第二章的MobileNet模型中介绍了相关原理，MobileNet模型代指MobileNet-V1，是最初提出的MobileNet模型版本，而MobileNet-V3是Google团队于2019年提出的最后一版MobileNet模型。 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  27 倒残差结构如图3.10所示，与ResNet模型中的残差结构不同，倒残差结构先升维后降维。  图3.10 倒残差结构图 该结构的引入原因是最初的MobileNet模型是一种类似VGG模型的直筒状网络，不能像ResNet一样复用特征，从而提升对梯度和损失的相关性，所以添加了shortcut结构。 SE模块是一种轻量型模块，具体结构已经于第二章的注意力机制中得到了介绍，不过与SE-Net模型中的SE模块不同，MobileNet-V3中的SE模块使用的激活函数是该模型提出团队设计的新激活函数h-swish。 h-swish是swish的改进版本，swish非线性激活函数作为ReLU的替代，可以有效提高神经网络的精度，其定义如下： 𝑠𝑤𝑖𝑠ℎ 𝑥=𝑥∙𝜎(𝑥)                          （3.1） 𝜎(𝑥)=11+𝑒−𝑥                               （3.2） 其中的𝜎(𝑥)对于轻量型网络来说，其计算较复杂，求导困难，影响了模型的部署。因此Google团队提出了h-swish激活函数，其定义如下： ℎ−𝑠𝑤𝑖𝑠ℎ[𝑥]=𝑥𝑅𝑒𝐿𝑈6(𝑥+3)6                    （3.3）                    𝑅𝑒𝐿𝑈6(𝑥)=min (max(𝑥,0),6)                   （3.4） 如图3.11所示，h-swish与swish的计算曲线是几乎一致的，但计算难度差距显著。  图3.11 h-swish与swish的计算曲线，引用自文献[6] 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  28 3.2.2 特征融合模块 特征融合层是该模型设计的核心，与MVCNN的View-pooling层完全不同，MVCNN的特征融合方案仅仅是用不同视图的特征图按元素取最大值的操作实现“聚合”作用，没有考虑到各视图之间的特征差异。因此本文在特征融合层使用了名为视图注意力机制（View-based Attention Module，VAM）的模块，该模块对视图使用注意力机制，针对多视图进行重要性权重分析，提升有效视图的权重，抑制无效或效果不明显的视图权重。 VAM模块分为两个子模块，特征图的空间特征提取和基于视图的空间特征权重学习。 （1）特征图的空间特征提取 对所有图片的特征图进行空间特征提取，使用的方案基于空间注意力机制（Spatial Attention Module，SAM），如图3.12所示。在特征提取层得到的特征图尺寸为[B,576,7,7]，其中B为batchsize，576为通道数，7*7为特征图的高宽。在通道方向上使用global max pooling和global mean pooling两个操作，进行全局最大池化和全局平均池化，将两个[B,1,7,7]进行拼接得到[B,2,7,7]，再使用一个尺寸为1*1的过滤器进行卷积操作，经过激活函数Sigmoid后得到的[B,1,7,7]即为各个图片的空间特征。  图3.12 空间特征提取子模块 （2）基于视图的空间特征权重学习 对获取的图片空间特征基于视图进行权重学习，即视图的空间特征注意力机制，如图3.13所示。在得到[B,1,7,7]的空间特征后，将特征图尺寸进行转换，增加一个num_views尺度，即视图数量尺度，得到[B/num_views,num_views,1,7,7],其中B在图片输入时设置为b *num_views，所以B/num_views永远为一个整数。该操作是将一个batchsize中的图片依据视南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  29 角数（num_views）进行划分，加上图片输入时采用同一时间的图片连续输入的方法，使得视图注意力机制作用在一个时间内的多视图内。由于第二个维度尺寸为1，因此可以省略该维度，再转换为[B/num_views, num_views,7,7]，通常同样尺度的特征图，第二个维度往往是通道数Channels，而该特征图的第二个维度是指视图数（num_views），所以该特征图可以表示为B/num_views组特征，每组是B/num_views个7*7的特征，即B/num_views组多视图特征图的空间特征。然后针对空间特征进行global max pooling和global mean pooling两个操作，实现全局最大池化和全局平均池化，得到两个[B/num_views, num_views,1,1]的特征图，将两者分别送入一个两层的神经网络中，第一层1*1的过滤器个数为1024，第二个1*1的过滤器个数恢复至num_views,中间使用了ReLU作为激活函数。最后，将神经网络输出的两个特征图进行相应元素相加，再经过Sigmoid函数激活，得到最终经过权重学习的视图注意力特征（view-based attention feature）。 最后将视图注意力特征与对应的多视图特征图相乘，最终得到经过视图权重学习的多视图特征图。  图3.13 基于视图的空间特征权重学习子模块 3.2.3 分类模块 模型的分类模块，使用三层卷积的全卷积层取代了通常的全连接层，卷积层之间使用了Dropout层和h-swish激活函数。全卷积层更为高效，只需执行一次前向计算，减少计算量，该操作已经在大量的神经网络模型中得到了越来越多的使用。从特征融合模块得到的特征图南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  30 尺寸为[B,576,7,7]，第一个卷积层卷积核尺寸为7*7，数量为576，输出为[B，576，1，1]。第二个卷积层卷积核尺寸为1*1，数量为1024，输出为[B,1024,1,1]。第三个卷积层卷积核尺寸为1*1，数量为任务分类数，输出为[B,nclasses,1,1]，nclasses为任务分类数。  模型训练 3.3.1 数据预处理 通常的单视图模型的图像数据除了确定标签外不需要对图片进行额外的标注，因此在一个分类中图片的位置不要求固定，而多视图模型的图像数据则需要对图片除了确定标签外还需要进行额外的标注，其图片在分类文件中必须处于相对的固定位置。多视图模型的图像数据需要如此要求的原因是模型的输入格式。多视图模型需要几个同一时间的视角图片一起送入，因此需要对数据进行一定的预处理。 首先是对NMDA数据集进行数据标注。尽管图片已经针对内容进行了标签处理，但由于多视图模型数据输入要求，需要重新对数据集进行标签处理。将图片的标签按照“采集批次-行为编号-志愿者编号-时间编号-视角编号”的格式进行重新标注，如图3.14所示。进行标注后新文件中的图片会按照名称进行自动排序，使得同一时间的4个视角图片会固定得连续排列，不会再像单视图模型数据一样分散。此外按照此格式进行标注，也有助于后文的多视图视角对比实验，便于筛选视角图片，形成新的两视角数据集或三视角数据集。  图3.14 “采集批次-行为编号-志愿者编号-时间编号-视角编号”图片标签格式 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  31 其次是缩小图片的尺寸，720P的图像对模型来说过于庞大，此外在训练时输入数据数据量过大也会限定batchsize的大小，过大会严重压缩GPU的显存，使得训练时间过长。将图片从720P的大小通过reshape操作缩为224x224，如图3.15所示，此尺寸是目前大多数图像分类模型的通用输入尺寸。  图3.15 经过reshape操作的图片 然后扩充数据集,进行数据增广。由于NMDA数据集的参与人数有限，且数据规模也有限，所以必须要进行数据增广，扩充数据集。数据增广是深度学习中常见的技巧，它与数据增强不同，数据增广的核心目的是通过各种方法扩充数据集，增加数据的数量，而数据增强是对数据进行去除数据本身噪声、去除标签噪声等方法来增强数据的质量。数据增广在计算机视觉（Computer Vision,CV）领域使用的方法包括标准数据增广，如旋转、裁剪和缩放等等，图像的镜像处理，像素变化处理，噪声注入处理和模型变换处理等。 本节实验采用的数据增广方法主要为2种，一种是随机裁剪，使用的函数为transforms.RandomResizedCrop（），其功能为将给定图像随机裁剪为不同的大小和长宽比，然后缩放裁剪得到的图像为指定的大小，该函数的输入参数中主要设定了指定尺寸为（224，224），随机裁剪的范围为（0.7，1.0）和缩放尺度为（9.0/16.0，16.0/9.0）。第二种是色彩的随机调整，函数为transforms.ColorJitter（），其功能为随机修改图像的亮度、对比度、饱和度和色调，各个参数均设定为0.5，即图像的各项色彩属性值随机变化在原图像的50%至150%之间。图3.16为原图像与经过数据增广后的图像的对比。 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  32  图3.16 原图像与经数据增广的图像对比 最后是对数据进行标准化处理。进行标准化处理前先要对图像数据进行格式转化，使用transforms.ToTensor（）函数将原始的PILImage格式转化为可被pytorch快速处理的张量类型，同时将输入数据的[W,H,C]格式转为[C,W,H]。数据标准化处理是指将数据进行处理，得到新的数据要符合正态分布，即均值为0，标准差为1，这样的数据会使模型更容易收敛。数据标准化处理使用的函数为transforms.Normalize（），该函数需要输入数据集的均值和标准差，因此需要编写Python脚本求得NMDA数据集的均值和标准差，最终得到的NMDA数据集训练集均值为[0.4148134,0.4065143,0.4115489]，标准差为[0.27508628, 0.2553274, 0.32647355]，测试集均值为[0.43159482,0.42250603,0.42940098],标准差为[0.2713834,0.24952935,0.22935694]。 3.3.2 参数设置及相关函数选择 在进行一个深度神经网络模型的训练前首先要确定batchsize和epochs的大小，batchsize是指一次模型训练中输入的数据样本数量，epochs是训练模型的次数，合适的batchsize大小有助于加快模型的训练，减小训练时的loss震荡，而epochs无需考虑过大， epochs过大会严重影响训练时间。经过测试及训练环境GPU的显存限制，将batchsize定于32，epochs定为50。 由于新模型的特征提取模块利用了MobileNet-V3的features部分，因此可以利用迁移学南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  33 习的思想将MobileNet-V3已经训练完善的参数运用到新模型中，以此获得更好的初始参数，以减少训练时间，因此新模型中的特征提取模块加载MobileNet-V3的features参数，其他部分随机初始化。 模型的损失函数选用交叉熵函数（CrossEntropyLoss,CE），该函数通用适应在多分类模型中，在一个batch中，n分类模型的交叉熵损失函数计算公式为3.5。 𝐿𝑜𝑠𝑠=−1𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒∑∑𝑦𝑗𝑖log𝑦𝑗𝑖̂𝑛𝑖=1𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒𝑗=1               （3.5） 其中𝑦𝑗𝑖为真实结果，𝑦𝑗𝑖̂为模型预测的结果。 过拟合现象是小规模数据集训练模型时不可避免出现的现象之一，除了在特征提取模块中使用Dropout层外，正则化中的L2正则化也是有效的抑制过拟合的方法之一。正则化抑制过拟合的方法是降低神经网络在训练过程中的复杂度，Dropout层也属于正则化的方法之一。正则化的本质是对模型参数过多的惩罚，通常的惩罚方式是在损失函数后面添加一个正则项，L2正则化对应着L2范数，如公式（3.6）。 ||𝑎||=√∑𝑤𝑖2𝑛𝑖                           （3.6） 其中的𝑤𝑖是损失函数的输入参数，而加入L2正则项的损失函数就可以表示为公式（3.7）。 𝐿(𝑤)=𝐿(𝑤)+𝜆√∑𝑤𝑖2𝑛𝑖                          （3.7） 其中的𝜆为控制正则项大小的参数，越大则对参数的抑制程度越大，在本实验中𝜆的取值为0.001。 模型的训练需要优化器进行更新和计算模型参数，使得模型的输出逼近或达到最优值，从而最大化或最小化损失函数。本实验采用的优化器为Adam，是2014提出的一种通用型优化器，使用简便，梯度下降速度快。优化器中需要设定的参数为学习率（Learning Rate，LR），它可以确定每次迭代的步长，使损失函数收敛到最小值。过小的学习率收敛速度慢，训练时间过长，而过大的学习率收敛速度过快，容易跨过最小值而没有达到收敛效果，经过预先实验测试，确认学习率设定为1e-3。 由于深度神经网络是在一个未知、巨大且复杂的函数上进行训练的，训练的目的是寻找到最优的函数参数解。训练的过程是需要多次进行迭代，迭代的次数也就是epochs，迭代次数有限，而参数更新的速度，即模型学习的速度是固定的，由学习率控制。在训练初期，模型需要迭代的步长比较大，以此迅速达到最优解附近，而训练后期达到最优解附近后，又需要迭代的步长缩小，便于找到最优解。因此本实验设置了动态调整学习的方案，具体方法是使用了Pytorch框架里的lr_scheduler.ReduceLROnPlateau函数，该函数观测测试集的loss或南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  34 acc（准确率）的变化而改变学习，本实验采用了观测loss的方法，观测loss是否连续3个epoch都没有下降，如果是的话就将学习率缩小0.5倍，即变为原先学习率的一半。  比较与讨论 针对本章提出的基于多视图学习的分心驾驶识别模型MMobNet，将其在NMDA数据集上进行实验，下面将从不同的角度详细介绍。 3.4.1 MMobNet模型与单视图模型对比 NMDA数据集是由四个视角驾驶员驾驶行为图像构成的数据集，每个视角可以单独构成单视角数据集，本文将使用MobileNet-V3模型作为单视图模型与使用了相同特征提取模块的MMobNet做对比，相关模型训练参数一致。实验如图3.17所示，其中single-1、single-2、single-3和single-4为MobileNet-V3模型在视角1、2、3、4的数据集上分别训练得到的测试集准确率，single-1234是把四个视角的图像放在一起进行单视角模型训练得到的准确率，最后的MMobNet是MMobNet模型在4视角数据集中训练得到的准确率。  图3.17 多视图模型与单视图模型对比结果 通过图3.17显示，四个视角的单视图模型的准确率低于多视图模型MMobNet10%以上，由此证明多视图学习机制对于分心驾驶的图像识别是有相当大的效果的，此外可以看出视角2，即驾驶员右侧方的图像识别效果是四个视角中最高的，说明视角2是分心驾驶图像识别主要的特征视角。 南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  35 3.4.2 MMobNet模型与多视图模型对比 MMobNet模型的基础框架为MVCNN，但由于MVCNN的特征融合模块不能将各个视图特征更好地结合，因此本文提出了VAM模块，从而提出了多视图分心驾驶行为识别模型MMobNet。以下通过实验进行MVCNN、GVCNN和MMobNet在NMDA数据集上的对比。结果如图3.18所示，其中MV-vgg16、MV-alexnet、MV-resnet34和MV-resnet50是MVCNN提出文献[8]中所使用的MVCNN模型版本，后面的vgg16等为MVCNN使用的特征提取模块。而MV-mobilenetv3是使用MobileNetV3作为特征提取模块的MVCNN，以此与同样使用MobileNetV3作为特征提取模块的MMobNet作比较。GV-inceptionv4是GVCNN的模型版本。  图3.18 MMobNet模型与多视图模型对比结果  从图3.8可以看出，MMobNet在同类型的多视图模型中，分心驾驶行为识别的准确率位列第一，比同样使用MobileNetV3作为特征提取模块的MVCNN高出1.94%，由此证明MMobNet模型的特征融合模块VAM在分心驾驶行为识别任务上具有一定优势。 3.4.3 MMobNet模型视角选择对比 MMobNet是多视图模型，可以通过调整数据输入的视图数量提高或降低模型的识别效果。前文实验采用的视图数为4，将NMDA数据集的全部图像数据都利用到。由图3.17可以看出，各个视角的图像数据不是均等的提供特征，其中视角2的特征效果最为明显，视角1其次，而视角3和视角4提供的特征效果就比较差，因此可以通过MMobNet的视图数量和提供视图特征的视角组合的对比实验得到最为有效的视角组合和视图数量，最终达到使用较低数据南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  36 规模和提高模型识别效果的目的。 NMDA数据集采集了4个视角的分心驾驶行为图像数据，分别为视角1、视角2、视角3和视角4，通过组合得到6种两视图数据集和4种三视图数据集，分别将MMobNet送入这些数据集进行训练，最终得到的结果如表3.4所示，其中数据规模是以四视图的数据集作为100%标准，两视图和三视图的数据规模分别为四视图数据的50%和75%。 表3.2 MMobNet视角对比实验结果 视角数 数据规模 视角选择 准确率 2 50% 1、2 83.01% 1、3 69.00% 1、4 70.21% 2、3 79.56% 2、4 80.13% 3、4 66.26% 3 75% 1、2、3 87.52% 1、2、4 85.74% 1、3、4 72.64% 2、3、4 87.70% 4 100% 1、2、3、4 88.76% 由表3.4可知，使用四个视角的MMobNet模型识别准确率最高，为88.76%，使用视角2、3、4三个视角的的准确率居于第二，为87.70%，然后是使用视角1、2、3三个视角的准确率位列第三，为87.52%。此外使用两个视角作为数据集的实验中，使用视角1、2和视角2、4的准确率也高于80%，分别为83.01%和80.13%。由此可见视角2的视图特征是最重要的，脱离了视角2，分心驾驶的识别准确率显著下降。 同时可以看出视角1和视角4两个视图特征的重叠程度很高，使用三视角的实验中，使用视角1、2、3和视角2、3、4的准确率极为接近，仅相差0.18%，而同时使用了视角1和4的1、2、4准确率较前两者分别下降了1.78%和1.96%。此外可以通过使用视角1、2和视角2、4的准确率对比得知，视角1的视图特征优于视角4。 最后可以明确，使用视角1、2、3的MMobNet模型准确率较使用四视角数据集，数据规模降低了25%，但准确率却未大幅降低，仅仅降低了1.24%。该部分实验证明了视角1、2、3各自的视图特征具有互补性，可以大幅度提高分心驾驶行为的识别准确率。  本章小结 本章介绍了多视图驾驶行为图像数据集NMDA数据集和多视图分心驾驶行为识别模型南京邮电大学专业学位硕士研究生学位论文 第三章 基于多视图学习的分心驾驶行为识别研究  37 MMobNet。NMDA数据集通过数据标签的设计，可以形成多个视角组合的数据集，以适应不同视角需求的模型，同时完成了数据集的有效性证明实验，该实验证明了NMDA数据集可以应用于分心驾驶行为识别任务，由其训练得到的模型具有可靠性和严谨性。在多视图模型设计过程中，着重考虑了视图之间的特征区别，以注意力机制作为理论基础，设计了VAM模块，经过与同类型的多视图模型对比实验可知，VAM模块在分心驾驶行为识别任务上相对其他多视图模型的特征融合策略具有一定优势。最后通过MMobNet选取不同视角组合进行实验，确认了视角1、2、3为最佳视角组合，在降低数据需求规模的同时没有大幅度降低模型性能。南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  38 第四章  基于多任务学习的联合驾驶行为识别研究 第三章针对分心驾驶识别进行了介绍与研究，并对提出的MMobNet多视图分心驾驶分类模型与单视图模型、多视图模型进行实验对比。本章主要对驾驶行为中的疲劳驾驶行为识别进行介绍与研究，通过多任务学习的机制将上章提出的MMobNet模型和疲劳驾驶特征识别模型进行参数共享，在减少计算成本的情况下完成分心驾驶行为识别和疲劳驾驶特征识别两个任务，为第五章分心驾驶和疲劳驾驶的驾驶行为检测系统的设计提供坚实基础。  疲劳驾驶行为识别方案研究 对于疲劳驾驶行为识别任务，本文选取了人脸疲劳特征中的人眼闭合和嘴巴打哈欠进行疲劳驾驶的识别，主要选取理由为人眼闭合与嘴巴打哈欠的图像特征明显，且与分心驾驶的图像识别技术方向相一致，便于进行多任务识别。 4.1.1 疲劳驾驶行为图像数据集 疲劳驾驶行为图像数据在NMDA数据集中已经进行了采集，其中包含疲劳驾驶行为中的闭眼和打哈欠两种类别，因此本文将采用NMDA数据集中的01正常驾驶、13闭眼驾驶、14打哈欠驾驶三个类别组成疲劳驾驶行为图像数据集（NJUPT Fatigue Driving Action Dataset，NFDA），如图4.1所示。  图4.1 疲劳驾驶行为图像数据集 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  39 其中正常驾驶训练集4288张、测试集1132张，闭眼驾驶训练集2940张、测试集356张，打哈欠驾驶训练集3168张、测试集484张。 4.1.2 人脸检测算法研究 疲劳驾驶特征的闭眼和打哈欠需要人脸检测器对采集的疲劳驾驶行为图像进行人脸检测，检测到人脸脸部区域后，将其从原始图像上划分出来，再使用驾驶疲劳特征识别模型进行疲劳驾驶识别。基于此流程，本文选取了旷视科技研究院的研究团队于2019年提出的RetinaFace[76]作为提取人脸眼部和嘴巴区域图像的人脸检测器。RetinaFace属于InsightFace项目的一部分，InsightFace项目是一个开源的2D和3D深度学习人脸分析库，包含了人脸识别、人脸检测和人脸对齐的相关算法和深度神经网络模型，而RetinaFace是针对2D图像进行人脸检测的CNN类多任务目标检测器。 RetinaFace是一种单极人脸检测器，基于传统物体检测模型RetinaNet改进而成，利用自监督(Self-Supervised Learning)和额外监督结合（extra supervision learning）的多任务学习，对不同尺寸的人脸进行定位，划分出人脸的上下左右四个边框和双眼、鼻子和嘴巴两侧五个人脸关键点。 RetinaFace的算法流程如图4.2所示。  图4.2 RetinaFace算法流程 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  40 其属于多任务学习，利用主干网络进行特征提取，然后对提取的特征图进行特征融合和特征强化，最后使用RetinaHead的三个子检测，分别是类别检测、框架检测和关键点检测三个检测头进行多任务输出。以下将针对RetinaFace的各个关键技术进行研究，并进行训练以获得检测率较高的RetinaFace模型。 （1）主干网络 Retinafce的主干网络在原论文[76]中有多种，本文选取了MobilenetV1-0.25作为RetinaFace人脸检测器的特征提取网络，由于RetinaFace模型属于多任务模型，因此主干网络也是该多任务模型的共享参数层，类检测、框架检测和关键点检测三个检测头都针对该网络提取的特征进行检测。Mobilenet在本文介绍较多，V1版本使用的主要技术为深度可分离卷积，属于轻量化深度神经网络模型，而MobilenetV1-0.25模型则更为轻量，0.25指该模型的各层通道数为MobilenetV1模型的1/4。 RetinaFace不止需要主干网络最后的输出特征图，对于中间层产生的特征图也需要，如图4.3所示，RetinaFace需要第三个block、第四个block和最后一个block输出的特征图，为便于说明，将它们成为c3特征图，c4特征图和c5特征图。  图4.3 MobilenetV1-0.25主干网络结构 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  41 （2）特征金字塔 特征金字塔（Feature Pyramid Networks，FPN）[77]是2016年提出的应用于目标检测的一种结构，该结构对主干网络提取的3个特征进行特征融合。主要流程如图4.4所示，FPN结构对通道数最多的，也就是c5特征图进行上采样，即缩小通道数，扩大特征图，属于下采样的逆运算。上采样至尺寸与c4特征图一致，将两个特征图进行元素相加，再使用一层卷积层进行特征图融合，形成p4特征图，类似于p4特征图的操作，将p4特征图进行上采样，缩至与C3特征图尺寸一致，同样进行元素相加与卷积操作，形成p3特征图。最后该结构将特征图c5进行上采样后的特征图p5与特征图p4、特征图p3送入下结构，完成本结构的特征融合任务。  图4.4 RetinaFace的FPN结构 （3）SSH SSH[78]是人脸检测算法中较为常用的模块，全称为单极检测器（Single Stage Headless），是ICCV2017中论文提出的针对人脸检测的算法模块，在RetinaFace中SSH具体结构如图4.5所示，使用三个并行结构，并利用3x3尺寸卷积核的堆叠替代5x5和7x7尺寸的卷积核，类似GoogLeNet中的Inception结构。  图4.5 RetinaFace的SSH结构 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  42 （4）RetinaHead RetinaHead是RetinaFace模型的检测部分，分为三个检测头，ClassHead、BoxHead和LandmarkHead，如图4.6所示。在使用RetinaHead进行检测前需要先设置先验框（Anchors），先验框是目标检测模型中使用的概念。先验框是提前在需要检测的特征图上预制目标区域的框，例如在RetinaFace中对p3、p4、p5三个特征图的每个点位绘制两个不同尺寸的框，每个特征图因为特征提取深度的不同，其绘制的框尺寸都不一样。先验框的作用是让检测网络只需要对每个先验框进行位置和大小的参数调整就可以完成目标检测任务。ClassHead是对每个先验框是否存在人脸输出概率，BoxHead是对每个先验框进行中心位置和高宽的调整，而LandmarkHead则是对先验框内预测的五个人脸关键点进行各个位置的调整。  图4.6 RetinaHead结构，引用自文献[1] （5）非极大抑制 非极大抑制（non_max_supprssion，NMS）是目标检测常用的算法，在使用NMS前，首先需要提出交并比（Intersection over Union，IoU）这个定位精度评价公式。IoU表示了两个框的重叠度，即两个框共同占有的区域占两个框所有区域的面积比例，如图4.7和公式4.1所示，其中SI为A和B共同占有面积，SA为A占有面积，SB为B占有面积。  图4.7 IoU概念示意图 𝐼𝑜𝑈=𝑆𝐼/(𝑆𝐴+𝑆𝐵−𝑆𝐼)                       （4.1） 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  43 而NMS则是对多个框进行筛选的算法。在RetinaFace中首先依据ClassHead和BboxHead得到经过调整的先验框和其存在人脸的概率，进行概率从高到低的排序，然后对最高的一个先验框与其他框进行IoU计算，剔除重叠较高的框，最后对未处理的框依次进行重复操作，以此得到多个存在人脸概率较高且不重叠的先验框，具体效果如图4.8所示。  图4.8 NMS算法效果示意图 （5）训练与疲劳特征区域采集 RetinaFace训练所需要的数据集选自Widerface数据集[79]，Widerface数据集是人脸检测的基准数据集，可以使用它进行模型训练，也可进行模型性能对比，本文使用的Widerface数据集训练集12880张图片，测试集3226张图片。使用的优化器为Adam，总损失值为三个子任务损失函数之和，针对ClassHead输出人脸存在概率使用的是交叉熵损失函数，而对于BoxHead和LandmarkHead返回的先验框位置参数和人脸关键点参数使用的是回归损失函数。进行200次迭代训练，其总loss下降过程显示为图4.9。  图4.9 RetinaFace训练过程 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  44  然后进行Widerface数据集的模型性能评估，得到的平均精度（average precision，AP）如表4.1所示，其中RetinaFace的数据为文献[76]中的指标，Easy、Medium和Hard是Widerface数据集的三个图像类型，依据图像人脸检测的难度划分。平均精度是多标签分类中衡量分类结果的一种评价指标，原论文的性能要高于本文实现的RetinaFace，主要原因是本文并没有完全的RetinaFace实现数据与模型调参过程，部分模型结构不能完全复现，且本文核心重点并不在人脸检测环节，因此本文实现的RetinaFace性能已经能够实现疲劳驾驶识别任务中人脸检测任务。 表4.1 RetinaFace训练结果 数据集 RetinaFace Ours Easy 95.532 89.563 Medium 95.134 88.852 Hard 90.714 75.794  得到检测效果较高的RetianFace人脸检测模型后，将NFDA数据集送入RetinaFace得到人脸区域图像，再把得到的图像划分为上下两部分，分别建立眼部区域图像和嘴巴区域图像，最终形成基于眼部和嘴部的疲劳驾驶特征数据集（Eyes and Mouth Fatigue Driving Characteristics Dataset,EMFD），如图4.10所示。  图4.10 EMFD数据集 4.1.3 疲劳特征识别研究 由于本文采用的疲劳驾驶特征为闭眼特征和打哈欠特征，并且针对两个特征采集了南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  45 EMFD数据集，再加上为便于与分心驾驶行为识别模型MMobNet进行多任务学习，因此设计了基于MobileNetV3的闭眼和打哈欠识别模型MEMNet。 （1）模型设计 MEMNet的特征提取部分使用了MobileNetV3的特征提取模块，以此实现与MMobNet共享特征提取参数的目的，在分类模块部分与MMobNet不同，并没有使用多视图学习，因为不是每个视角都能成功检测到人脸，以此进行人眼和嘴巴的疲劳驾驶特征识别，而是单独重新设计了分类模块，如图4.11所示。  图4.11 MEMNet结构   （2）模型训练与对比 MEMNet模型的训练数据集采用EMFD数据集，优化器和损失函数分别是Adam和CrossEntropyLoss。经过训练与类似疲劳特征识别模型对比如图4.12所示，其中EM-Net是文献[80]提出的眼部和嘴部疲劳特征识别模型,ESR-Net和MSR-Net是文献[81]提出的分别针对眼部和嘴部疲劳特征进行识别的模型，而VGGDP-Net是文献[82]提出的眼部和嘴部疲劳特征识别模型。 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  46  图4.12 疲劳特征识别模型对比 由图4.12可知，本文提出的MEMNet在EMFD数据集训练得到的准确率最高，分析原因在于其他模型的输入眼部和嘴部图像都为正面图像，即视角1，而EMFD数据集是基于多视角NMDA数据集得到的，所以该数据集中的眼部和嘴部图像除驾驶员正面视角外还包含其他视角，因此MEMNet在EMFD数据集中较其他同类型模型性能更加优越。  多任务模型设计研究 为实现分心驾驶和疲劳驾驶的联合驾驶行为识别，需要基于多任务学习，设计多任务模型，因此本文提出了分心驾驶行为识别和疲劳驾驶特征识别的联合多任务模型MTDFNet。除了实现分心驾驶和疲劳驾驶的联合识别外，多任务学习还可以对相关联的任务实现降过拟合、提高一定准确率的效果。 4.2.1 模型结构 多任务模型的框架分为基于参数的共享和基于约束的共享，本文多任务模型结构采用基于参数的共享，使用前文提出的MMobNet和MEMNet两个模型进行多任务联合，实现分心驾驶行为识别和疲劳驾驶特征的识别。由于MEMNet仅针对静态的闭眼和打哈欠图像进行识别，因此本节提出的多任务模型并不能实现真正的分心驾驶和疲劳驾驶联合识别，其将于第五章通过对疲劳驾驶判定进行策略设计，在视频数据集上实现分心驾驶和疲劳驾驶的联合识别。本节提出的多任务模型MTDFNet的模型结构如图4.13所示，将MMobNet和MEMNet的特征提取模块进行共享，通过对模型数据输入层添加一个任务标志值task_id确认提取的特征图进入到各自任务的后续模块中，如task_id为0表示模型处理的是疲劳驾驶特征数据，而南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  47 task_id为1表示模型处理的是分心驾驶数据。  图4.13 MTDFNet模型结构  4.2.2 模型训练参数处理 在本文的多任务模型训练的数据预处理中，对分心驾驶行为识别子任务的数据预处理多了一步数据增强工作。由于两个子任务尽管具有高任务关联性，但分心驾驶行为识别任务的分类数为12个，而疲劳驾驶特征识别任务的分类数为3个，分心驾驶行为识别要求难度远远高于疲劳驾驶特征识别，因此分心驾驶行为识别在多任务模型训练前需要添加数据增强工作，提高数据的泛化性，以匹配疲劳驾驶特征识别任务难度，有效实现多任务模型提高高关联性任务性能的特点。 本文在分心驾驶行为识别子任务的数据集上使用的数据增强为AutoAugment[83]，AutoAugment是Google提出的无监督自动数据增强算法，它包含种图像增强方法，使用强化学习方法进行搜索合适的方法组合以更好地增强原始数据。 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  48 在数据预处理环节外，多任务模型的核心在于各个子任务模型的参数优化策略，这关键在于优化器和损失函数。优化器是对整个多任务模型进行参数更新，不再单独影响各个子任务模型，因而只需要一个优化器。损失函数则不同，多任务模型的结构就显示了与单任务模型的不同，它具有多个损失函数和多个loss值，每个子任务的输出都会进行损失函数的计算，确认预测值与真实值的差距，此外因为模型只有一个优化器，多个loss值需要进行处理形成一个总loss进行后向传播。因此对于MTDFNet模型需要考虑的是两个子任务的损失函数选择和各自损失函数输出loss的组合策略。 （1）子模型的损失函数选择 原先MMobNet和MEMNet训练中使用的损失函数均为CrossEntropyLoss，而结合形成的MTDFNet经过预先测试时发现各自子任务输出的loss震荡明显且在个别类别准确率较低，因此分心驾驶行为分类子任务的损失函数将使用具有样本类别权重增益功能的损失函数Focalloss[84]。 Focalloss是著名深度学习学者何恺明（Kaiming）于2017年提出的，其原先应用于目标检测领域的正负样本数量极不平衡的问题，后来也逐渐应用于多分类模型,处理样本类别难易区别明显的问题。 Focalloss是基于交叉熵函数（Cross Entropy Loss,CE）改进而成的，交叉熵函数针对一个loss的计算可以简化为公式4.2。 𝑙𝑜𝑠𝑠𝑐𝑒=−∑𝑦𝑗𝑖log𝑦𝑗𝑖̂𝑛𝑖=1=−(1−𝑝𝑐)log𝑝𝑐                （4.2） 其中𝑝𝑐为c类样本属于正确分类的概率，𝑝𝑐值越大，说明c类样本越容易分类。而Focalloss的改进公式为公式4.3。 𝑙𝑜𝑠𝑠𝑓𝑙=−𝛼𝑐(1−𝑝𝑐)𝛾log𝑝𝑐                      （4.3） 其中的𝛼𝑐又称权重参数，控制c类样本对总loss的共享权重，即𝛼𝑐值越大c类样本的loss权重越大，通常c类样本越难以分类𝛼𝑐越大。而𝛾称为调制系数（modulating factor），其控制总样本中难分类样本的权重，𝛾值越大，难分类样本的loss权重越大。 因此𝛼控制各个样本loss的权重，输入的是一个向量，元素数量为分类数。而𝛾控制所有难分类样本loss的权重，输入的是一个整数。𝛼和𝛾的组合需要经过多次实验得到最好的模型训练效果。 （2）loss的组合策略 多任务模型中的loss组合最简单的策略是静态的直接相加，但简单的相加并不能达到稳定、有效的loss，本文为MTDFNet的训练选取了动态权重平均算法[85]（Dynamic Weight 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  49 Averaging，DWA），DWA算法的核心公式为4.4和4.5。    𝑟𝑛(𝑡−1)=𝑙𝑛(𝑡−1)𝑙𝑛(𝑡−2)                            （4.4）                                                       𝜔𝑖(𝑡)=𝑁𝑒𝑥𝑝(𝑟𝑖(𝑡−1)𝑇)∑exp (𝑟𝑛(𝑡−1)𝑇)𝑛                            （4.5） 𝑟𝑛(𝑡−1)是任务𝑛第𝑡−1个epoch的loss下降速度，越小表示下降速度越快，𝑙𝑛为任务𝑛的损失函数。而𝜔𝑖(𝑡)表示任务𝑖在第𝑡个eopch的loss权重，它是上个epoch的该任务训练速度占总训练速度的比值，其中的T是一个常数，当T=1时相对于对训练速度归一化处理，即进行softmax，而T足够大时，各个任务的𝜔趋近1，即权重相等，相对于loss直接相加。而在公式4.5中的𝑒𝑥𝑝是对𝑟𝑛(𝑡−1)和𝑟𝑖(𝑡−1)的映射，具体映射方式是在每个batch计算中更新loss的权重时把权重值乘以当前已训练的batch与总batch数的比值。  比较与讨论 针对本章提出的多任务模型MTDFNet，基于第三章实验结果将使用NMDA数据集中的视角1、2、3形成的多视角分心驾驶行为图像数据集和疲劳驾驶特征数据集EMFD数据集进行下面的实验，下面将从不同的角度详细介绍。 4.3.1 损失函数选择对比 在进行损失函数的选择前，首先需要对介绍一下通常用于图像分类任务性能评估的标准格式，混淆矩阵（Confusion Matrix）。混淆矩阵主要用于反映分类任务各个类别的识别准确率，以此全面的反映模型对各个类别进行分类的效果。  图4.14 使用CE训练得到的混淆矩阵 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  50 如图4.14所示，该图是MTDFNet中分心驾驶行为识别子任务中使用交叉熵损失函数（Cross Entropy Loss，CE）得到的混淆矩阵。可以看出在06、 07和01三个类别的准确率较其他类别有显著差距。由于Focalloss可以通过设置𝛼和𝛾两个参数解决分类任务中个别类别准确率有差异的问题，因此在分心驾驶行为识别子任务使用Focalloss作为损失函数，设置不同的𝛼和𝛾组合进行对比实验，以此得到较为优异的训练效果，结果如表4.2所示，其中𝛼是一组向量，分别代表各个类别的loss权重，𝛾是整数，控制所有难分类类别的loss权重，Hard-1为所有类别中准确率最低的类别及其准确率，Hard-2和Hard-3同理，分别为准确率倒数第二和第三的类别及其准确率，ACC为使用该组𝛼和𝛾训练得到的模型平均准确率。 表4.2 𝛼和𝛾组合对比实验结果 α γ Hard-1 Hard-2 Hard-3 ACC [1,1,1,1,1,1,1,1,1,1,1,1] 2 06 53 01 68 11 68 83.41 [1.5,1,1,1,1,1,1.5,1,1,1,1.5,1] 2 06 65 09 74 01 75 85.33 [1.5,1,1,1,1,2,1.5,1,1.5,1,1,1] 2 01 69 07 71 09 72 83.95 [1.5,1,1,1,1,2,1.5,1,1.5,1,1,1] 3 07 70 06 71 09 78 86.77 [1.5,1,1,1,1,2,2,1,2,1,1,1] 3 06 72 07 77 01 83 88.16 [1.5,1,1,1,1,2,2,1,2,1,1,1] 4 07 70 01 78 09 79 86.67 [1.5,1,1,1,1,2,2,1,2,1,1,1] 5 06 59 09 63 11 69 81.82 由表4.2可知𝛼和𝛾的值不是越大越好，只有通过实验匹配对应模型和数据集的才能得到较为 优异的组合。最终确定的𝛼为[1.5,1,1,1,1,2,2,1,2,1,1,1]，𝛾为3，最终模型准确率为88.16%，已经高于对应的单任务模型MMobNet0.64%。 4.3.2 损失函数权重对比 目前的多任务模型的整体loss计算通常为线性相加，例如人脸检测模型RetinaFace训练时使用的方案就是3个loss值直接线性相加。而在本文的分心驾驶行为识别和疲劳特征识别两个子任务的难度不同，各自的分类类别为12和3，因此单纯的线性相加并不满足MTDFNet的训练需求。因此本文采用了DWA算法，该方法与常规的静态线性相加对比实验如表4.3所示。 表4.3 DWA算法与线性相加算法对比结果 loss组合方案 a_loss b_loss a_acc b_acc a+b 0.059  0.102  91.67  88.16  0.5*a+0.5*b 0.088  0.165  90.75  85.90  0.3*a+0.7*b 0.067  0.099  91.50  89.15  0.1*a+0.9*b 0.152  0.124  82.53  86.15  a+2*b 0.135  0.089  84.77  88.49  DWA 0.036  0.070  93.30  91.22   南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  51 其中loss组合方案中，a为疲劳特征识别子任务的输出loss，b为分心驾驶行为识别子任务的输出loss，a_acc和b_acc分别为多任务模型MTDFNet的两个子任务的准确率。 由表4.2可知，DWA算法已经优于线性相加的算法，在维持较为容易的疲劳特征识别子任务的准确率的同时提高了较为困难的分心驾驶行为识别子任务，与最好的线性相加结果相比，提高了2.07%。 4.3.3 多任务模型与单任务模型对比 前文的实验中已经得到了准确度最高的分心驾驶行为与疲劳驾驶疲劳特征联合识别多任务模型MTDFNet，与对应的单任务模型MMobNet和MEMNet进行的对比实验如图4.14所示。  图4.15 多任务模型子任务与对应单任务模型对比 由图4.15可以看出，多任务模型MTDFNet相对于两个单任务模型MMobNet和MEMNet并没有全部提升，分心驾驶行为识别在多任务模型中提升了3.7%，而疲劳驾驶特征识别则降低了0.47%，其中分心驾驶行为识别是较为困难的，其具有12个分类，而疲劳驾驶特征识别则较为容易，只有3个分类，因此可以将分心驾驶行为识别作为主任务，疲劳驾驶特征识别作为副任务，副任务以略微的代价有效提高了主任务的准确率，证明了多任务模型在实现分心驾驶行为与疲劳驾驶疲劳特征联合识别任务中既完成了两个子任务的联合识别，又保证了两个子任务的准确率相对较高。 南京邮电大学专业学位硕士研究生学位论文 第四章 基于多任务学习的联合驾驶行为识别研究  52  本章小结 本章主要就疲劳驾驶行为识别方案进行了深入研究，确认了具体方法为使用人脸检测模型RetinaFace采集驾驶员人脸图像，将人脸图像分为上下部分作为眼部特征区域和嘴部特征区域进行疲劳特征识别，基于分心驾驶行为识别模型MMobNet建立了相同特征提取模块的MEMNet，最终基于多任务学习，将分心驾驶行为识别和疲劳驾驶疲劳特征识别两个任务联合，建立多任务模型MTDFNet，并完成了相关实验，最终确认使用Focalloss和DWA两种算法进行模型的训练，得以有效提高了多任务模型的准确率。分心驾驶行为和疲劳特征联合识别的多任务模型MTDFNet与对应单任务模型对比，确认了在略微降低较容易任务准确率的代价下可以有效提高较为困难的任务准确率。南京邮电大学专业学位硕士研究生学位论文 第五章 驾驶行为检测系统设计与实现  53 第五章  驾驶行为检测系统设计与实现  系统结构 图5.1显示了驾驶行为检测系统的整体结构，主要包括多个视角的数据获取处理和使用RetinaFace人脸检测器获取的人脸区域数据处理，多任务模型MTDFNet对送入的多视角图像数据和人脸区域数据进行行为识别工作，以及显示数据内容的可视化界面。  图5.1 系统整体结构  疲劳驾驶行为检测方案 MTDFNet模型中的疲劳驾驶检测任务只是针对静态图像中的驾驶员是否闭眼和打哈欠进行了识别分类，而在实际应用中，拍摄驾驶员的摄像头完全有可能会在驾驶员眨眼和张嘴时采集图像数据，从而模型将目前正在正常驾驶的情况分类为疲劳驾驶，造成不必要的错误。因此疲劳驾驶的识别是动态的判定，是基于模型在一定时间内不断识别到闭眼和打哈欠的图像，从而真正准确的识别到驾驶员的疲劳驾驶行为。所以需要针对疲劳驾驶特征进行疲劳特征判定疲劳驾驶的策略设定，在实际的视频数据中得到有效验证。 南京邮电大学专业学位硕士研究生学位论文 第五章 驾驶行为检测系统设计与实现  54 5.2.1 眼睛疲劳特征判定策略 眼部图像在疲劳驾驶识别领域是最为常用的特征数据，人在疲劳时会发生不自觉的长时间闭眼行为，而在城市公共交通道路上，2秒的行驶时间往往汽车已经前进十几米开外，如果驾驶员在这个时间内持续闭眼，很容易发生安全事故。因此人眼的闭合特征是疲劳驾驶主要的评判准则，在人眼闭合的疲劳驾驶特征上，据相关论文与商业界实际使用情况，主要是两个标准，一是持续闭眼时间，二是PERCLOS准则。 持续闭眼时间通常是指驾驶员从开始闭眼的那一帧到重新睁开眼睛的那一帧，两帧时间之差，如公式5.1所示。 𝐸𝑡=𝐸𝑒𝑛𝑑−𝐸𝑠𝑡𝑎𝑟𝑡                            （5.1） 𝐸𝑡为持续闭眼的时间，该时间需要与正常的眨眼时间区别。正常人每一分钟需要眨眼10至20次，每一次眨眼的时间约为0.2至0.4秒，因此本文将疲劳驾驶的持续闭眼时间设置为1秒，如果持续1秒检测到的每一帧都在闭眼那表明驾驶员正在疲劳驾驶。 PERCLOS准则，全称为Percentage of EyeIid CIosure over the PupiI over Time，第一章的疲劳驾驶国内外研究介绍中有相关涉及。PERCLOS准则是美国卡内基梅隆研究所于1994年提出的度量疲劳或瞌睡的物理量，1999年美国联邦公路管理局（FHWA）和美国国家公路交通安全管理局（NHTSA）通过九种疲劳检测指标的对比实验，最终证明PERCLOS与疲劳驾驶的相关性最高。PERCLOS准则的测量参数为在单位时间内眼睛闭合程度超过某一闭值的时间占总时间的百分比。PERCLOS准则目前有三种标准，分别是P70、P80和EM。P70指眼睑遮住瞳孔的面积超过70%就视作为眼睛闭合，然后统计在一定时间内眼睛闭合所占时间比例。P80和EM同理。因此PERCLOS准则可以使用公式5.2来简易表示。 𝑃𝐸𝑅𝐶𝐿𝑂𝑆=眼睛闭合时间检测时间段×100%                     （5.2） 在本文中因为没有相关统计眼睑占据瞳孔百分比面积的算法以及对应数据集，因此采用完全闭眼的状态，即眼睑遮住瞳孔的面积为100%才视作为眼睛闭合，此外由于识别眼睛闭合的模型需要依据一帧帧图像进行识别工作，无法是连续的时间占比，因此本文的PERCLOS准则公式为公式5.3所示。 𝑃𝐸𝑅𝐶𝐿𝑂𝑆=∑𝐸𝑡𝑇𝑡=1𝑇𝐸×100%                     （5.3） 𝑇𝐸为检测图像总帧数，∑𝐸𝑡𝑇𝑡=1为检测到闭眼的总帧数。  由于人脸检测和疲劳特征识别两个模型不是百分百实现任务，实际的持续闭眼时间内，系统不一定在这个时间内识别的全都是闭眼，因此持续闭眼时间这个标准在实际使用中效果南京邮电大学专业学位硕士研究生学位论文 第五章 驾驶行为检测系统设计与实现  55 较差，本文将使用PERCLOS准则作为眼睛疲劳特征的判定策略。 5.2.2 嘴巴疲劳特征判定策略 人在疲劳时会发生打哈欠的行为，即频繁张嘴，正常人打哈欠的行为通常在5到10秒左右，且会发生长大嘴巴后缓慢闭合的动作。因此疲劳驾驶的嘴巴特征评判策略类似于眼部的疲劳特征评判策略，同样是采用张嘴持续时间和张嘴时间占检测时间比例两种策略进行疲劳驾驶判定，其中张嘴时间占检测时间比例可以用张嘴时间占比（Opening Mouth Ratio, OMR）来表示。张嘴持续时间公式如公式5.4所示。 𝑀𝑡=𝑀𝑒𝑛𝑑−𝑀𝑠𝑡𝑎𝑟𝑡                        （5.4） 𝑀𝑡为张嘴持续时间。而张嘴时间占比的公式如公式5.5所示。 𝑂𝑀𝑅=∑𝑀𝑡𝑇𝑡=1𝑇𝑀×100%                        （5.5） 𝑇𝑀为检测图像总帧数，∑𝑀𝑡𝑇𝑡=1为检测到张嘴的总帧数。  基于同样的原因，嘴巴疲劳特征判定策略将采用OMR标准。 5.2.3 疲劳驾驶检测测试                                                                                                                                                             通过对疲劳驾驶的眼睛与嘴巴疲劳特征设立判定策略，确定了相关函数公式，但各个公式的关键参数并没有设定，这些关键参数就是PERCLOS准则检测帧数和闭眼帧数占比上限𝑃𝐸𝑅𝐶𝐿𝑂𝑆𝑚𝑎𝑥和张嘴时间占比𝑂𝑀𝑅𝑚𝑎𝑥，只有确定了这些参数才能通过模型识别到的疲劳特征输入各个判定策略公式后确认是否属于疲劳驾驶。因此本文将利用NMDA数据集的初始版本视频数据集进行关键参数对比测试，以此得到较为合适的参数值。 使用的DMDA数据集初始版本为视频格式，总共1161个视频，时长均在8秒至40秒之间，其中模拟闭眼的视频84个，模拟打哈欠的视频84个。实验结果如表5.1所示。 表5.1 𝑃𝐸𝑅𝐶𝐿𝑂𝑆𝑚𝑎𝑥与𝑂𝑀𝑅𝑚𝑎𝑥参数设置对比测试结果 关键参数 参数值 检测视频总数 检测闭眼视频总数 检测打哈欠视频总数 疲劳检测率 非疲劳 检测率 平均准确率 PERCLOSmax 0.4 1077 68 \ 80.95 97.07 95.82 0.6 79 \ 94.05 98.19 97.86 OMRmax 0.4 1077 \ 63 75.00 95.87 94.24 0.6 \ 78 92.86 97.58 97.21 PERCLOSmax or OMRmax 0.6 1161 79 78 93.45  98.69  97.93   南京邮电大学专业学位硕士研究生学位论文 第五章 驾驶行为检测系统设计与实现  56 如表5.1可知，使用参数值均为0.6的𝑃𝐸𝑅𝐶𝐿𝑂𝑆𝑚𝑎𝑥和𝑂𝑀𝑅𝑚𝑎𝑥可以完成较为准确的疲劳驾驶行为识别任务，其准确率为97.93%，对疲劳视频的检测率为93.45%，对非疲劳驾驶行为的检测率为98.69%。本文使用的疲劳驾驶判定策略得到的测试准确率均高于多任务模型MTDFNet中对疲劳特征识别的准确率，证明该策略可应用于疲劳驾驶行为识别任务。  系统实现与测试 依据图5.1系统整体结构，本文于PC端实现了驾驶行为检测系统，由于该系统没有适配的车载边缘AI设备，且使用PC在汽车上进行路上测试与使用采集的视频进行测试没有实际区别，因此没有进行路试。首先使用PyQt5框架，建立可视化界面，如图5.2所示。然后通过PyQt5的Button.clicked.connect()函数将界面按钮与相关函数相关联，实现多视角视频的选择、播放、暂停和进行分类四种数据处理功能，并将视频通过cv2的cv2.VideoCapture()函数进行分帧处理，转化为多张图片，其中使用RetinaFace模型进行人脸检测采集脸部图像。最后原图像与上下脸部图像一起送入多任务驾驶行为识别模型MTDFNet内，实现分心驾驶行为识别和疲劳驾驶特征识别，并结合疲劳驾驶的疲劳特征判定策略，完成最终的驾驶行为检测，并将结果显示在可视化界面中。  图5.2 可视化界面 在进行分心驾驶行为与疲劳驾驶行为联合识别时，疲劳驾驶检测的优先度高于分心驾驶，主要原因是疲劳驾驶的疲劳特征判定策略中默认了分心驾驶行为是不存在长时间闭眼和长时间张嘴打哈欠，且实际环境中疲劳驾驶的危险性也远远高于分心驾驶。最终的驾驶行为检测系统的可视化界面展示如图5.3所示。 南京邮电大学专业学位硕士研究生学位论文 第五章 驾驶行为检测系统设计与实现  57  图5.3 系统测试显示  本章小结 本章以PyQt5作为系统可视化界面框架，实现了驾驶行为检测系统的开发。首先提出系统整体结构，然后针对多任务模型MTDFNet中对疲劳驾驶任务只完成疲劳特征识别工作的问题，提出了疲劳驾驶疲劳特征判定策略，最终提出了针对眼部疲劳特征判定的PERCLOS准则和针对嘴部疲劳特征判定的OMR标准，并通过实验得到了最佳的判断参数。最后展示了驾驶行为检测系统的可视化界面，并进行了系统测试，该系统的实现为驾驶安全领域提供了有效方法和可靠应用。南京邮电大学专业学位硕士研究生学位论文 第六章 总结与展望  58 第六章  总结与展望  总结 本文针对基于深度学习与计算机视觉的驾驶行为检测展开研究，论文主要包括多视角驾驶行为图像数据集NMDA数据集的建立及基于此数据集的多视图分心驾驶行为识别模型的研究、疲劳驾驶行为识别方案的研究及基于多任务学习的分心驾驶行为与疲劳特征识别联合识别模型的研究和驾驶行为检测系统的设计与实现三个部分。 在多视角驾驶行为图像数据集NMDA数据集的建立及基于此数据集的多视图分心驾驶行为识别模型的研究部分，首先是基于分心驾驶识别和疲劳驾驶识别两个任务的图像视角需求不同建立了NMDA数据集，该数据集包含正常驾驶类别和11个分心驾驶行为类别、2个疲劳驾驶行为类别，总计14个类别。并且NMDA数据集可以根据后续模型需求修改数据集的视角数量，形成不同的数据数量和视角组合的多视角驾驶行为图像数据集。然后针对分心驾驶行为和疲劳驾驶行为联合识别多个视角的需求，基于多视图学习，建立了多视图分心驾驶行为识别模型MMobNet，该模型框架来源于应用于3D物体识别领域的多视图模型MVCNN，但针对MVCNN没有分析各视图特征权重的问题，提出了视图注意力机制模块VAM，并应用于MMobNet。最后通过实验对比多视图模型MMobNet对比各种单视图模型准确率要高出10%以上，且使用了VAM模块的MMobNet准确率较于同类型的多视图模型MVCNN和GVCNN要高出1.02%和5.64%，此外针对不同视角数量和视角组合的数据，使用MMobNet进行实验对比得出，使用视角1、2、3的组合在降低数据需求规模的基础上实现了较高的识别准确率。 在疲劳驾驶行为识别方案的研究及基于多任务学习的分心驾驶行为与疲劳特征识别联合识别模型的研究部分，首先针对疲劳驾驶行为任务确认了先进行人脸检测模型采集疲劳驾驶特征最明显的人脸区域图像再继续疲劳特征识别的方案。使用RetinaFace作为人脸检测模型，对其关键技术算法进行了研究及模型的训练与评估，将RetinaFace采集得到的人脸图像分为上下眼部和嘴部两部分，形成新的疲劳特征数据集EMFD数据集，并基于此数据集建立疲劳特征识别模型MEMNet，与相关同类型模型进行对比实验，证明在EMFD数据集上MEMNet具有一定的优势。然后基于已经建立的分心驾驶行为识别模型MMobNet和疲劳特征识别模型MEMNet建立多任务模型MTDFNet，实现分心驾驶行为和疲劳特征联合识别，并针对多任务模型的训练确认了数据增强算法AutoAugment、损失函数Focalloss和动态权重平均算法南京邮电大学专业学位硕士研究生学位论文 第六章 总结与展望  59 DWA。最后针对Focalloss的参数设置进行对比实验得出了最优组合，训练得到的MTDFNet中分心驾驶行为识别子任务的准确率已经高于单任务模型MMobNet0.64%，针对loss的线性组合与DWA算法进行对比，得到了分心驾驶行为识别和疲劳特征识别两个子任务的最优准确率91.22%和93.30%，将多任务模型MTDFNet与对应的两个单任务模型进行对比实验，确认了在略微降低较容易任务准确率的代价下可以大幅提高较困难任务的准确率，以疲劳特征识别任务准确率降低0.47%的情况下分心驾驶行为识别任务准确率提高了3.7%。 在驾驶行为检测系统的设计与实现部分，首先提出了整个系统的结构，然后针对疲劳特征识别模型MEMNet确认了疲劳驾驶中的眼睛和嘴巴的疲劳特征判定策略，并对NMDA初始的视频数据集进行了测试，确认了策略的关键参数PERCLOS准则检测帧数和闭眼帧数占比上限𝑃𝐸𝑅𝐶𝐿𝑂𝑆𝑚𝑎𝑥与张嘴时间占比上限𝑂𝑀𝑅𝑚𝑎𝑥分别为0.6和0.6。最后基于PyQt5框架建立了可视化界面和包含该界面的驾驶行为检测系统，并展示了系统的初始状态和测试状态。 综上所述，本文将原先各自单独完成的分心驾驶行为识别和疲劳驾驶行为识别进行了联合，利用两个任务视角需求不同的特点建立了多视图分心驾驶行为识别模型， 并通过确认疲劳驾驶行为识别方案和疲劳驾驶的疲劳特征判断策略实现了疲劳驾驶行为识别，最后基于多任务学习将两者结合形成多任务模型。        展望 当前，对于驾驶行为识别的工作仍然处于多个任务分别进行研究的阶段，通过将驾驶行为细分为分心驾驶、疲劳驾驶、危险驾驶等等分别针对各个细分任务进行研究。但各个细分的驾驶行为其关联性较高，虽然各自的识别特征不一致，但可以通过多任务学习进一步统合各个细分任务，实现完整的驾驶行为识别。此外驾驶行为识别的数据来源种类基于目前新能源汽车的迅速发展也越来越多，因此在多任务学习的机制下，通过多个数据源建立融合多模态的多任务驾驶行为识别模型也属于一种较为有效且易于考虑的方案。因此，如果有机会的话，我会基于融合多模态的方案继续进行多任务驾驶行为识别的研究工作，为中国的道路驾驶安全贡献自己的一份力量。    南京邮电大学专业学位硕士研究生学位论文 参考文献  60 参考文献 [1]邱彬,王芳,刘万祥.中国汽车产业发展趋势分析[J].汽车工业研究,2022(01):2-9. [2] World Health Organization. (2020). Road Traffic Injuries. [Online].Available: https://www.who.int/news-room/fact-sheets/detail/road-traffic-injuries [3] Koesdwiady A, Soua R, Karray F. Recent Trends in Driver Safety Monitoring Systems: State of the Art and Challenges[J]. IEEE Transactions on Vehicular Technology, 2017:4550-4563. [4] Guettas A, Ayad S, Kazar O. Driver state monitoring system: A review[C]//Proceedings of the 4th International Conference on Big Data and Internet of Things. 2019: 1-7. [5] Alkinani M H, Khan W Z, Arshad Q. Detecting human driver inattentive and aggressive driving behavior using deep learning: Recent advances, requirements and open challenges[J]. Ieee Access, 2020, 8: 105008-105030. [6]Xing Y, Lv C, Wang H, et al. Driver activity recognition for intelligent vehicles: A deep learning approach[J]. IEEE transactions on Vehicular Technology, 2019, 68(6): 5379-5390. [7]曾诚,刘富佳,于潇,孟兴凯.国内外客车驾驶员疲劳驾驶预防管理政策比较[J].人类工效学,2014,20(03):88-91+95.DOI:10.13837/j.issn.1006-8309.2014.03.0020. [8] Merlhiot G, Bueno M. How drowsiness and distraction can interfere with take-over performance: A systematic and meta-analysis review[J]. Accident Analysis & Prevention, 2022, 170: 106536. [9] Alonso F, Esteban C, Montoro L, et al. Conceptualization of aggressive driving behaviors through a Perception of aggressive driving scale (PAD)[J]. Transportation research part F: traffic psychology and behaviour, 2019, 60: 415-426. [10] Senders J W, Kristofferson A B, Levison W H, et al. The attentional demand of automobile driving[J]. 1967. [11] Martens M, Van Winsum W. Effects of speech versus tactile driver support messages on driving behaviour and workload[R]. SAE Technical Paper, 2001. [12] Stojmenova K, Jakus G, Sodnik J. Sensitivity evaluation of the visual, tactile, and auditory detection response task method while driving[J]. Traffic injury prevention, 2017, 18(4): 431-436. [13] Stojmenova K, Sodnik J. Detection-response task—uses and limitations[J]. Sensors, 2018, 18(2): 594. [14] Mattes S. The lane-change-task as a tool for driver distraction evaluation[J]. Quality of Work and Products in Enterprises of the Future, 2003, 57(60): 7. [15] Hsieh L, Seaman S. Evaluation of the two-dimensional secondary task demand assessment method[J]. Unpublished report, Department of Communication Sciences and Disorders, Wayne State University. [16] Trommler D, Morgenstern T, Wögerbauer E M, et al. The box task-a method for assessing in-vehicle system demand[J]. MethodsX, 2021, 8: 101261. [17] Körber M, Cingel A, Zimmermann M, et al. Vigilance decrement and passive fatigue caused by monotony in automated driving[J]. Procedia Manufacturing, 2015, 3: 2403-2409. [18] Bretzner L, Krantz M. Towards low-cost systems for measuring visual cues of driver fatigue and inattention in automotive applications[C]//IEEE International Conference on Vehicular Electronics and Safety, 2005. IEEE, 2005: 161-164. [19] Morimoto C H, Koons D, Amir A, et al. Pupil detection and tracking using multiple light sources[J]. Image and vision computing, 2000, 18(4): 331-335. [20] Sigari M H, Pourshahabi M R, Soryani M, et al. A review on driver face monitoring systems for fatigue and distraction detection[J]. International Journal of Advanced Science and Technology, 2014, 64: 73-100. [21] Kountouriotis G K, Spyridakos P, Carsten O M J, et al. Identifying cognitive distraction using steering 南京邮电大学专业学位硕士研究生学位论文 参考文献  61 wheel reversal rates[J]. Accident Analysis & Prevention, 2016, 96: 39-45. [22]贾硕,惠飞,马峻岩,彭娜.商用车辆异常驾驶行为检测算法研究[J].公路交通科技,2017,34(S2):29-36. [23] Li L,Boxuan Z,Clayton H, et al. Detection of driver manual distraction via image-based hand and ear recognition[J]. Accident Analysis and Prevention,2020,137 : 105432. [24] Chun C, Sheng H, Ko L, et al. An EEG-based perceptual function integration network for application to drowsy driving. [J]. Knowledge-Based Systems, 2015,80(1):143-152. [25] Marchegiani L, Posner I. Long-term driving behaviour modelling for driver identification[C]//2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018: 913-919. [26] Lee J D, Moeckli J, Brown T L, et al. Distraction detection and mitigation through driver feedback[R]. 2013. [27] Nakayama O, Futami T, Nakamura T, et al. Development of a steering entropy method for evaluating driver workload[J]. SAE transactions, 1999: 1686-1695. [28] Zhang W, Zhang H. Research on distracted driving identification of truck drivers based on simulated driving experiment[C]//IOP Conference Series: Earth and Environmental Science. IOP Publishing, 2021, 638(1): 012039. [29] Fernández A, Usamentiaga R, Carús J L, et al. Driver distraction using visual-based sensors and algorithms[J]. Sensors, 2016, 16(11): 1805. [30] Billah T, Rahman S M M, Ahmad M O, et al. Recognizing distractions for assistive driving by tracking body parts[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2018, 29(4): 1048-1062. [31] Chihang Z, Binli Z, Jie H, et al. Recognition of driving postures by contourlet transform and random forests[J]. Iet Intelligent Transport Systems, 2012, 6(2): 161-168. [32] Hesham M. Eraqi,Yehya Abouelnaga,Mohamed H, et al. Driver Distraction Identification with an Ensemble of Convolutional Neural Networks[J]. Journal of Advanced Transportation,2019,2019. [33] Eraqi H M, Abouelnaga Y, Saad M H, et al. Driver Distraction Identification with an Ensemble of  Convolutional Neural Networks[J]. Journal of Advanced Transportation, 2019, 2019: 1-12. [34] Alotaibi M, Alotaibi B. Distracted driver classification using deep learning[J]. Signal, Image and Video Processing, 2020, 14(3): 617-624. [35] Behera A, Keidel A, Debnath B. Context-driven multi-stream LSTM (M-LSTM) for recognizing fine-grained activity of drivers[C]//German Conference on Pattern Recognition. Springer, Cham, 2018: 298-314. [36] 夏瀚笙,沈峘,胡委.基于人体关键点的分心驾驶行为识别[J].计算机技术与发展,2019,29(07):1-5. [37] Lal S K, Craig A. A critical review of the psychophysiology of driver fatigue[J]. Biological Psychology, 2001, 55(3):173. [38] Kithil P W. Driver alertness detection research using capacitive sensor array[J]. 2001. [39] Mingcong W, Chitseng C, Hsiang K. Remote surveillance system for driver drowsiness in real-time using  low-cost embedded platform[C]// IEEE International Conference on Vehicular Electronics and Safety. IEEE, 2008:288-292. [40] 夏芹,宋义伟,朱学峰.基于PERCLOS的驾驶疲劳监控方法进展[J].自动化技术与应用, 2008(06): 43-46 +39. [41] Zhang Y,Hua C.Driver fatigue recognition based on facial expression analysis using local binary pattems[J].Optik-International Hournal for Light and Electron Optics,2015:4501-4505. [42] Akrout B, Mahdi W. Spatio-temporal features for the automatic control of driver drowsiness state and lack of concentration[J]. Machine Vision and Applications, 2015, 26(1): 1-13.  [43] Mbouna R O, Kong S G, Chun M G. Visual analysis of eye state and head pose for driver alertness monitoring[J]. IEEE transactions on intelligent transportation systems, 2013, 14(3): 1462-1469. [44] Yang G,Lin Y,Bhattacharya P.A driver fatigue recognition model based on information fijsion and dynamic 南京邮电大学专业学位硕士研究生学位论文 参考文献  62 Bayesian network[J].Information Sciences,2010,J 80(10):1942-1954. [45] 郑伟成,李学伟,刘宏哲,代松银.基于深度学习的疲劳驾驶检测算法[J].计算机工程,2020,46(07):21-29. [46] Kim W, Jung W S, Choi H K. Lightweight driver monitoring system based on multi-task mobilenets[J]. Sensors, 2019, 19(14): 3200. [47] Fan C, Peng Y, Peng S, et al. Detection of train driver fatigue and distraction based on forehead EEG: a time-series ensemble learning method[J]. IEEE Transactions on Intelligent Transportation Systems, 2021, 23(8): 13559-13569. [48] Zhuang F, Qi Z, Duan K, et al. A comprehensive survey on transfer learning[J]. Proceedings of the IEEE, 2020, 109(1): 43-76. [49] Lu J, Behbood V, Hao P, et al. Transfer learning using computational intelligence: A survey[J]. Knowledge-Based Systems, 2015, 80: 14-23. [50] 陈淑環, 韦玉科, 徐乐, 等. 基于深度学习的图像风格迁移研究综述[J]. 计算机应用研究, 2019, 36(8): 2250-2255. [51] Alom M Z, Taha T M, Yakopcic C, et al. A state-of-the-art survey on deep learning theory and architectures[J]. electronics, 2019, 8(3): 292. [52] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014. [53] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9. [54] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778. [55] Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017. [56] Zhang X, Zhou X, Lin M, et al. Shufflenet: An extremely efficient convolutional neural network for mobile devices[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 6848-6856. [57] Mnih V, Heess N, Graves A. Recurrent models of visual attention[J]. Advances in neural information processing systems, 2014, 27. [58] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30. [59] Hu J, Shen L, Sun G. Squeeze-and-excitation networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7132-7141. [60] Sun S. A survey of multi-view machine learning[J]. Neural computing and applications, 2013, 23: 2031-2038. [61] Sun S. A survey of multi-view machine learning[J]. Neural computing and applications, 2013, 23: 2031-2038. [62] Su H, Maji S, Kalogerakis E, et al. Multi-view convolutional neural networks for 3d shape recognition[C]//Proceedings of the IEEE international conference on computer vision. 2015: 945-953. [63] Feng Y, Zhang Z, Zhao X, et al. Gvcnn: Group-view convolutional neural networks for 3d shape recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 264-272. [64] Wei X, Yu R, Sun J. View-gcn: View-based graph convolutional network for 3d shape analysis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 1850-1859. [65] Zhang Y, Yang Q. An overview of multi-task learning[J]. National Science Review, 2018, 5(1): 30-43. [66] Ruder S. An overview of multi-task learning in deep neural networks[J]. arXiv preprint arXiv:1706.05098, 南京邮电大学专业学位硕士研究生学位论文 参考文献  63 2017. [67] Shazeer N, Mirhoseini A, Maziarz K, et al. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer[J]. arXiv preprint arXiv:1701.06538, 2017. [68] Ma J, Zhao Z, Yi X, et al. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts[C]//Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 2018: 1930-1939. [69] Ma X, Zhao L, Huang G, et al. Entire space multi-task model: An effective approach for estimating post-click conversion rate[C]//The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 2018: 1137-1140. [70] State Farm Distracted Driver Detection. Accessed: Aug. 2, 2016. [Online]. Available: https://www.kaggle.com/c/state farm distracteddriver detection [71] Abouelnaga Y, Eraqi H M, Moustafa M N. Real-time distracted driver posture classification[J]. arXiv preprint arXiv:1706.09498, 2017. [72] Magic Data . MULTI-MODAL DRIVER BEHAVIORS DATASET FOR DMS. [Online]. Available: https://magichub.com/datasets/driver-behaviors-dataset-for-dms [73] Chang Y L, Tan T H, Lee W H, et al. Consolidated convolutional neural network for hyperspectral image classification[J]. Remote Sensing, 2022, 14(7): 1571. [74] Xu X, Zhao M, Shi P, et al. Crack detection and comparison study based on faster R-CNN and mask R-CNN[J]. Sensors, 2022, 22(3): 1215. [75] Howard A, Sandler M, Chu G, et al. Searching for mobilenetv3[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 1314-1324. [76] Deng J, Guo J, Ververas E, et al. Retinaface: Single-shot multi-level face localisation in the wild[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 5203-5212. [77] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2117-2125. [78] Najibi M, Samangouei P, Chellappa R, et al. Ssh: Single stage headless face detector[C]//Proceedings of the IEEE international conference on computer vision. 2017: 4875-4884. [79] Yang S, Luo P, Loy C C, et al. Wider face: A face detection benchmark[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 5525-5533. [80]史瑞鹏,钱屹,蒋丹妮.一种基于卷积神经网络的疲劳驾驶检测方法[J].计算机应用研究,2020,37(11):3481-3486.DOI:10.19734/j.issn.1001-3695.2019.07.0313. [81]季映羽. 基于面部特征分析与多指标融合的疲劳状态检测算法研究[D].吉林大学,2019. [82]陈之坤. 基于深度学习的驾驶员疲劳检测[D].山东科技大学,2020.DOI:10.27275/d.cnki.gsdku.2020.001307. [83] Cubuk E D, Zoph B, Mane D, et al. Autoaugment: Learning augmentation strategies from data[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 113-123. [84] Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988. [85] Liu S, Johns E, Davison A J. End-to-end multi-task learning with attention[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 1871-1880.