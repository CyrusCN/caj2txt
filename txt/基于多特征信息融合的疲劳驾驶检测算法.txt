重庆工商大学学报(自然科学版) 

Journal of Chongqing Technology and Business University(Natural Science Edition) 

ISSN 1672-058X,CN 50-1155/N 

《重庆工商大学学报(自然科学版)》网络首发论文 

题目： 
基于多特征信息融合的疲劳驾驶检测算法 
作者： 
张兴旺，王凤随，杨海燕 
收稿日期： 
2024-01-04 
网络首发日期：  2024-03-07 
引用格式： 

张兴旺，王凤随，杨海燕．基于多特征信息融合的疲劳驾驶检测算法[J/OL]．重
庆工商大学学报(自然科学版). 
https://link.cnki.net/urlid/50.1155.N.20240304.1723.006 

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶

段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期

刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出

版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出

版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编

辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、

出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。

为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，

只可基于编辑规范进行少量文字的修改。 

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国

学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷

出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出

版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首
发论文视为正式出版。 

 
 
 
 
 
 
 
 
 
 
 
 
网络首发时间：2024-03-07 10:59:12
网络首发地址：https://link.cnki.net/urlid/50.1155.N.20240304.1723.006

基于多特征信息融合的疲劳驾驶检测算法 

张兴旺 1,2，王凤随 1,2，杨海燕 1,2 

1.  安徽工程大学电气工程学院  安徽  芜湖  241000； 
2.  高端装备先进感知与智能控制教育部重点实验室  安徽  芜湖  241000 

摘要：目的  针对目前驾驶员疲劳驾驶检测不能兼顾检测速度与检测准确率的问题，提出了
一种基于 YOLOv7 改进模型的疲劳驾驶检测算法。方法  首先，为提高模型的收敛速度，增
强模型的检测性能，算法将传统的卷积层替换为深度过度参数化卷积层，通过增加可学习的
参数加快拟合过程。其次，针对存在遮挡目标的场景，传统的下采样过程容易导致特征丢失
严重，为提高对遮挡目标检测的准确性，算法引入了基于 SE 注意力改进的 DS-Conv 模块。
再次，为了提高模型对小目标的检测能力，算法在特征提取层中添加了多尺度特征提取 MSS
注意力模块，能够在不同尺度上捕捉目标的细节和上下文信息。最后，在检测出的人脸上根
据 PERCLOS 准则进行疲劳驾驶判定。结果  实验结果表明，改进算法在 WIDER FACE 数据
集的 Easy，Medium，Hard 子集上分别达到了 96.0%，94.6%，88.1%。结论  改进算法结构
简单，参数量较少，满足人脸目标实时检测的要求，适合部署在车载系统等资源有限的环境
中，有效保障驾驶员的驾驶安全。 
关键词：疲劳驾驶检测；多特征信息融合；通道注意力；多尺度特征提取；PERCLOS 准则 
中图分类号：TP391.4        文献标识码：A 

Fatigue Driving Detection Algorithm Based on Multi-Feature Information Fusion 
ZHANG Xingwang1,2, WANG Fengsui1,2, YANG Haiyan1,2 
1. School of Electrical Engineering, Anhui Polytechnic University, Wuhu 241000, China; 
2.  Key  Laboratory  of  Advanced  Perception  and  Intelligent  Control  of  High-end  Equipment, 

Ministry of Education, Wuhu 241000, China 

Abstract: Objectives Due to the current problem that driver fatigue driving detection can not take 
into  account  the  detection  speed  and  accuracy,  this  paper  proposes  a  fatigue  driving  detection 
algorithm based on YOLOv7. Method Firstly, in order to improve the convergence speed of the 
model and enhance the detection performance of the model, the algorithm replaces the traditional 
convolutional  layer  with  a  depthwise  over-parameterized  convolutional  layer  which  accelerates 
the fitting process by adding learnable parameters. Secondly, in the scene with occluded targets, 
loss  of  numerous  features  is  caused  by  the  traditional  down-sampling  process.  To  improve  the 
accuracy of occluded target detection, the DS-Conv module based on the improved SE attention is 
proposed. Thirdly, in order to improve the ability of detecting small targets, the algorithm adds a 
multi-scale feature extraction MSS attention module to the feature extraction layer, which is able 
to capture the details and contextual information of the target at different scales. Finally, fatigue 
driving  determination  is  performed  on  the  detected  faces  according  to  the  PERCLOS  criterion. 

收稿日期：2024-01-04    修回日期：2024-02-28 
基金项目：安徽省自然科学基金(2108085MF197)；安徽高校省级自然科学研究重点项目(KJ2019A0162)；安

徽工程大学国家自然科学基金预研项目(Xjky2022040). 

作者简介：张兴旺（1998—），男，安徽六安人，硕士研究生，从事图像处理研究. 
通讯作者：王凤随（1981—），男，安徽宿州人，博士，教授，硕士生导师，主要从事图像与视频信息处理

和计算机视觉等研究. Email: fswang@ahpu.edu.cn. 

 
 
 
                                           
Result  The  experimental  results  show  that  the  improved  algorithm  achieves  96.0%,  94.6%,  and 
88.1%  on  the  Easy,  Medium,  and  Hard  subsets  of  the  WIDER  FACE  dataset.  Conclusion  The 
improved  algorithm  with  simple  structure  and  a  small  number  of  parameters  is  conducive  to 
deployment  of  real-time  detection  of  face  targets,  and  is  suitable  for  deployment  in 
resource-limited  environments  such  as  in-vehicle  systems  to  effectively  protect  the  driver’s 
driving safety. 
Keywords:  fatigue  driving  detection;  multi-feature  information  fusion;  channel  attention; 
multi-scale feature extraction; PERCLOS criterion 
1    引言 

疲 劳 驾 驶 占 总 交 通 事 故 发 生 原 因 的
20%，在大型卡车和公路交通事故中约占
37%，在重大交通事故中约占 43%，因此为
了保障人们的生命财产安全，实施驾驶员疲
劳驾驶检测具有十分重要的意义。 

Zhang 等[1]提出基于面部行为分析的驾
驶员疲劳检测方法，通过检测眼睛和嘴巴的
状态，判断驾驶员是否处于疲劳状态。现阶
段 卷 积 神 经 网 络 （ Convolutional  Neural 
Networks，CNN）已广泛使用在图像分类、
目标检测、人脸识别以及自动驾驶等任务中
[2-3]，并且在各任务中都卓有成效。目前卷
积神经网络目标检测算法分为单阶段检测
和两阶段检测。两阶段检测算法其基本思想
是先生成一些候选框，再对这些候选框进行
分类和回归，以得到最终的检测结果。常见
的两阶段目标检测算法有 MaskR-CNN[4]、
R-CNN[5]、FastR-CNN[6]、FasterR-CNN[7]等。
两阶段目标检测算法虽然准确率较高，但计
算量较大，检测速度较慢，不能满足驾驶员
疲劳驾驶检测实时性的要求。单阶段检测算
法则可以直接预测边界框和类别，而不需要
通过两个阶段（即候选区域生成和分类）来
完成。由于没有额外的候选区域生成和筛选
过程，单阶段检测算法能够在减少计算资源
需求的同时，实现较快的检测速度，常见的
单阶段目标检测算法有 YOLO 系列[8-10]、
SSD[11]、EfficientDet[12]、RetinaNet[13]等。但
相较于两阶段检测算法，单阶段检测算法在
准确率上可能略有不足。 

臧露奇[14]在 YOLOv7 中进行了一些改
进，他增加了感受野增强模块，通过捕获多
尺度信息和不同距离的依赖关系，以提升模
型的检测性能。此外他还引入了排斥损失和
注意力模块，以增强对于复杂场景下的目标

检测能力。然而，这些改进导致了模型的检
测速度下降。许卓凡[15]提出了一种不同的思
路来解决准确率问题，他引入了浅层特征和
深层特征的融合机制，通过使用两种类型的
金字塔结构，将上下文信息和背景信息引入
金字塔结构中。这种融合机制避免了特征融
合过程中的不适应现象，并提高了对于小尺
度和遮挡人脸的检测能力。另外 Jia[16]采用
GhostNet 为 backbone 来提取特征，并通过
特征融合进一步增强模型的性能，同时设计
了一个新的损失函数，以平衡模型的检测速
度和准确率，然而最终表现的准确率并不优
异。 

为了解决现有算法在疲劳驾驶检测任
务上准确率和检测速度无法兼顾的问题，提
出了一种基于改进 YOLOv7 的疲劳驾驶检
测算法。首先，通过将传统卷积层替换为深
度 过 度 参 数 化 卷 积 层 [17] （ Depthwise 
Convolutional ，
Over-parameterized 
DO-Conv），增加可学习的参数，使模型能
够学习到更复杂和精细的特征信息，提高目
标检测的准确性，还可以帮助模型更好地训
练数据，降低欠拟合的风险。其次，在特征
提取层中添加了多尺度特征提取 MSS 注意
力模块[18]。该模块能够在不同尺度上捕捉目
标的细节和上下文信息，从而提高对小目标
的检测能力。最后，引入了基于 SE[19]注意
力改进的 DS-Conv 模块，用于替换原有模
型的下采样卷积层。DS-Conv 模块能够帮助
模型关注与目标相关的通道信息，减少对于
无关信息的依赖，提高目标检测的准确性，
它还可以学习到对于遮挡目标更具区分度
的通道信息，从而提高对于遮挡目标的检测
能 力 。 通 过 以 上 改 进 ， 提 出 的 基 于 改 进
YOLOv7 的疲劳驾驶检测算法在提高检测
准确性的同时，也保持了较高的检测速度，

从而有效解决现有算法在疲劳驾驶检测任
务中的问题。 
2    YOLOv7 网络模型及其改进 
2.1    YOLOv7 网络模型 

Yolov7 是目标检测算法 YOLO 系列的
一种改进版本，它以快速且准确的目标检测
能力而受到广泛关注。 

Yolov7 主干网络采用了 Darknet，不仅
具有较少参数，还拥有着高效的计算性能，
同时采用了更深的网络层次结构，使得模型
能够学习更丰富的特征。Yolov7 通过引入特
征金字塔，可以在不同层次的特征图上进行
目标检测，可以更好地处理不同尺度的目
标，并使用了 PANet 来进行特征融合。PANet
通过自上而下和自下而上的路径，将来自不
同尺度特征图的信息进行聚合，以提高目标
检测的准确性。Yolov7 在训练和推理过程中
采用了一些优化策略，如使用 Mish 激活函
数、使用 Group Normalization（组归一化）
等，以提高模型的效率和准确性。Mish 激
活函数是一种非线性激活函数，可以提供更
广泛的激活范围和更好的梯度性质，进而改
善模型的准确性。组归一化用来规范化特征
图。相比于传统的批量归一化，组归一化对
小批量数据更稳定，并且在训练过程中不依
赖批量大小，有助于提高模型的泛化性能。
Yolov7 在训练过程中还应用了多种数据增
强技术，如随机缩放、随机裁剪、图像翻转
等，以增加数据样本的多样性，提高模型的
鲁棒性。 

将 Yolov7 用于人脸检测任务中，主要

有以下一些优点： 

Yolov7 相对轻量而且具有高效的性能，
可以在实时或近实时的条件下进行人脸检
测。这对于需要快速响应和处理的实时人脸
识别应用非常关键。虽然 Yolov7 主要用于
目标检测，但它可以提取人脸图像中的丰富
特征。这些特征可以用于进一步的人脸识别
任务，例如提取人脸特征向量并进行比对。

Yolov7 的网络结构可以根据需要进行修改
和扩展，以适应特定的人脸识别任务，同时
可以在 Yolov7 的基础上添加额外的网络层
或模块，以提高人脸识别的准确性。 
2.2    改进 YOLOv7 网络模型 
2.2.1    下采样卷积层 DS-Conv 

下采样操作通常会减小特征图的尺寸，
从而丢失一些重要的特征信息，通过添加通
道注意力，可以对每个通道进行自适应的特
征加权，使得模型能够更有针对性地保留重
要的特征，将更多的注意力集中在对当前任
务有用的通道上，从而增强模型的表达能力
和性能。通道注意力还可以通过对通道进行
加权乘法运算，减少对冗余信息的关注，这
有助于降低模型的计算和存储成本，提高模
型的计算效率。并且通过引入通道注意力，
模型可以更好地适应不同的输入情况和场
景变化，自适应地调整通道权重，从而提高
模型的鲁棒性，使其在各种输入条件下都能
有更好的性能表现。 

SE（Squeeze-and-Excitation）注意力模
块结构图如图 1 所示。它通过学习通道间的
关系，动态地调整每个通道的权重，以便更
好地捕捉重要的特征，增强模型的表达能
力。SE 注意力机制由三个模块组成，分别
为压缩（Squeeze）模块、激励（Excitation）
模块和缩放（Scale）模块。压缩模块将卷积
神经网络的输出特征图通过全局平均池化
操作进行压缩，将通道维度压缩为一个特征
向量。这个特征向量捕获了整个特征图的全
局统计信息，反映了每个通道的重要性。激
励模块通过使用两个全连接层（或卷积层）
对压缩后的特征向量进行处理，以产生一个
激励向量。第一个全连接层用于降低维度，
第二个全连接层用于增强特征。最后使用激
活函数将激励向量的值限制在 0 到 1 之间。
缩放模块将激励向量乘以输入特征图，对每
个通道进行缩放。这个缩放操作根据通道的
重要性调整特征图中每个通道的权重，从而 
使更重要的特征得到增强，不重要的特征得 

图 1    SE 注意力模块结构图 
Fig.1 SE attention module structure diagram

X'H'C'WtrFHHUC W W ()sqF 11C 11C (,)exFW (,)scaleF X C 
图 2    DS-Conv 模块结构图 
Fig.2 DS-Conv module structure diagram 

到抑制。 

SE 注意力机制的作用是通过自适应地
学习通道权重，增强模型对重要特征的关
注，抑制对于任务无关或不重要的特征。它
可以帮助模型更好地适应不同样本和场景，
提高模型的表达能力和准确性。同时 SE 注
意力机制可以嵌入到现有的卷积神经网络
中，而不需要额外的复杂结构。 

DS-Conv 模块结构图如图 2 所示。在除
了添加 SE 通道注意力以外，DS-Conv 还增
加了几个卷积层。首先通过添加第一个普通
卷积层将输入的通道数增加至原来的四倍，
增加网络的表达能力和特征提取能力。然后
添加一个深度可分离卷积层，减少参数量和
降低计算量并获取各个通道的信息，目的是
通过深度可分离卷积层来提取更加丰富和
有表达力的特征。之后将特征输入到 SE 模
块中，SE 模块对每个通道的特征进行降维，
从而减少计算量，并在学习各个通道的权重
后，融合各个通道的特征信息。最后增加一
个普通卷积层恢复增加的通道数，为后续的
操作提供适当的输入。最终通过整合和融合
各个通道的特征信息，DS-Conv 可以提高网
络的性能和表达能力，从而更好地处理输入
数据并提取有用的特征。 
2.2.2    MSS 注意力 

MSS 注 意 力 主 要 由 多 头 混 合 卷 积
（Multi-Head Mixed Convolution，MHMC）、
多尺度感知聚合（Scale-Aware Aggregation，
SAA ） 和 尺 度 感 知 调 制 器 （ Scale-Aware 
Modulation，SAM）三个模块组成，结构图
如图 3 所示。 

MHMC 设置了 N 个卷积核，并将卷积
核的大小初始化为 3×3，以 2 为步幅逐头递
增，对每个检测头应用独立的深度可分离卷
积操作。通过增加检测头的数量，可以使用
更大的卷积核来扩大感受野，从而增强模型
建模长距离依赖关系的能力。由于逐渐增加
卷积核的大小，从而可以捕捉到不同尺度上
的空间特征，较小的卷积核关注局部细节，
较大的卷积核则能够捕捉更大范围的上下

文信息。通过多个检测头的并行操作，模型
能够同时处理多个尺度上的特征，从而提高
目标检测的性能。因此 MHMC 具有通过调
整头的数量来调节感受野的范围和多粒度
信息的能力，MHMC 结构图如图 3（b）所
示，过程可表述为： 

其中

检测头，
调增加 2。 

(1) 
表示分割成的 N 个
表示卷积核大小单

由于 MHMC 检测头之间存在信息交互
不足的问题，因此引入了 SAA 模块，设计
如图 3（c）所示。SAA 模块从每个检测头
中选择一个通道来共同构建一个组，这样每
个组包含了来自不同检测头的特征通道。通
过这种方式，SAA 模块对 MHMC 生成的不
同粒度的特征进行重新组合和分组，以增强
多尺度特征的多样性。然后 SAA 模块使用
一个 1×1 卷积操作来进行组内和组间的信
息融合，这种跨组信息融合的方式可以实现
轻量化和高效的聚合效果。通过将不同组内
的特征进行融合，SAA 模块促进了多尺度特
征之间的交互，从而提高了目标检测模型的
性能。当给定输入
，组的数量
Groups=C/Heads，每个组中包含了 N 个特
征通道，其中 C 为通道数，N 为检测头的数
量，SAA 的过程可表述为： 

(2) 

其中

和

阵 ，

是点 向卷积的 权 矩
。
，

表示深度可分离卷积的第 j 个

头部， 表示第 j 个头部的第 i 个通道。 

如图 3（a）中，在使用 MHMC 捕捉多
尺度空间特征并通过 SAA 进行信息融合后，
得到一个输出特征图，然后通过 SAM 进行

ConvBNGELUDWConvBNGELUSEConvBNDBConvLinearAvg PoolSiLuLinearSigmoidSE11()1((),...,())nnMHMCXkkkknOConcatDWxDWx12[,,...,]nxxxx{3,5,...,}ikKHWCXRint12int121([,,...,]),([,,...,]),().jjerMiiiiraNiiHWjkkjMWGGGGWHHHHDWConvxRinterWintraW{1,2,...,}jN{1,2,...,}iMHWMjHRijH 
 
 
 
图 3    SAM 注意力结构图 
Fig.3 SAM attention structure chart 

，计算输出 Z 如

2.2.3    深度过度参数化卷积层 DO-Conv 

调制。对于输入
下： 

(3) 

其中 和 是线性层的权矩阵。SAM
保留了信道维度，可以增强模型对不同通道
特征的建模能力和灵活性。同时还可以在不
同的输入下动态变化，实现自适应自调制，
使模型能够独立调节每个通道的特征权重，
实现特征选择和多样性的特征融合，从而提
高目标检测模型的性能。 

通过 MSS 多尺度特征的提取，即使目
标在图像中可能以不同的尺度出现，也可以
帮助模型对不同尺度的目标进行感知和定
位。同时帮助模型适应这种尺度变化，捕捉
目标在不同尺度下的视觉特征。多尺度特征
提取还可以提高目标定位的准确性，较小的
尺度能够更好地捕捉目标的局部细节，而较
大的尺度能够提供目标的全局上下文信息，
通过综合利用不同尺度的特征，模型可以更
准确地定位目标并减少定位误差。最后 MSS
通过融合不同尺度的特征来提高目标检测
的性能，提供更全面和丰富的信息，有助于
提高模型的准确性、鲁棒性和泛化能力，使
其在各种场景中更加有效和可靠。 

深度过度参数化卷积层（DO-Conv）是
由深度卷积核 D 和传统卷积核 W 两个可训
练 核 共 同 组 成 。 给 定 一 个 输 入 特 征 P ，
DO-Conv 的输出为：
。如图
4 所示，DO-Conv 可以用图中数学等效的方
式应用，即： 

(4) 

，

， 
，
是
在第一维度和第二维度上

其中

的转置。 

如图 4 所示，在使用卷积核重参化时，
和 的组合是通过一个复合核 实现的，
即深度过度参数化卷积算子首先通过可训
，然后
练核 来转换 W，得到
在 和 之间应用传统的卷积算子，得到

。 

DO-Conv 是把传统卷积层做了一个过
度参数化的处理。传统卷积层的参数 A 和
DO-Conv 的参数 分别可表示为： 

(5)

3×3DW Conv 5×5DW Conv K×KDW Conv 7×7DW Conv   N heads(b) MHMC   Group 1 Group 2 Group 3 Group M  1x1Conv(c) SAA 多头混合卷积LinearLinear(a) SAM轻量级1×1轻量级1×1轻量级1×1轻量级1×1ConvConvConvConv多头信息融合HWCXR,,(()).vSZMVVWXMSAAMHMCWXvWsW(,)ODWP))((,TPOPWWDDoutmulinCDCWR()mulinMNDCDR()inMNCPR()mulinDMNCTDR()mulinMNDCDR'WTD'TWDW'WP'OWP'A:()':()outinoutmulinmulinACMNCACDCMNDC 
 
 
 
 
 
 
图 4    卷积核重参化 
Fig.4 Convolutional kernel reparameterisation

其 中

， 即 使 在

的情况下，参数的数量也增

加了
。在推理阶段，计
算得到的权重 用于推理，由于 的形状
与卷积层的核相同，所以 DO-Conv 在推理
阶段的计算与传统卷积层的计算完全相同。 
通过增加可学习参数，模型可以更好地
拟合训练数据，提高模型的表达能力，从而
更好地适应复杂的数据分布和任务要求。同
时也增加了模型的灵活性，降低欠拟合的风
险，提高了模型的拟合能力。在一些复杂的
任务和大规模数据集上通常需要更强大的
模型来提取更复杂的特征信息，通过增加可
学习参数，模型可以更好地处理这些挑战，
从而提高任务的准确性和性能。 
3    仿真实验与结果分析 
3.1    实验数据集及环境配置 

数据集采用的是 WIDER  FACE[20]数据
集。WIDER FACE 数据集是一个大规模的人
脸检测数据集，由伯克利大学的研究人员于
2016 年发布，共包含 32203 张训练图像和
393703 张个人脸部标注，覆盖大量的人脸大
小、姿势、表情和遮挡情况。同时数据集基
于 61 个事件类别进行组织，对于每个事件
类别，随机选择 40%/10%/50%数据作为训
练、验证和测试集，使得数据集适合和评估
人 脸 检 测 算 法 。 除 了 人 脸 边 界 框 之 外 ，
WIDER FACE 还提供了五个面部关键点（左
右眼、鼻子、左右嘴角）的标注，这有助于
开展面部关键点检测和姿势估计等研究。
WIDER  FACE 数据集已成为人脸检测领域

的一个重要基准，许多研究人员和工程师利
用它来开发和评估新的人脸检测方法。 

WIDER  FACE 数据集的训练集包含了
12876 张图片，验证集包含了 3226 张图片。
实验均在 Linux 操作系统的环境下对数据集
进行训练及验证，具体运行环境配置如表 1
所示。 

表 1    实验运行环境 
Table1 Experimental operating environment 
类型 
操作系统 

具体参数 
Linux 
12 vCPU Intel(R) Xeon(R) 
Platinum 8255C CPU @ 
2.50GHz 
GeForce RTX 3090 
1.90 
11.4 
3.8 

CPU 

GPU 
PyTorch 
CUDA 
python 

实验训练 100 个 epoch，具体的网络模

型参数设置如表 2 所示。 

表 2    网络模型参数 
Table2 Network model parameter 
Value 
16 
100 
0 
8 

Parameter 
Batchsize 
epoch 
device 
workers 

3.2    驾驶疲劳判定与评价指标 

通过检测驾驶员眼睛是否处于闭合状
态以及嘴巴是否打哈欠的方法，来判定驾驶
员是否处于疲劳状态。具体步骤如下： 

首先通过改进的 YOLOv7 网络模型检
测输入图像，从图像中检测到脸部区域后，

  inCmulDTDMNoutC 'OWPMNoutCinC'TWDWMNinC PmulDoutCinC W()mulDMN()mulDMN()mulinMNDC'W'W 
 
将脸部区域作为输入图像输入，之后通过基
于 Dlib 库的人脸关键点检测器获取关键点，
并在图像中绘制出关键点位置。如图 5（a）
为人脸 68 个关键点描绘图，图 5（b）为右
眼及嘴巴关键点坐标。通过提取眼部特征并
计算眼部纵横比（ ）判断眼睛是否闭合。
以左眼为例， 计算公式为： 

(6) 

图 5    Dlib 人脸关键点检测器的 68 个关键
点坐标和右眼及嘴巴关键点坐标 
Fig.5 Dlib face keypoint detector with 68 keypoint 
coordinates and right eye and mouth keypoint 
coordinates 

同理，计算出右眼的 ，然后将计算

得出的两个 进行平均后获得最终 结
果。得到的 结果结合 PERCLOS 准则，
判断驾驶员是否处于疲劳状态。PERCLOS
是一种用于疲劳检测的指标，全称为“闭眼

时间百分比”。目前 PERCLOS 有 EM 准则、
P70 准则和 P80 准则三种准则判断被检测者
是否处于疲劳状态。研究表明当处于驾驶状
态时，P80 准则与驾驶员疲劳状态的相关性
最高，即瞳孔被眼睑覆盖超 80%的面积时判
定眼睛处于闭合状态，并计算检测周期时间
内闭合状态所占的时间比例，因此实验中的
PERCLOS 采用 P80 准则。一般来说，闭眼
时间百分比在 0%到 15%之间的驾驶员为正
常状态，闭眼时间百分比在 15%到 30%之间
的驾驶员可能处于疲劳状态，闭眼时间百分
比超过 30%的驾驶员则极有可能处于严重
的疲劳状态。闭眼时间百分比 计算方式
如下： 

(7) 

其中， 为检测周期时间内的总帧数， 
为检测周期内处于闭眼状态时的总帧

数。 

通过提取嘴部特征并计算嘴部纵横比
）判断嘴巴是否打哈欠， 计算公

（
式为： 

(8) 

驾驶员在说话时嘴巴也处于张开状态，
但当打哈欠时， 值会持续上升，并会持

续一定的时间。因此将得到的
结果结合
PERCLOS 准则，判断驾驶员是否处于疲劳
状态。相比于使用神经网络分类，计算眼部
状态和嘴部状态来判断驾驶员是否疲劳的
方法更加简便，同时可以提高判断的准确
性。 

实验通过准确率 P（Precision）、召回率
R（Recall）、mAP（mean Average Precision）
值作为评价网络模型优劣的指标。精确率 P
表示分类器预测为正样本且实际为正样本
的目标在所有被分类器预测为正样本的目
标中所占的比例。精确率 P 计算公式如下： 

(9) 

其中， 表示实际为正样本且预测结果

为正样本的目标，即正确分类的正样本；
表示实际为负样本但预测结果为正样本的
目标，即错误分类的负样本。 

召回率 R 表示分类器预测为正样本且
实际为正样本的目标在所有实际为正样本
的目标中所占的比例。召回率计算公式如

AREARE2635142ARPPPPEPP(a) Dlib人脸关键点坐标Dlib face keypoint coordinatesP1P2P3P4P6P5K1K6K7K4K5K3K2K8(b) 右眼及嘴巴关键点坐标Right eye and mouth keypoint coordinatesAREAREAREARE80PP80closePaFPFaFcloseFARMARM283746153ARKKKKKKMKKARMARMPPPTPTFPTPF 
 
 
 
 
 
 
表 3    不同算法在 WIDER FACE 验证集上的结果对比 
Table3 Comparison of results of different algorithms on the WIDER FACE validation set 
Model 
DSFD[21] 
SFDet[22] 
FAN[23] 
FA-RPN[24] 
文献 25 
SFD[26] 
Face R-CNN[27] 
SSH[28] 
CMS-RCNN[29] 
Ours 

Medium 
95.7 
94.5 
94.0 
94.1 
93.0 
92.5 
92.1 
92.1 
87.4 
94.6 

Easy 
96.6 
95.4 
95.2 
94.9 
94.9 
93.7 
93.7 
93.1 
89.9 
96.0 

Hard 
90.4 
88.8 
90.0 
89.4 
83.6 
85.9 
83.1 
84.5 
62.4 
88.1 

下： 

(10) 

其中， 与精确率计算中的含义相同；

表示实际为正样本但预测结果为负样本
的目标，即错误分类的正样本。以召回率 R
为横坐标，精确率 P 为纵坐标，绘制 P-R 曲
线，平均精度 AP 的值可以表示为该曲线下
的面积大小。 
3.3    实验结果 

图 6    算法在 WIDER FACE 验证集 Easy，
Medium，Hard 子集上的 mAP@0.5/% 
Fig.6  Algorithms on the  WIDER  FACE  validation 
set Easy, Medium, Hard subset of the mAP@0.5/% 
在 WIDER  FACE 验证集的三个子集上
的 mAP 曲线如图 6 所示。以 mAP@0.5/%作
为评价指标，提出的算法最终在 WIDER 
FACE 验证集的 Easy，Medium，Hard 子集
上分别达到 96.0%，94.6%，88.1%。为了进
一步证明使用模块的有效性，将所提出的算
法与其它算法进行对比，结果如表 3 所示。 

通过表 3 可以看出，提出的算法在准确
率上仍有提升的空间，不过参数量较少，仅
有 49.5M，并且采用单阶段检测，适合在车
载系统等资源有限的环境下部署。相比之
下，DSFD 算法的参数量为 459M，需要更
多的计算资源。因此提出的算法在性能和计
算资源之间取得了一定的平衡。此外与一些
多阶段的人脸检测算法，如 Face  RCNN、
CMS-RCNN 相比，提出的算法在准确率上
取得了显著的提升。 
3.4    消融实验 

通过对 YOLOv7 模型进行了一系列的
改进，包括将传统卷积层替换为 DO-Conv，
把多尺度特征提取 MSS 注意力模块添加在
特征提取层中，引入基于通道注意力 SE 改
进的 DS-Conv 模块，用于替换原有模型的
下采样卷积层。实验的结果如表 4 所示。 
根据表 4 中的数据可以看出，在增加多尺度
特征提取 MSS 注意力模块后，模型可以更
好地捕捉目标在不同大小和比例下的特征
信息，Easy、Medium 和 Hard 子集上的精度
均有较高提升，分别提升了 0.7%、0.6%和
1.1%。另外在添加 DS-Conv 模块后，Hard
子集提升较多，精度提高了 0.7%，这是因
为在 Hard 子集中小脸所占的像素太少，导
致在下采样过程中特征丢失严重，通过引入
通道注意力，可以更好地关注细节信息，从
而提升在 Hard 子集上的准确率。最后引入
DO-Conv 增加了模型可学习的参数，Easy、
Medium 和 Hard 子集上的准确率也有所提
升，分别提高了 0.3%、0.4%和 0.2%。综上 

表 4    消融实验检测结果对比 
Table4 Comparison of ablation test results 

Model  DO-Conv  DS-Conv  MSS 
Base 
Ours1 
Ours2 
Ours3 

× 
× 
× 
 

× 
× 
 
 

× 
 
 
 

parameters 
37196556 
38518962 
41562290 
49545394 

Easy  Medium 
94.7 
95.0 
95.3 
96.0 

93.2 
93.6 
94.0 
94.6 

Hard 
86.1 
86.3 
87.0 
88.1 

PPNTRTFPTNFmAP@0.5/%epoch/轮 
 
 
 
所述，通过增加 MSS 注意力模块、DS-Conv
模块和 DO-Conv 模块，可以改善模型在不
同环境下的特征提取能力，提升检测准确
性，为模型提供了改进和优化。 
3.5    可视化结果 

图 7    图像识别对比结果 
Fig.7 Image recognition comparison results 
图 7 比较了使用改进算法与基线算法进
行人脸检测任务的结果。图 7（a）、图 7（c）、
图 7（e）展示了基线算法的检测结果，图 7
（b）、图 7（d）、图 7（f）展示了改进算法
的检测结果。在图 7（a）、图 7（b）中可以
看到经过训练的 YOLOv7 模型在人脸识别
任务中取得了出色的表现，但与改进算法结
果相比，基线算法出现了一例误检，证明了
改进算法在检测任务中的稳定性。在图 7
（c）、图 7（d）中，当人脸存在遮挡情况时，
基线算法的漏检率和误检率较高。相比之
下，改进算法基本完成了检测任务，极大降
低了漏检率和误检率，这表明改进算法具备
较高的准确性，能够应对遮挡情况下的人脸
检测挑战。在图 7（e）、图 7（f）中，在人
脸较为密集且包含大量小脸的场景下，基线
算法对于图片中远处小脸的检测效果较差，
然而改进算法检测出的小脸数量大幅提升，
对人脸的定位也更加精准。基线算法和改进
算法漏检率和误检率如表 5 所示，在 Hard
子集中，改进算法漏检率下降了 2.7%，对
复杂环境下的目标检测的检出率大幅度提
高。在误检率上，改进算法较基线算法在
Easy、Medium 和 Hard 上分别下降 0.6%、
0.7%和 0.9%，表明改进算法在不同难度级
别的场景中能够减少错误检测的情况。实验
结果表明改进算法在人脸检测任务中相较

于基线算法展现了更好的稳定性、准确性，
以及在遮挡和小脸等复杂场景情况下也表
现了更出色的检测能力。 

表 5    漏检率和误检率检测结果对比 
Table 5 Comparison of leakage rate and false 
detection rate detection results 
Model  datasets  漏检率%  误检率% 

Easy 
Medium 
Hard 
Easy 
Medium 
Hard 

Base 

Ours 

4    结论 

12.5 
14.2 
21.1 
11.2 
12.7 
18.4 

7.6 
8.1 
10.2 
7 
7.4 
9.3 

提出了一种基于 YOLOv7 改进的疲劳
驾驶检测算法，在显著提高准确率的同时，
保持了较高的检测速度。首先通过将深度过
度参数化卷积层 DO-Conv 替换传统的卷积
层，在不增加推理计算复杂度的情况下加快
拟合过程，增强模型的检测性能。其次引入
了基于通道注意力 SE 改进的 DS-Conv 模
块，用于替换原有模型的下采样卷积层，极
大改善了遮挡目标场景下下采样过程特征
丢失严重的问题，提高了目标检测的准确
性。再次在特征提取层中添加多尺度特征提
取 MSS 注意力模块，能够从不同尺度上捕
捉目标的细节和上下文信息，增强了对小目
标的检测能力。最后实验结果表明，提出的
算 法 在 WIDER  FACE 数 据 集 的 Easy ，
Medium，Hard 子集上的检测精度分别达到
了 96.0%，94.6%，88.1%。综上所述，提出
的算法结构简单、参数量较少、准确率较高，
适用于资源受限和复杂条件下的环境，因此
具有实际应用的潜力，这使得提出的算法成
为一种值得考虑的疲劳驾驶检测模型解决
方案。 
参考文献： 
[1]  ZHANG F, SU J, GENG L, et al. Driver 
fatigue  detection  based  on  eye  state 
recognition[C]//2017 
International 
Conference  on  Machine  Vision  and 
Information Technology (CMVIT). IEEE, 
2017: 105-110. 

[2]  FLOREZ  R,  PALOMINO-QUISPE  F, 
COAQUIRA-CASTILLO  R  J,  et  al.  A 
cnn-based approach for driver drowsiness 
detection 
state 
identification[J]. Applied Sciences, 2023, 

real-time 

eye 

by 

(b)(a)(c)(d)(e)(f) 
 
13(13): 7849. 

[3]  KIM  M,  JAIN  A  K,  LIU  X.  Adaface: 
face 
Quality 
adaptive  margin 
recognition[C]//Proceedings 
the 
IEEE/CVF  Conference  on  Computer 
Vision  and  Pattern  Recognition.  2022: 
18750-18759. 

for 
of 

[4]  HE K M, GKIOXARI G, DOLLÁR P, et 
al.  Mask  r-cnn[C]//Proceedings  of  the 
IEEE 
on 
International  Conference 
Computer Vision. 2017: 2961-2969. 
DONAHUE 
R, 

[5]  GIRSHICK 

J, 
DARRELL  T,  et  al.  Rich 
feature 
hierarchies  for  accurate  object  detection 
semantic 
and 
the 
of 
segmentation[C]//Proceedings 
IEEE  Conference  on  Computer  Vision 
and Pattern Recognition. 2014: 580-587. 
Fast 
r-cnn[C]//Proceedings  of 
IEEE 
International  Conference  on  Computer 
Vision. 2015: 1440-1448. 

the 

R. 

[6]  GIRSHICK 

with 

region 

Analysis 

[7]  REN S Q, HE K M, GIRSHICK R, et al. 
Faster  r-cnn:  Towards  real-time  object 
detection 
proposal 
IEEE  Transactions  on 
networks[J]. 
and  Machine 
Pattern 
Intelligence, 2017, 39(6): 1137-1149. 
[8]  REDMON  J,  DIVVALA  S,  GIRSHICK 
R,  et  al.  You  only  look  once:  Unified, 
object 
real-time 
detection[C]//Proceedings  of  the  IEEE 
Conference  on  Computer  Vision  and 
Pattern Recognition. 2016: 779-788. 
[9]  REDMON J, FARHADI A. YOLO9000: 
better, faster, stronger[C]//Proceedings of 
the 
IEEE  Conference  on  Computer 
Vision  and  Pattern  Recognition.  2017: 
7263-7271. 

[10] BOCHKOVSKIY A, WANG C Y, LIAO 
H  Y  M.  Yolov4:  Optimal  speed  and 
accuracy 
object 
detection[C]//Proceedings  of  the  IEEE 
Computer 
on 
Computer Vision and Pattern Recognition. 
Piscataway: IEEE, 2020:321-333. 

Society  Conference 

of 

Springer  International  Publishing,  2016: 
21-37. 

and 

efficient 

[12] TAN M, PANG R, LE Q V. Efficientdet: 
object 
Scalable 
detection[C]//Proceedings 
the 
IEEE/CVF  Conference  on  Computer 
Vision  and  Pattern  Recognition.  2020: 
10781-10790. 

of 

loss 

[13] LIN T Y, GOYAL P, GIRSHICK R, et al. 
Focal 
object 
for 
detection[C]//Proceedings  of  the  IEEE 
International  Conference  on  Computer 
Vision. 2017: 2980-2988. 

dense 

[14] 臧露奇.  基于 YOLOv7 改进的人脸检
测算法[D].  南昌:  南昌大学, 2023. 
ZANG  Lu-qi.  Improved  face  detection 
algorithm 
on  YOLOv7[D]. 
Nanchang: Nanchang University, 2023. 

based 

fusion 

features 

[15] XU  Z,  BAI  H,  XIAO  J,  et  al.  Occluded 
and  tiny  face  detection  with  deep  and 
and 
shallow 
in 
compensation[M]//Advances 
and 
Intelligent 
Processing: 
Signal 
Multimedia 
Proceeding  of  the  IIH-MSP  2021  & 
FITAT 2021, Kaohsiung, Taiwan, Volume 
1. Singapore: Springer Nature Singapore, 
2022: 181-190. 

Information  Hiding 

[16] JIA  H,  XIAO  Z,  JI  P.  Real-time  fatigue 
driving  detection 
system  based  on 
multi-module  fusion[J].  Computers  & 
Graphics, 2022, 108: 22-33. 

[17] CAO  J,  LI  Y,  SUN  M,  et  al.  Do-conv: 
over-parameterized 
Depthwise 
convolutional 
IEEE 
Transactions on Image Processing, 2022, 
31: 3726-3736. 

layer[J]. 

[18] LIN  W,  WU  Z,  CHEN  J,  et  al. 
meet 
Scale-aware 
transformer[C]//Proceedings 
the 
IEEE/CVF  International  Conference  on 
Computer Vision. 2023: 6015-6026. 

modulation 

of 

Ssd: 

[11] LIU  W,  ANGUELOV  D,  ERHAN  D,  et 
shot  multibox 
Single 
al. 
Vision–ECCV 
detector[C]//Computer 
14th  European  Conference, 
2016: 
Amsterdam,  The  Netherlands,  October 
11–14,  2016,  Proceedings,  Part  I  14. 

[19] HU 

J, 

SHEN 

L, 

SUN  G. 

Squeeze-and-excitation 
networks[C]//Proceedings  of  the  IEEE 
Conference  on  Computer  Vision  and 
Pattern Recognition. 2018: 7132-7141. 
[20] YANG S, LUO P, LOY C C, et al. Wider 

A 

face 

detection 
face: 
benchmark[C]//Proceedings  of  the  IEEE 
Conference  on  Computer  Vision  and 
Pattern Recognition. 2016: 5525-5533. 
[21] LI  J,  WANG  Y,  WANG  C,  et  al.  DSFD: 
Dual  shot  face  detector[C]//Proceedings 
of 
IEEE/CVF  Conference  on 
Computer Vision and Pattern Recognition. 
2019: 5060-5069. 

the 

[22] ZHANG  S,  WEN  L,  SHI  H,  et  al. 
Single-shot  scale-aware  network 
for 
real-time  face  detection[J].  International 
Journal  of  Computer  Vision,  2019,  127: 
537-559. 

[23] WANG J, YUAN Y, YU G. Face attention 
network:  An  effective  face  detector  for 
the  occluded  faces[J].  ArXiv  Preprint 
ArXiv:1711.07246, 2017. 

[24] NAJIBI  M,  SINGH  B,  DAVIS  L  S. 
Fa-rpn: Floating region proposals for face 
detection[C]//Proceedings 
the 
IEEE/CVF  Conference  on  Computer 
Vision  and  Pattern  Recognition.  2019: 
7723-7732. 

of 

[25] 董 子 平 ,  陈 世 国 ,  廖 国 清 .  基 于
YOLOv5s 的密集多人脸检测算法[J]. 

计 算 机 工 程 与 科 学 ,  2023,  45(10): 
1838-1846. 
DONG  Zi-ping,  Chen  Shi-guo,  Liao 
Guo-qing.  Dense  multiple  face  detection 
algorithm 
on  YOLOv5s[J]. 
Computer  Engineering  &  Science,  2023, 
45(10): 1838-1846. 

based 

shot 

[26] ZHANG  S,  ZHU  X,  LEI  Z,  et  al.  S3fd: 
face 
Single 
detector[C]//Proceedings  of 
the  IEEE 
International  Conference  on  Computer 
Vision. 2017: 192-201. 

scale-invariant 

SAMANGOUEI 

[28] NAJIBI  M, 

[27] WANG H, LI Z, JI X, et al. Face r-cnn[J]. 
ArXiv Preprint ArXiv:1706.01061, 2017. 
P, 
CHELLAPPA  R,  et  al.  Ssh:  Single  stage 
headless face detector[C]//Proceedings of 
the  IEEE  International  Conference  on 
Computer Vision. 2017: 4875-4884. 
[29] ZHU  C,  ZHENG  Y,  LUU  K,  et  al. 
Cms-rcnn: 
multi-scale 
Contextual 
region-based  cnn  for  unconstrained  face 
detection[J].  Deep 
for 
Biometrics, 2017: 57-79. 

Learning 

 
