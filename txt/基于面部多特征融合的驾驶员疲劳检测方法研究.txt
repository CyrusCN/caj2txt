分类号：TU311密级：公开UDC：单位代码：10076工学硕士学位论文基于面部多特征融合的驾驶员疲劳检测方法研究作者姓名：张闯指导教师：朱天军企业导师：杨建森申请学位级别：工学硕士学科专业：机械培养单位：机械与装备工程学院授予学位单位：河北工程大学ADissertationSubmittedtoHebeiUniversityofEngineeringFortheAcademicDegreeofMasterofEngineeringResearchonDriverFatigueDetectionMethodBasedonFacialMulti-FeatureFusionCandidate：ZhangChuangSupervisor：Prof.ZhuTianjunPluralisticSupervisor：Dr.YangjiansenAcademicDegreeAppliedfor：MasterofMechanicalSpecialty：MechanicalCollege/Department：CollegeofMechanicalandEngineeringHebeiUniversityofEngineeringJune,2023摘要I摘要由于全球社会经济的快速发展，车辆总量与日俱增，同时因驾驶员疲劳驾驶导致的车祸大幅度增加，造成大量的人员伤亡。如果能够实时准确检测到驾驶员的疲劳状态并给予警示提醒，就能有效减少疲劳行驶造成的车祸。本文主要围绕基于驾驶员面部多特征融合的检测方法进行研究，主要研究内容包含以下几个方面：（1）基于软硬件系统搭建驾驶员疲劳驾驶实验平台，同时设计驾驶员疲劳驾驶检测系统，邀请不同年龄段的志愿者进行疲劳驾驶实验，采集志愿者在相同的测试环境下清醒、疲劳和重度疲劳状态的面部特征。（2）驾驶员疲劳检测过程中，人脸检测是最重要的一步。本文将采用深度级联多任务学习的卷积神经网络框架，获得面部图像和五个关键点，然后再对面部区域进行划分，提取眼睛、嘴巴和鼻子各自的特征。对获得的面部特征进行多任务学习，使每个特征点都能够共享，通过这种方式不仅能够快速准确提取面部的全部特征，而且能够提高人脸特征点检测的鲁棒性。（3）对采集的驾驶员睁眼、闭眼、嘴巴闭合、打哈欠和是否戴眼镜或太阳镜等特征先进行离线训练，再应用任务约束深度卷积网络对所采集的图像进行面部特征识别，判断眼睛和嘴巴的开合状态，使用AdaBoost分类器对眼睛和嘴巴的状态进行分类。在离线训练的基础上，采用一个实时模块：在线测试。在实时视频序列中检测驾驶员的疲劳特征，实时获得眼睛和嘴巴的特征地标，通过这些特征地标可以得出眼睛纵横比（EAR），眼睛闭合百分比（PercentageofEyelidClosureoverthepupilovertime，PECLORS）和嘴巴高宽比（MAR）的值。（4）利用权重比例的方法对疲劳状态下的面部特征进行融合，基于获得的EAR值、PERCLOS值和MAR值按不同权值相加得到参数M，在一定的时间内累加M>0.605的图像帧数记为参数N，通过N值的大小来判断驾驶员的疲劳程度。通过在封闭且安全的道路中对不同年龄段的志愿者进行疲劳测试，试验结果表明：本文基于驾驶员面部多特征融合的方法能够有效地利用视频图像实时检测驾驶员疲劳状态，其准确率为93.1%，对于保护驾驶员及车辆行驶安全具有重要意义。关键词：疲劳驾驶检测；眼睛纵横比；眼睛闭合百分比；嘴巴高宽比；深度学习AbstractIIAbstractDuetotherapiddevelopmentoftheglobalsocialeconomy,thetotalnumberofvehiclesisincreasingdaybyday,whilethenumberofcaraccidentscausedbydriverfatiguehasincreasedsignificantly,resultinginalargenumberofcasualties.Ifthedriver'sfatiguestatuscanbeaccuratelydetectedinrealtimeandgivenawarning,itcaneffectivelyreduceaccidentscausedbyfatiguedriving.Thisarticlemainlyfocusesontheresearchofdetectionmethodsbasedondriverfacialmultifeaturefusion,andthemainresearchcontentincludesthefollowingaspects:(1)Establishadriverfatiguedrivingexperimentplatformbasedonsoftwareandhardwaresystems,anddesignadriverfatiguedrivingdetectionsystem.Invitevolunteersofdifferentagestoconductfatiguedrivingexperiments,andcollectfacialfeaturesofvolunteersinawake,fatigue,andseverefatiguestatesunderthesametestenvironment.(2)Facedetectionisthemostimportantstepindriverfatiguedetection.Inthisthesis,adeepcascadedmultitasklearningconvolutionalneuralnetworkframeworkisusedtoobtainfacialimagesandfivekeypoints,andthenthefacialregionsaredividedtoextracttherespectivefeaturesoftheeyes,mouth,andnose.Multitaskinglearningofacquiredfacialfeaturesenableseachfeaturepointtobeshared.Thisapproachnotonlyenablesrapidandaccurateextractionofallfacialfeatures,butalsoimprovestherobustnessoffacialfeaturepointdetection.(3)Conductofflinetrainingonthecollectedfeaturessuchasdriver'seyesopen,eyesclosed,mouthclosed,yawning,andwhethertowearglassesorsunglasses.Then,applyataskconstraineddepthconvolutionnetworktorecognizefacialfeaturesofthecollectedimages,judgetheopeningandclosingstatusoftheeyesandmouth,anduseanAdaBoostclassifiertoclassifythestatusoftheeyesandmouth.Basedonofflinetraining,areal-timemoduleisadopted:onlinetesting.Detectthefatiguecharacteristicsofdriversinreal-timevideosequences,andobtainreal-timeeyeandmouthfeaturelandmarks.Throughthesefeaturelandmarks,thevaluesofeyeaspectratio,eyeclosurepercentage,andmouthaspectratiocanbeobtained.(4)Themethodofweightproportionisusedtofusefacialfeaturesunderfatiguestatus.BasedontheobtainedEARvalue,PERCLOSvalue,andMARvalue,theAbstractIIIparameterMisobtainedbyaddingthemwithdifferentweights.ThenumberofimageframeswithM>0.605isrecordedastheparameterNwithinacertainperiodoftime,andthedegreeoffatigueofthedriverisjudgedbythesizeoftheNvalue.Throughfatiguetestingonvolunteersofdifferentagesonclosedandsaferoads,theexperimentalresultsshowthatthemethodbasedondriverfacialmultifeaturefusioninthisthesiscaneffectivelyusevideoimagestodetectdriverfatiguestatusinrealtime,withanaccuracyof93.1%,whichisofgreatsignificanceforprotectingdriverandvehicledrivingsafety.Keywords:Fatiguedetection;Eyeaspectratio;Percentageofeyeclosure;Mouthaspectratio;Deeplearning目  录 第1章 绪论............................................................................................... 1 1.1 研究背景和意义 ............................................................................... 1 1.2 疲劳驾驶的因素 ............................................................................... 1 1.3 国内外研究现状分析 ....................................................................... 1 1.3.1 主观评价法 ................................................................................. 2 1.3.2 客观检测法 ................................................................................. 3 1.4 研究现状存在的问题 ....................................................................... 5 1.5 研究的内容和方法 ........................................................................... 5 第2章 人脸关键点的检测 ...................................................................... 7 2.1 多任务卷积神经MTCNN网络模型 .............................................. 7 2.1.1 P-Net ............................................................................................ 8 2.1.2 R-Net ............................................................................................ 9 2.1.3 O-Net .......................................................................................... 10 2.2 深度学习的深度卷积网络DCNN模型 ....................................... 10 2.2.1 DCNN网络模型 ....................................................................... 10 2.2.2 level 1 网络结构 ....................................................................... 11 2.2.3 level 2 网络结构 ....................................................................... 12 2.2.4 level 3 网络结构 ....................................................................... 13 2.3 多任务约束的深度卷积网络TCDCN模型 ................................. 13 2.3.1 多任务学习 ............................................................................... 14 2.3.2 TCDCN网络结构 ..................................................................... 15 2.3.3 损失函数 ................................................................................... 16 2.4 MTCNN、DCNN 和 TCDCN 网络模型对比分析 .................... 17 2.5 本章小结 ......................................................................................... 18 第3章 基于深度卷积网络模型的疲劳特征识别 ................................ 19 3.1 神经网络与深度学习 ..................................................................... 19 3.2 离线训练和在线测试模块 ............................................................ 23 3.3 任务约束的深度卷积网络 ............................................................ 25 3.4 驾驶员面部特征点 ......................................................................... 26 3.5 驾驶员疲劳识别特征 ..................................................................... 27 3.6 本章小结 ......................................................................................... 27 第4章 面部特征的标定与计算 ............................................................ 29 4.1 基于dlib 68 个地标及面部特征标定 .......................................... 29 4.2 EAR计算 ......................................................................................... 29 4.3 PERCLOS 值计算 .......................................................................... 32 4.4 MAR值计算 ................................................................................... 33 4.5 疲劳状态在线检测技术路线 ........................................................ 35 4.5.1 KNN分类器 .............................................................................. 37 4.5.2 面部多特征融合 ....................................................................... 38 4.6 本章小结 ......................................................................................... 38 第5章 实验数据和结果 ........................................................................ 39 5.1 环境和数据集 ................................................................................. 39 5.2 驾驶员疲劳驾驶检测系统 ............................................................ 40 5.2.1 软件系统 ................................................................................... 41 5.2.2 硬件系统 ................................................................................... 42 5.3 短期实验 ......................................................................................... 43 5.4 长期实验 ......................................................................................... 45 5.5 不同性别对疲劳检测结果的影响 ................................................ 47 5.6 驾驶员疲劳验证 ............................................................................. 50 5.6.1 疲劳主观问卷调查验证 ........................................................... 50 5.6.2 疲劳检测系统验证 ................................................................... 51 5.7 本章小结 ......................................................................................... 51 结论 ........................................................................................................... 53 参考文献 ................................................................................................... 55 第1章绪论1第1章绪论1.1研究背景和意义由于全球社会经济和智能科技的快速发展，使得人民的生活水平逐步提升。同时车辆生产成本大幅下降，机动车数量与日俱增，造成的交通事故频繁出现，进而导致大量的人员伤亡。研究发现造成交通事故的因素中，驾驶员疲劳驾驶是极为重要的一部分[1]。据国家公安部统计2021年国内汽车保有量达到了3.95亿辆，同时国家统计局在2021年统计年鉴中统计的交通事故共计244674起，造成61703人死亡，250723人受伤，直接造成的经济损失累计131360.6万元；其中因疲劳驾驶导致的交通事故占总交通事故的10%~20%左右[2-3]，占重大交通事故的比例高达40%[4]。不仅我国存在因疲劳驾驶导致的交通事故，而且世界上其他国家也是如此。例如美国国家交通管理局（NationalHighwayTrafficSafetyAdministration,NHTSA）依赖于警方和医院的报告，得出驾驶员因疲劳驾驶导致撞车事故的发生率，NHTSA估计在2017年涉及疲劳驾驶的司机多达9.1万人，导致5万多人受伤，近800人死亡[5]。通过上述分析可知，驾驶员疲劳驾驶是造成道路交通事故的重要因素之一。若能够实时检测出驾驶员疲劳驾驶状态并及时提醒，这样能够很大程度上避免交通事故的发生。因此，疲劳驾驶检测系统的研究十分重要，一种能够准确且迅速判断出驾驶员处于疲劳驾驶状态，并能及时提醒驾驶员注意安全驾驶，对人民生命财产安全的保护和降低交通事故的发生率具有重大意义。1.2疲劳驾驶的因素导致驾驶员疲劳驾驶的原因包括驾驶环境因素和自身因素。驾驶环境包括：驾驶室内长时间不通风、长时间在高速公路上行驶、驾驶室内温度过高等都会导致驾驶员产生睡意；驾驶员自身因素包括：缺乏充足的睡眠、连续4小时以上驾驶车辆、饮酒、吃药等导致驾驶员产生睡意。因此，充足的睡眠或短暂的休息是减轻驾驶员嗜睡和疲劳驾驶的最好方式。1.3国内外研究现状分析河北工程大学硕士学位论文2疲劳驾驶检测方法可以分为主观评价和客观检测。主观评价是通过问卷调查判断驾驶员疲劳的方法[6]，例如斯坦福嗜睡量表和派珀疲劳量表；对驾驶员的面部表情特征进行评分，例如打哈欠、眨眼睛、视线转移、头部姿势等。客观检测分为接触式和非接触式两类。接触式测试主要通过对驾驶员生理状态的特征参数进行检测分析，例如脑电、心电、眼电、呼吸等信号，其好处是检测精度高，不足是检测仪器成本太高，要求司机穿戴复杂的检测设备，而且这些复杂的检测设备对驾驶员驾驶车辆造成干扰。相对于接触式检测，非接触式检测更加便宜、方便；例如基于车辆行为特征检测，检测车辆状态和参数，包括车道位置、方向盘转角、行驶速度、加速度、制动等[7-11]；基于驾驶员面部特征检测，主要通过眼睛纵横比（EAR），眼睛闭合百分比（PECLORS）、嘴巴高宽比（MAR）的值对驾驶员的状态进行判断。本文采用客观检测的方法，基于驾驶员面部多特征融合对驾驶员疲劳驾驶进行检测。1.3.1主观评价法主观评价法主要利用驾驶员面部表情变化、主动应急反应时间、问卷调查等方法对驾驶员驾驶的状态进行判定。主观评价法具有操作简单、花费较低等优点被广泛利用，但是存在误差、实时性较差等劣势。主观评价法包括问卷调查、口头问答、面部表情特征分析等方法（1）问卷调查分析研究者通过对驾驶员填写问卷调查表的形式，对驾驶员的疲劳状态进行分析。Guo等人为了了解军飞行人员的疲劳状况和特点，采用疲劳评定量表（FatigueAssessmentInstrument,FAI）和疲劳量表（FatigueScale-14,FS-14）对924位军事飞行人员进行问卷调查[12]。李晅等人通过随机抽取510名空军飞行人员填写自行设计的飞行人员飞行疲劳状况调查表，疲劳人员出现全身卷懒、腰酸痛、头沉、打哈欠、眨眼等特征[13]。万斌等人对现代矿工心理疲劳状况进行了研究，采用问卷调查的方式对山西高平某煤矿一线矿工进行了调查[14]。田春雨等人通过对海军某舰队373名远航任务舰员填写眼表疾病指数量表（OcularSurfaceDiseaseIndex,OSDI）和视疲劳调查表，分析远航任务舰员眼部疲劳发生率和严重程度[15]。（2）口头问答分析口头问答分析主要对测试者进行简单的询问，其受主观意识的影响存在一定的局限性。Lee等人通过对929名飞行员进行口头问答分析，飞机的飞行方向、天气状况、工作类型等都会影响飞行员的疲劳状况[16]。Filtness等人通过小组讨论的方法对铁路驾驶员的疲劳相关的数据进行归纳，得出疲劳驾驶的原因、后果和第1章绪论3应对措施[17]。1.3.2客观检测法（1）基于驾驶员生理特征的疲劳检测对于驾驶员生物电信号的早期研究主要依赖于脑电信号。Mervyn等人建立支持向量机（SupportVectorMachines,SVM），根据驾驶员的脑电信号（Electroencephalogram,EEG）进行建模，识别了95％的睡意驾驶状态[18]。Lin等人基于无线脑电图的实时脑计算机接口系统和基于脑电图的自组织模糊神经系统来预测疲劳驾驶[19-20]。Dong等人通过嵌入人脑的电极获得脑电信号，并对这些信号进行分析，以确定驾驶员是否处于疲劳状态[21]。Sang等人利用嵌入在方向盘上的心电传感器来采集驾驶者的心电信号，并利用心率变异性（HeartRateVariability,HRV）时频域指数来提高信号的准确性[22]。Lei等人提出一种面向短周期心电信号的方法，提高心电信号的可识别性和频率分辨率的预处理方案，能够在仅5秒间隔的心电信号下有效地识别疲劳特征[23]。Zhang等人提出将不同电极放置在前额，并提取眼球转动的各种特征用于疲劳检测[24]。Bulling等人发现由眼球运动引起的EOG信号是心理活动的良好指标[25]。Hu等人根据眼电图（Electro-oculogram,EOG）中提取与眼睑相关的参数作为数据输入，并利用SVM将驾驶员的状态分为清醒、疲劳和重度疲劳[26]。同时还结合EEG、心电图（Electrocardiogram,ECG）、EOG来提取相应的特征参数来检测疲劳驾驶。Fu等人基于驾驶员司机HRV的睡意检测算法能力分析与基于EEG的睡眠比较[27]。Ahn等人收集EEG、ECG、EOG等多个模态和功能性近红外光谱仪镜数据，对驾驶员的神经生理相关性的状态进行识别[28]。Wang等人结合EOG和EEG信号分析融合熵，检测驾驶员的疲劳状态[29]。王磊等人针对EEG信号的非平稳性，在基于通过利用ReliefF通道选择算法和稀疏表示特征融合算法的基础上，能够有效地分析疲劳脑电的共性通道[30]。杨硕等人通过去趋势的模糊熵值实现对昏昏欲睡的状态进行辨别，研究表明昏昏欲睡状态的熵值比正常状态的值要小[31]。张春翠等人利用非线性复杂度参数，对疲劳状态下的顶叶、额叶、颞叶和认知运动等有关脑区的脑电信号进行特征描述[32]。李增勇等人通过分析司机疲劳行驶过程中HRV功率频谱的变化，结合主观评价分析驾驶员的心脏自主神经功能状态[33]。王琳等人以传感器为载体，收集驾驶员在驾驶车辆的过程中其股二头肌做出相关反应所发射出的生理信号，再将这种生理信号独立分解成肌电和心电两种信号，综合这3个特征参数区分驾驶员清醒或疲劳两种状态[34]。王福旺等人通过对脑电信号进行小波包分析，获得信号中的α河北工程大学硕士学位论文4波，通过统计相对功率谱和眨眼次数，依据脑电和眼电信号的融合特征确定司机是否处于疲劳状态[35]。（2）基于车辆行为特征的疲劳检测当驾驶员疲劳驾驶汽车时，汽车行驶状态会有所改变，进而运用车道偏置、方向盘角度、车速等参数进行疲劳检测。汽车车道偏移量是指在汽车行驶过程中偏移当前轨迹的位移量，一般是运用计算机视觉技术辨认以及检测汽车行驶轨迹。Riera等人设计一个能够检测和跟踪车道偏移量的方法来识别疲劳驾驶，因此车道偏移量也可用作疲劳检测的参数[36]。Takei等人利用方向盘转向角信息对驾驶员的疲劳状态进行识别，通过使用快速傅立叶变换和小波变换对方向盘转角信号进行预处理，进而获得相应的特征[37]。汽车的行驶车速可以通过汽车传感器测量得出，当司机处于疲劳状态时，车速就会发生大幅度变化。DSandberg等人采集汽车的速度、汽车的横向位置、方向盘转角和气动角度等数据，通过前馈神经网络对疲劳信息进行分析，辨别司机是否处于疲劳状态[38]。蔡素贤等人利用控制局域网络（ControllerAreaNetwork,CAN）收集汽车行驶情况信息，获得车辆状态相关的参数，通过随机森林算法对获取的相关参数进行处理，再对驾驶员的疲劳状态进行辨别[39]。张弛等人通过选择与疲劳状况存在重要关联的数据因素，给出数据预处理和因素标准化的方法，利用层次分析法理论实现正确的数据因素权重划分，进而建立疲劳状况辨别模型[40]。屈肖蕾等人研究司机在不同疲劳情况下的转向行为和驾驶状态变量的波动幅度、车速、频度的变化特征，通过统计学方法对疲劳参数显著性变化进行验证[41]。黄皓等人通过驾驶模拟器提供仿真驾车环境，并按照需求设定收集的信息,包括方向盘转动角度、汽车横摆角度、汽车横向位移等，通过自主评分技术，确定实验过程中驾驶者的状况，形成疲劳样本数据库[42]。（3）基于驾驶员面部特征的疲劳检测随着机器视觉和人工智能科技的迅速发展，计算机计算能力有了长足的提升。基于驾驶员面部特征融合的疲劳检测方法日益受到人们的关注。Zhao等人提出一种卷积神经网络（ConvolutionalNeuralNetworks,CNN）进行疲劳检测的算法，该算法的第一网络对人眼训练，将人眼和非人眼进行特征划分，第二网络标定人眼特征点位置，按照特征点位置计算眼睛的睁开度，进而得出PERCLOS值来确定驾驶员的疲劳[43]。Zhang等人利用面部行为分析的检测方法，结合Adaboost分类器和KFC相关滤波器（KernelCorrelationFilter,KCF）将红外收集装置收集的人脸图像实施人脸检测和跟踪，通过级联回归定位特征点提取眼睛和嘴巴区域，利用CNN识别眼睛和嘴巴的外观表现[44]。为进一步提高司机疲劳检测的精确度和时效性，Liu等人采用深度学习的方法分析驾驶员的面部表情，以识别驾驶员第1章绪论5是否处于疲劳状态[45]。其先利用采集的眼睛和嘴巴的大量数据进行模型训练，再使用所建模型对眼睛和嘴巴的状态进行识别，最终通过模糊推理系统推理出司机行驶过程中的疲劳状况。廖明明等人对采集的驾驶员图像进行预处理，然后采用多任务级联神经网络进行人脸检测并提取眼部和嘴部特征；同时设计眼部状态识别网络和嘴部状态识别网络分别对眼睛和嘴巴区域的状态进行识别[46]。苏锦瑾等人经过计算眼睛纵横比与阈值EAR比较以统计眨眼次数，通过嘴巴高宽比与阈值MAR比较统计打哈欠次数，根据驾驶员眨眼的次数和打哈欠的次数判断驾驶员的疲劳状态[47]。刘炜煌等人通过多任务级联卷积网络定位驾驶员的嘴部和眼睛，从驾驶员嘴部和眼睛的图像中获取静态特征信息，结合CNN从嘴部、眼睛光流图中获取动态特征进行分类训练，并通过静态图像实现驾驶员的疲劳检测[48]。周云鹏等人通过眼睛、嘴部的状况、头部的位置描述驾驶者状况，使用模糊系统推理出驾驶者最终的疲劳状况[49]。1.4研究现状存在的问题经过检索国内外关于疲劳检测的文献获悉，检测驾驶员疲劳的方式主要包括：基于驾驶员生理特征、车辆行为特征、驾驶员面部特征等。驾驶员生理特征包括EEG、ECG、EOG、呼吸等信号用作判别疲劳的参数，其优点精度较高，缺点成本较高、要求司机穿戴复杂的检测设备，干扰司机的驾车状况。汽车的行为特征检测主要通过车辆的行驶路径、方向盘转向角、车速等参数确定驾驶员的疲劳状况，该方法不能直接反馈出司机的驾驶状况，受外界影响因素较大。驾驶员面部特征检测主要根据眨眼、打哈欠、点头的频率等来判断驾驶员的驾驶状态，相对于以上两种检测方法，其能够较为直观的检测出驾驶员的疲劳状态，不需要复杂的检测设备，只需要一颗高清摄像头就能够判断驾驶员的驾驶状态。目前，国内外对疲劳驾驶检测研究方法所面临的主要问题：要求驾驶者佩戴复杂的检测装置影响驾驶员驾驶，或仅采用单个特征或参数确定驾驶者是否疲劳驾驶等，而关于上述问题本文将通过基于驾驶员面部多特征融合的方式进行解决。1.5研究的内容和方法本文主要针对基于驾驶员面部多特征融合的疲劳检测系统进行研究，应用深度学习理论、可靠性理论等研究方法，进行以下工作：第1章，绪论。阐述了疲劳驾驶检测方法的研究背景和意义，对国内外驾驶河北工程大学硕士学位论文6员疲劳检测方法的研究现状进行归纳，总结出不同检测方法的优缺点。第2章，人脸关键点检测。阐述了多任务卷积神经网络模型、深度学习的深度卷积网络模型、任务约束的深度卷积网络模型在人脸关键点检测方面的优缺点，对不同的网络模型结构进行详细阐述。通过三种模型的对比，本文采用任务约束的深度卷积网络模型对人脸关键点进行检测，其模型能够解决人脸检测耗时过长的问题，并且能使各个任务的特征点共享。在能够保证快速且精确的检测到人脸及标定，为提取眼睛和嘴巴区域打下了坚固的基础。第3章，疲劳检测。阐述了疲劳检测中神经网络和深度学习之间的关联，对神经网络的卷积层、池化层和全连接层的作用进行了说明，详细阐述了具有代表性的卷积神经网络模型。同时对提出的离线训练模块和在线测试模块的作用进行详细阐述，采取多任务训练、多任务学习的方式能够准确快速获得驾驶员面部关键点以及面部特征，并且提供稳定的人脸图像。第4章，面部特征的标定与计算。采用Dlib的68个地标标记面部轮廓、左眼、右眼、鼻子和嘴巴区域，通过地标点计算眼睛的纵横比和嘴巴的高宽比。同时对眨眼过程、打哈欠过程进行检测，对各自相应的曲线图进行分析。对在线监控工作流程图进行了详细阐述、确定最近邻K值以及描述了面部多特征参数的融合。第5章，实验数据与结果。对实验环境、数据收集、设备软硬件系统进行了详细阐述，为了验证疲劳检测系统的准确性与实时性，展开了两种实验：一种是短期实验，验证疲劳状态下的眼睛和嘴巴特征对应的核心阈值是否有效；另一种是长期实验，在封闭且安全的道路上对测试者进行测试，实时监测测试者的疲劳状态。通过对实验数据的分析及验证，得出基于面部多特征融合的疲劳检测系统的准确率达到93.1%；最后对测试者进行问卷调查，对测试者的疲劳程度进行评估，对疲劳检测系统进行验证。第2章人脸关键点的检测7第2章人脸关键点的检测人脸关键点检测是指在一张包含人脸的图片中，检测出人脸的关键点位置，有的方法能够定位人脸的5个关键点，有的方法能够定位出68个关键点，甚至有的方法能够定位出108个关键点。人脸关键点检测方式主要包括以下三类：利用级联回归、基于活动形状模型（ActiveShapeModel,ASM）和主动外观模型（ActiveAppearanceModel,AAM）以及深度学习。本章将重点阐述利用级联回归的多任务卷积神经网络模型（Multi-taskCascadedConvolutionalNetworks,MTCNN）、基于深度学习的深度卷积网络模型（DeepConvolutionalNetwork,DCNN）和任务约束的深度卷积网络模型（Tasks-ConstrainedDeepConvolutionalNetwork,TCDCN）。2.1多任务卷积神经MTCNN网络模型关于深度学习和面部多特征融合的方法检测驾驶员疲劳的过程中，人脸检测是最重要的一步。由于驾驶员头部姿势的变化以及黑暗的环境因素影响，在实际驾驶过程中很难快速准确检测出人脸的全部特征。因此，疲劳驾驶检测系统既要检测速度快又要检测精度高，为满足这一要求，在检测系统中采用卷积神经网络模型，其可以提高对图像特征识别的精确度，同时可以提高算法的速度。经典的人脸关键点检测模型是MTCNN网络[50]，由于使用级联结构，它能迅速进行人脸检测和对齐，也能准确提取人脸的全局特征，进而得到人脸、右眼、左眼和嘴巴的左右两角位置。原始图像通过MTCNN模型分析后，可以获得人脸框架和人脸五个关键点，并利用先验知识“三庭五眼”的概念，然后再通过人脸框架和关键点坐标获得眼睛、嘴巴和鼻子的区域。MTCNN网络的损失函数主要包括：面部5个关键点、人脸区域分类、边框回归（BoundingBoxRegression）三部分，以下为各个部分的解释：（1）人脸五个关键点：其与边框回归相似，都是利用模型生成的回归值和真值（GroundTruth）两者间的偏移量。因此，可以根据偏移量得出网络输出值和真实值的差值。（2）人脸区域分类：将其定义为一个二分类问题，检测图片中是否含有人脸。（3）边框回归：针对模型生成的各种候选窗口，都可以获得其偏移量的数河北工程大学硕士学位论文8据，这些数据分别是回归的高度和宽度以及回归框的左侧和顶部，可以获得回归与给定窗口两者间的差值。MTCNN的网络结构如图2-1所示，它是由三个级联子网络构成，分别为建议网络（ProposalNetwork,P-Net）、精细网络（RefineNetwork,R-Net）和输出网络（OutputNetwork,O-Net），从粗略到精细依次检测人脸和特征点的位置。图2-1MTCNN网络结构Fig.2-1StructurespecificationforMTCNN2.1.1P-NetP-Net基本构造如图2-2所示，通过图片金字塔获得图像，其图像尺寸各不相同，使用全卷积网络（FullyConvolutionalNetworks,FCN）确定各个12x12的图像中是否包含一张人脸，提取候选面部范围和回归向量的边框；通过边界框回归（Bounding-BoxRegression,BBR）和非极大值抑制（NonMaximumSuppression，NMS）完成这些窗口的筛选。P-Net是区分人脸区域的建议性网络，然后利用人第2章人脸关键点的检测9脸分类器判定这些区域中是否都含有人脸，在这个部分会获得多个存在人脸的范围，然后再把这些范围输入R-Net实现进一步筛选。图2-2P-Net网络Fig.2-2P-Net2.1.2R-NetR-Net的基础结构是一种卷积式神经网络如图2-3所示，相比P-Net网络而言，添加一个全新的全连接层，这样对于输入内容的过滤就会比较严格。当图像通过P-Net时，将生成多个预测窗口。假设将所有预测的图像都被输入到R-Net网络中，网络会将一些比较差的候选框筛选出来。最后，通过BBR和NMS过滤判断结果。由于R-net输出是一个足够精确的面部范围，所以在这种网络技术中，输入是经过精炼和过滤的。因此所有不正确的输入都会被抛弃，然后通过回归和面部关键点定位器完成面部的关键点定位。最后，获得可靠的面部范围，并传输到O-Net网络。与使用全卷积输出1x1x32特点的P-Net网络相比较而言，只是把128的全连接层放在R-Net网络的卷积层之后，让图像特征得到更多的保存。图2-3R-Net网络Fig.2-3R-Net河北工程大学硕士学位论文102.1.3O-NetO-Net相对于R-Net多了一个3x3的卷积层如图2-4所示。O-Net和R-net两者的差别：O-Net结构通过更多的监督来缩小面部的范围，减少回归的面部特征点，最终对面部的5个特征关键点进行输出。在网络结构的末尾，是一个更大的全连接层，保存更多的图像功能，实现人脸识别、校准人脸区域边界回归和人脸特征。图2-4O-Net网络Fig.2-4O-Net2.2深度学习的深度卷积网络DCNN模型DCNN网络模型是将CNN运用于人脸关键点识别的初步展示方法，由Sun等人在2013年设计的[51]。Sun等人运用级联网络的设计思路，设置三种不同的卷积神经网络，以便能更好的获取特征点，并且能更加精确的定位特征。如果使用卷积神经网路训练回归特征点位置，仅仅通过一个网络完成回归训练，可能会出现特征点位置不够准确的情况，而通过级联回归CNN的方式，可以实现分段式特征点的位置标定，从而能够更加迅速、精确地标定人脸关键点。如果使用比较大的网络系统，对特征点的检测会更加精确，但花费时间会有所延长；为了在效率上和性能上寻找一个平衡点，可以采用较小的网络系统以及级联的思想，先通过粗过滤，之后再微调关键点，而DCNN就是采用这种思路。2.2.1DCNN网络模型DCNN网络模型分成三个部分，依次是：level1、level2、level3。level1代表粗糙级别，level2代表精细级别，level3代表更加精细级别，level里面包括不同个数的CNN模型。DCNN模型检测分析，首先经过人脸检测器，裁剪出人脸图片，提取哈尔特征（Haar-likeFeatures）；然后从输入level1的图片中裁剪人脸第2章人脸关键点的检测11图像。以下将分阶段地阐述各个level的具体细节。2.2.2level1网络结构输入图像进入人脸检测器后，level1网络将面部的边界框裁剪出来，接着将裁剪的图像转换成灰度图像，随后将灰度图像尺寸大小转换成3939，最后将其输入到level1网络中，level1的网络结构如图2-5所示。第一层的网络结构是由三种神经网络构成，这三种卷积神经网络依次是：F1（输入包含人脸的图像）、EN1（输入的图像包括眼睛和鼻子）、NM1（输入的图像包括鼻子和嘴巴）。图2-5level1网络结构Fig.2-5Structurespecificationforlevel1F1输入是3939的人脸图像，输出是检测的五个关键点。F1的网络结构如图2-6所示，将包含人脸的图像输出，再通过卷积神经网络，再将10维的特征向量输出。图2-6F1网络结构Fig.2-6StructurespecificationforF1河北工程大学硕士学位论文12F1的基本结构为：第一个卷积特征图选取20，第二个卷积特征图选取40，第三个卷积特征图选取60和第四个卷积特征图选取80。EN1可以用来标定左右眼和鼻子的特征点，所以在模型设计时，输出层的神经元数量是6。输入的图像是根据比例剪裁的，把3939图像的上半部分提取出来，再裁剪出3939图像，并确定裁剪出的图像包括眼睛和鼻子的区域。NM1被用于标定左右嘴角和鼻子的区域。类似的，输出6个神经元以及输入的图像，也是根据3939的大小裁剪出来，裁剪的部分包括嘴巴和鼻子的区域。2.2.3level2网络结构通过level1就可以估计出图像上各个关键点的坐标，level2的目的是将搜索区域缩小，以level1检测的5个关键点为中心，使用一个较小的边界框（boundingbox），从图片中把5个关键点的区域提取出来。level2网络结构如图2-7所示，其网络包含10个CNN，每个CNN有6层。这10个CNN，依次用来检测5个关键点，每个关键点占用两个CNN，最后对检测关键点的位置进行平均分配。图2-7level2网络结构Fig.2-7Structurespecificationforlevel2第2章人脸关键点的检测132.2.4level3网络结构level3利用在level2获得的位置基础上，再次进行裁剪。根据level2网络获悉，5个关键点的区域更加精确化，距离获得关键点的具体位置就更加贴近，level3对level2裁剪后的图片再进行一次更加精确的裁剪，对新裁剪的区域进行特征点检测。level3的结构如图2-8所示，其包括10个CNN，一个特征点检测需要两个CNN。level3与level2的不同是对裁剪范围进行改变，使level3裁剪图片中包含关键点的区域更加具体，关键点识别精度更高。图2-8level3网络结构Fig.2-8StructurespecificationforLevel32.3多任务约束的深度卷积网络TCDCN模型2014年Zhang等人建立了TCDCN网络，该网络最大的贡献就是把多任务学习的思想运用到检测人脸关键点上[52]。在对人脸关键点模型的训练过程中发现某些辅助数据（例如性别、面部表情、是否戴眼镜、环境是否黑暗等），这些辅助数据都有可能干扰模型的训练结果，对检测产生一定的影响。通过TCDCN训练人脸的关键点位置，正是利用多任务学习的方式，来提高人脸特征点检测的鲁棒性。河北工程大学硕士学位论文142.3.1多任务学习多任务学习是TCDCN通过一些辅助数据实现多任务学习，其好处是检测速度快和多种任务共同学习，而且在人脸关键点检测上使用端到端的技术。并通过辅助特征可以更好的标定人脸关键点，如眼睛和嘴巴的实时位置。依据Zhang等人的研究成果表明，对人脸关键点检测技术有一定的帮助。多任务学习（Multi-taskLearning,MTL）的问题就是：不同的任务对应着各自的特征，以及不同的收敛方式，而关于这两种情况，提出的处理方法分别是采用任务约束的深度模型（Task-constrainedDeepModel）和提前停止任务（Task-wiseEarlyStopping）的方法。Zhang等人的模型对处理有覆盖和姿态变化时的效果很好，并且模型也比较简单[52]。为了验证TCDCN对人脸关键点检测的准确性及优越性，将TCDCN、CNN和级联CNN（CascadedConvolutionalNeuralNetworks）进行对比。准备7张不同的人脸图像进行测试，比如性别、面部的朝向、是否戴眼镜、是否微笑等特征。CNN、级联CNN和TCDCN在相同的实验条件下对7张图像进行测试，对眼睛、鼻子和嘴巴的检测结果如图2-9所示。图2-9人脸特征点对比Fig.2-9FacefeaturepointcomparisonCNN在面部朝向右侧或者左侧时对眼睛和嘴巴的定位点存在偏差；例如图像1、图像2、图像3、图像4和图像6的嘴角关键点定位存在偏差，以及图像1、图像3、图像5、图像6和图像7对眼睛关键点定位同样存在偏差，戴眼镜情况下受到一定的影响；而在面部朝向为正面时都不能够准确标定嘴巴、眼睛和鼻子的关键点位置，关键点检测表现力较差。级联CNN在面部不同朝向时对嘴巴的关键点检测相对准确，但是在面部朝向不同时对鼻子的关键点定位存在误差，例如第2章人脸关键点的检测15图29中图像3的鼻子定位点在鼻孔下方没有准确定位到鼻子的位置；戴眼睛的情况下级联CNN同样受到了一定的干扰导致关键点定位出现偏差，例如图29中戴眼镜的图像5定位点出现在眼睛的上方；相对于CNN而言，级联CNN对关键点的检测表现较好，综合情况显示级联CNN的效果一般。TCDCN网络无论面部朝向不同、是否戴眼镜都能够对嘴巴、眼睛和鼻子的关键点进行准确点位，正确区分嘴巴、眼睛和鼻子的区域；因此TCDCN对嘴巴、眼睛和鼻子的关键点检测效果最佳。在TCDCN能够准确的对嘴巴、眼睛和鼻子关键点定位的情况下，对TCDCN进行进一步的测试。图像的面部在不同朝向下对是否戴眼镜、是否微笑和性别进行测试，测试结果如表2-1所示，通过这7张图像的测试结果来看，TCDCN对图像是否戴眼镜、是否微笑和性别均能检测正确，具有较强的表现力。表2-1TCDCN测试结果Table2-1TCDCNtestresults类型图像1图像2图像3图像4图像5图像6图像7戴眼镜否否是否是否否微笑否是否否否否是性别女性男性女性女性男性男性女性姿势右侧正面正面左侧正面正面右侧2.3.2TCDCN网络结构TCDCN的网络比较简单，如图2-5所示。其网络的输入图像是4040的灰度图，图片经过55卷积层和22最大池化层之后得到181816的图片；181816的图片分别经过两次33卷积层和22最大池化层后依次得到8848和3364的图像，再经过一次22卷积层后得到2264的图像，激活函数选择双曲正切函数（tanh）。而2264的多层经过全连接层后变成100个共享特征，因此可以把整个过程看作对图像共享特征的提取，末尾对这些共享特征实现了不同的提取。河北工程大学硕士学位论文16图2-10TCDCN网络结构Fig.2-10StructurespecificationforTCDCN该模型将关键点输入所有的子程序之中，从而使各个子程序都能共用到该图像的特征，实现特征共享。通过线性回归实现关键点检测，其余选择逻辑回归的方式，最后将人脸五个关键点的结果输出。2.3.3损失函数将多个任务的特征共同享用就是TCDCN网络的主要优点，线性回归模型用来处理驾驶员面部特征关键点的检测问题，而逻辑回归用来处理面部特征分类问题。在传统的MTL中，每个任务都具有同等重要的程度，其目标方程如2-1所示：1argmin11,;TttTNttttiiwtiyfxww（2-1）相对而言，每个任务t的重要性都是相同的，但是针对更多任务学习时，由于各个任务学习难易有所不同，所以使用同样的损失权重，将使得下一个任务无法收敛。针对多任务学习时，不同的任务处理方式进行了调整，其权值的目标函数如2-2所示：aargmin11,;,;raANNrrraaaaiiiiWWiiaAyfxWyfxW，（2-2）主任务的损失函数是公式2-2的左侧部分，即人脸关键点检测的损失函数，其余各子任务的损失函数是公式2-2的右侧部分。针对人脸关键点检测任务，结合了三个子任务，分别是：性别、脸部位置、戴眼镜，最终目标函数如2-3所示：222111,;log|;NNTrraaaaiiiiiiiaAtyfxWypyxWW（2-3）第2章人脸关键点的检测17平方和误差是公式2-3中最左侧的部分，是人脸关键点的损失函数，分类任务是公式2-3中间的部分，使用的是交叉熵误差，正则项是该公式2-3中最右侧的部分。多任务学习中各个任务收敛的速度不同，在模型训练过程中是一个极为重要的问题，使用提前停止任务的方式。如果在某个子任务中取得良好的成绩后，对于主任务来说这个子任务就没有任何意义，那么就可以终止这个任务。关于自动终止子任务的计算公式如2-4所示：1..1...()()min()min()()()taaajtktrvaljttrtaaatajttrtrjtktritkkmedEjEtEjEjEjkmedEj（2-4）公式2-4中的表示阈值，公式乘积左侧为训练误差的趋势，右侧为泛化误差与训练误差之比，当阈值大于两项乘积时，则该任务终止。TCDCN通过多任务学习来识别人脸关键点。在人脸关键点识别任务中，多任务学习的主要问题是：根据不同任务间的难度和收敛速度的差异，提出一种新的目标函数和提前停止方法，并进行相应的改进。2.4MTCNN、DCNN和TCDCN网络模型对比分析通过对MTCNN、DCNN和TCDCN进行对比结果如表2-2所示。在表2-2中，可以发现MTCNN的主要特征是：能够同时完成人脸检测和人脸关键点标定，且能够挑选难以学习的样本（HardSample）；缺点为：耗时太高且定位一般。DCNN的网络的优点为：首次将深度学习引入关键点检测领域；缺点为：卷积神经网络的数量太多，耗时太长，关键点定位一般。TCDCN的好处在于：完成了多任务的训练学习后，可以迅速标定关键点。因此，检测系统通常选用TCDCN网络为脸关键点的检测与标定。表2-2各网络人脸特征点检测以及定位的优缺点Table2-2Theadvantagesanddisadvantagesoffacialfeaturepointdetectionandlocalizationineachnetwork方法优点比例缺点MTCNN可以同时进行人脸检测和人脸关键点定位两个任务，且能够进行挑选难以学习的样本。耗时太高且定位一般DCNN首次将深度学习引入关键点检测领域卷积神经网络的数量太多，耗时太长，关键点定位一般TCDCN实现了多任务的训练学习，且能够快速定位关键点。在本文场景中无问题河北工程大学硕士学位论文182.5本章小结本章节主要阐述了人脸关键点检测技术在神经网络和深度学习领域中的应用，并阐述了关键点检测技术在计算机等不同应用领域强大的表现能力。并阐述MTCNN、DCNN和TCDCN在人脸关键点检测领域的应用，对各自网络结构以及优缺点进行阐述；接着对MTCNN、DCNN和TCDCN在人脸关键点测试中进行比较，经过多次实验验证之后，最终确定TCDCN多任务学习作为检测驾驶员人脸关键点的网络模型。第3章基于深度卷积网络模型的疲劳特征识别19第3章基于深度卷积网络模型的疲劳特征识别对驾驶员疲劳检测系统中应用的神经网络和深度学习进行了详细的阐述，对比各个网络模型的优缺点，本文采用任务约束的深度卷积网络模型对疲劳特征进行识别。提出离线训练和在线测试模块，增加疲劳检测的实时性以及准确性；通过多任务训练可以准确获得驾驶员的面部特征，提供稳定的人脸图像。3.1神经网络与深度学习机器模型中存在一种学习类型被称作神经网络，其模仿动物神经网络的动作特性，运用数学模型以实现分布式网络并行信息处理的目的。典型的神经网络包括输入、输出和中间（也叫隐藏）三层，其结构如图3-1所示，从左至右依次是输入层、隐藏层和输出层。图3-1神经网络结构图Fig.3-1Neuralnetworkstructurediagram河北工程大学硕士学位论文20一般来说，神经网络的架构可以分为四类：前馈神经网络、循环网络、对称连接网络、卷积神经网络。卷积神经网络包括卷积层、池化层和全连接层三个网络层结构，以下将分别说明各个网络层结构。（1）卷积层卷积作为最有效地提取图像信息特征的方式，通常采用逐点式，其卷积核为正方形，对图像上所有像素点进行遍历。各个像素值表示为图像与卷积核相重合的位置，然后对卷积核中对应的权值进行相乘，最后对每个权值乘积相加同时再加上偏置就是图像输出的每一个像素值。其中灰度图和彩色图在包含卷积核的数量方面可能是单个或者多个，比如单通道（这里单通道是指灰度图）的卷积核数为1个，如图3-2所示。图3-2输入的灰度图是551图片，其中1表示单通道，55表示分辨率，而灰度值是5行5列，卷积核是331的。用331的卷积核对551的灰度图片进行卷积计算，偏置项=1b，则求卷积结果是：-11+00+12+-15+04+12+13041511。图3-2单通道卷积层Fig.3-2Singlechannelconvolution（2）池化层池化层是某种类型的降采样，而非线性池化函数有不同的种类，但其“最大池化”是最普遍的。输入的图像被池化层裁剪出许多个矩形区域，并且输出每个区域的最大值。池化层会进一步降低信息的空间尺度，随之图像参数的数量和运第3章基于深度卷积网络模型的疲劳特征识别21算量也会相应的降低很多，进而防止出现过拟合。使用池化层的效果与其输入相比大大减少了运算量，因为池化层的输入主要是模拟人的视觉系统。对于卷积神经网络普遍认为池化层三种作用：一是特征不变性。采用池化方式让模型比较注重一些特征而不是特征精确的坐标，其中不变性包含平移不变性、旋转不变性和尺度不变性。例如图3-3所示，输入1,0,4,6、2,3,6,8、3,1,1,2、1,0,2,4经过最大池化后会获取6、8、3、4，不改变其伸缩的方向，原来的神经元在通过最大池化后输出的数值会变大，但在通过伸缩（尺度变换）后，最大池化输出最大值的概率可能在该神经元上。二是下采样（特征降维），相当于池化在空间区域内进行维度约减，模型就可以得到更加宽泛的区域特征，但会使输入下一层的维度减小，进而减少图像的运算量和参数数量。一定程度上避免过拟合现象的发生。图3-3池化层Fig.3-3poolinglayer（3）全连接层全连接层在卷积神经网络中具有分类器的功能，假如卷积层、池化层和激活函数层等只是将原始数据投射到隐藏层特征空间，而全连接层则是将学到的分布式特征表示投射到样本标记空间。其主要功能就是将前层（卷积、池化层等层）运算得出的特征空间投射样本标记空间。简洁的讲便是把所有特征合并为一个值，其优点在于降低特征位置对分类结果的干扰，进而提高整个网络的鲁棒性。在整个卷积神经网络的最后通常会设置全连接层，负责把卷积输出的二维特征图转化成一维的某个向量，进而完成端到端的学习流程，即输入一张图片或者一段语音，输出一个向量或信息。为奠定本文的研究基础，将在本小节阐述现有研究者应用较多且具备一定代表性的网络模型。河北工程大学硕士学位论文22（1）LeNet-5LeNet-5，此名称取自于研究者LeCun的姓名，5是作者第五次改进后的模型结构，在LeNet-5之前还有LeNet-4和LeNet-1鲜为人知。LeNet-5神经网络主要应用于识别手写数字和机器印刷字符。LeNet-5描述了图像中像素特征之间的关联可以通过参数共享的卷积方法所获得，并且采用卷积、下取样（池化）以及非线性映射等的组成架构，是当前大多数深度图像识别网络的基石。LeNet-5虽然是早期提出的一种小网络系统，却涵盖多数深度学习卷积神经网络的基础：卷积运算层、池化层和全连接层。而LeNet-5中总共包括7层（输入层不作为网络结构），它们由2个卷积运算层、2个池化层和3个全连结层组合成，其中层和全部连接层的核尺寸分别代表采样范围和连接矩阵尺寸。LeNet-5模型的特点：实现卷积、池化、非线性映射三层序列的组合，为目前深层卷积系统提供支持。（2）AlexNetAlexNet是由Geoffrey和他的研究生Alex首次提出的，并于2012年在ImageNet大规模视觉识别竞赛（ImageNetLargeScaleVisualRecognitionChallenge,ILSVRC）比赛中夺得冠军。Alexnet共有八层结构，前面五层为卷积层，后三层为全连接层，并在激活方法中选择非直线非饱和度的relu函数，训练过程中梯度衰减速度变化时，relu函数比常规神经网络中选择的非直线饱和度参数要快很多。同时AlexNet还在双GPU上工作，大大提高整个网络系统的计算效率；其引入的局部响应归一化方法（LocalResponseNormalization,LRN），对非饱和函数relu而言，并不要求对其输入加以标准化，而Alex等人则认为在relu层添加了LRN，就会产生一定形式的横向抑制，改善网络的泛化性能；而池化方法则基于重叠池化（OverlappingPooling），即池化窗口的大小大于步长，因此池化都有重合部分，这种重叠的池化方法与一般无重叠的池化方法相比更具有有效性，并能够减少过拟合。（3）VGGNetVGGNet是由牛津大学的计算机视觉中心与谷歌DeepMind研究院联合开发的深层卷积计算网络，其网络深入研究了卷积神经网络的计算与其特性之间的关系，并使用可以反复堆叠的小型卷积核和最大池化层，从而成功建立16~19层深的卷积计算神经网络。在2014年的ILSVRC赛事上获得亚军以及定位项目的冠军。VGGNet在图像特征提取方面依旧被广泛应用，该网络在top5上的错误率仅为7.5%。而VGGNet通过33的卷积核和22的池化核来改善其网络的结构和特性，卷积核层数上的增加并不会引起参数在计算量上的巨增，主要问题在于末尾的三个全连接层汇集大部分的卷积核层数。因此，1个55的卷积层等同于2个串联的33卷积层，1个77的卷积层等同于3个串联的33卷积层，而3第3章基于深度卷积网络模型的疲劳特征识别23个串联的33卷积层与1个77卷积层的感受野是相同的，但是在参数数量方面3个串联的33卷积层只是1个77卷积层的一小部分。3个串联的33卷积层具有3个非线性操作，而1个77卷积层只有1个非线性操作，这也造成了3个串联的33卷积层在特征学习方面具有更好的表现能力。（4）GoogLeNetAlexNet在2012年ImageNet的图像分析比赛拿到了冠军，这也导致深度学习和卷积型神经网络研究获得高速进展。在2014年的ImageNet图像分析比赛上，GoogLeNet拿到了冠军的好成绩，其模型参数约为AlexNet的十二分之一。GoogLeNet的发展首先是因为Inception功能，而整个GoogLeNet的核心结构是由几个Inception功能所堆砌而成。在GoogLeNet以前的卷积计算神经网络，一般都是由多个卷积层和池化层所堆砌而成，然后接入一个或多个全连接层来预测输出。在卷积神经网络全连接层之前的卷积层和池化层的主要目的是为了获取各种图像特征，而这种图像特征对于在全连接层中的整个系统来说就被拉成一堆向量，通常这也造成了整个系统数据主要集中在全连接层，从而避免了过拟合，而在全连接层中一般都是采用丢弃（Dropout）的方法来减少过拟合的可能性。另外，池化方法主要包括平均池化层和最大池化层二种。平均池化层主要保存图像的背景数据，而最大池化方法的目的是保存图像数据，池化层的主要目的则是减少特征与网络参数，在GooLeNet之前通常使用最大池化层。但是最大池化层可能会造成空间信息的损失，大大降低模型的表达能力，为了解决此问题，Lin等人于2013年发明了Inception，Inception模块，主要在CNN中增加一个额外的卷积层，利用Relu作为激活函数，其主要作用是在不牺牲网络模型性能的前提下，实现网络特征的降维，减少计算量，有利于训练更深更广的网络[53]。3.2离线训练和在线测试模块本文研究的结构如图3-4所示，该方法包括以下两部分：离线训练：为了能够准确且快速实现驾驶员的疲劳识别，进行离线训练。驾驶员疲劳状态通过面部特征来识别，比如睁开眼睛的大小和打哈欠时张嘴的程度。因此，本文首先收集了驾驶员戴眼镜或太阳镜的驾驶员面部特征图像，例如睁眼、闭眼、张嘴的程度等。然后，应用任务约束的深度卷积网络（TCDCN）算法对所采集的每幅图像进行面部特征识别。该算法的优点是可以同时进行多任务学习（包括性别、是否戴眼镜或太阳镜以及面部姿势），这些辅助属性可以更好的帮助定位面部特征点。此外，本文还使用两种类型的闭眼时间百分比（PERCLOS）方法，如图3-4所示。PERCLOS1是戴眼镜或不戴眼镜睁眼时计算的值，河北工程大学硕士学位论文24PERCLOS2是驾驶员戴眼镜或睁眼状态时计算的值，也可以实时计算出嘴巴高度和宽度的比值（MAR）。最后，本文采取上述三种类型的数据作为样本进行训练，三种类型的数据分别为：睁眼时的PERCLOS1，闭眼时的PERCLOS2以及嘴巴高度和宽度的比值。使用AdaBoost分类器来确定眼睛是睁开还是闭合，以及嘴巴是张开还是闭合。AdaBoost是自适应增强功能的简写，这是由YoavFreund和RobertSchapire制作的一个机器学习算法。其核心思想是为相同的训练集训练不同的分类器（弱分类器），然后将这些弱分类器结合在一起，形成更强的分类器[54]。图3-4离线训练和在线测试模块Fig.3-4Offlinetrainingandonlinetestingmodule在线测试：是一个实时模块，从实时的视频序列中检测驾驶员的疲劳状态。将所有视频图像导入TCDCN中识别驾驶员的面部，实时获得眼睛和嘴巴的特征地标[52]。然后，使用离线训练的AdaBoost分类器将驾驶员的眼睛和嘴巴的状态进行判断[55]。最后，根据一段时间内的平均闭合时间（或张嘴时间）超过选定阈值的情况，对驾驶员是否困倦进行分类。此外，为了更准确地识别驾驶员疲劳，TCDCN可以消除戴眼镜、头部姿势等因素的影响。第3章基于深度卷积网络模型的疲劳特征识别253.3任务约束的深度卷积网络TCDCN算法的基本结构如图3-5所示。对于输入一个4040的灰度图像的特征获取将分为四个层次：4040的灰度图像分别经过55、两个33和22不同尺寸的卷积层，其使用的三个池化层都是22，最后一次特征提取不再使用池化层；全连接层的目的将相关的特征层进行分类，之后将分类好的特征层输出；把分类好的特征层分给不同任务处理，任务一代表性别，任务二代表面部位置，任务三代表戴眼镜。激活函数选择绝对值的双曲正切，并进行四次卷积后在全连接层可以获得特征向量。这些特征向量可以在计算中与多个任务共用，包括通过线性回归得到地标点的正确位置，以及通过多元逻辑回归处理多个其他任务，例如性别、面部位置、戴眼镜等。图3-5TCDCN算法框架Fig.3-5ThearchitectureofTCDCN通常来说，驾驶员的面部特征在图像中时，首先会检测到面部在图像中的位置，然后确定性别、面部特征的位置、驾驶员是否戴眼镜等。当系统执行这些任务时，它们通常需要设计独立的算法来解决不同的任务；但是，TCDCN可以设计一个复杂的网络来同时完成所有的任务，并利用这些任务之间的关系。多任务学习的难度在于不同的任务具有不同的特征和收敛速度。本文提出的方法优于现有的方法，特别是在处理严重遮挡和位置变化的情况，并降低了模型的复杂性。为能够让多任务学习上的特征得到应用，例如驾驶员的面部、面部的位置、性别、是否戴眼镜等信息，用多个网络训练不同任务的数据集，达到特征共享的目的。通过多任务学习的方式，TCDCN网络整个域的特征参数得到共享，而不是适应特定的任务域。在TCDCN的训练阶段，使用多任务面部地标（Multi-TaskFacialLandmark,MTFL）数据集[56]作为训练数据，该数据集共有12995张人脸图像。图像上标定了面部位置、性别、是否戴眼镜和头部姿态的属性。TCDCN网络使用动态参数调整来达到模型的最优化，并在训练过程中引入河北工程大学硕士学位论文26一个损失函数，目的是为了避免参数的不同对TCDCN网络模型训练的干扰。本文主要任务的损失函数是最小二乘法，交叉熵损失函数作为次要任务使用。主要的损失函数如3-1所示：21(,())(())NiiiLyfxyfx（3-1）公式3-1中L为损失函数，i表示第i个样本，N表示样本总数，y为预测值，()fx为真值。辅助任务（性别、面部位置、是否戴眼镜）的交叉熵损失函数如3-2所示：(,(|))log(|)LpXpX（3-2）公式3-2中损失函数L为分类条件下样本X的概率(|)pX达到的最大值。通过对TCDCN的训练，可以准确地获得驾驶员的面部，为以下的算法提供了稳定的人脸图像。3.4驾驶员面部特征点在本文中，疲劳是由驾驶员的眼睛和嘴巴状态来实时检测，获取眼睛和嘴巴的地标和形状是关键。Dlib是一个用于机器学习算法和面部地标检测的开源工具包。在这项研究中使用Dlib的训练模型，用OpenCV校准和成像68个地标，并在面部绘制68个地标。如上所述，TCDCN网络采用多任务学习模式学习面部相关的属性，能够更好检测到面部特征点的位置区域。具有相关属性的面孔姿态由人脸68个地标表示，如图3-6所示。整个面部由68个点组成，其中关键特征区域（比如眼睛、嘴巴和鼻子）也被标注出来。左眼和右眼各占用6个关键点，通过这6个关键点可以区分驾驶员是否眨眼睛；嘴巴的区域共由20个关键点组成，也可以通过这20个关键点之间的关系判断驾驶员是否打哈欠；鼻子部分总共占用9个关键点，本文不对鼻子的关键点以及除眼睛和嘴巴区域以外的关键点做讨论。下一章节会着重揭示了眼睛和嘴巴关键点的作用。图3-6人脸特征点标定Fig.3-6Facefeaturepointcalibration第3章基于深度卷积网络模型的疲劳特征识别273.5驾驶员疲劳识别特征在回顾相关文献后，驾驶员面部表情有两种普遍的疲劳状态：打哈欠（嘴巴张开，在较长时间内保持这种状态）和眨眼（或者稍微闭上眼睛，眨眼增加或变慢）。本文研究主要关注眼睛睁闭状态（眼睛纵横比：EAR）、眼睛闭合百分比（眼睑覆盖眼球的比例：PERCLOS）和嘴巴高宽比（MAR）、权重比例之和（M）和视频帧数N。基于这些数据，可以实时计算出驾驶员的注意力集中程度，并可以及时分析驾驶员是否处于昏昏欲睡的状态。Ru等人提出一个利用面部地标进行实时眨眼检测的模型概念[57]。然而，检测性能可能受以下实时的限制：（1）当驾驶员戴眼镜时，EAR值波动很大；（2）处理图像像素具有挑战性。图像的像素值越低，内存占用越小，处理的速度就越快，但是图像需要一定的分辨率才能提高识别能力。综上所述，本文提出的方法不同于传统的眨眼图像处理方法。使用TCDCN的EAR、PERCLOS和MAR是一个更简洁的解决方案。无论驾驶员是否戴眼镜，该方案都能准确识别眼睛的特征点。3.6本章小结本章阐述了神经网络和深度学习之间的相互作用，对卷积神经网络的卷积层、池化层和全连接层进行了详细的阐述以及对一些常见且具有代表性的卷积神经网络模型进行阐述，例如LeNet-5、AlexNet、VGGNet和GoogLeNet网络。同时本文所采用的TCDCN中的离线训练和在线测试模块进行了详细的阐述，离线训练模块是为了能够准确且快速的实现驾驶员的疲劳识别进行的训练；在线测试模块是一个实时模块，从实时的视频序列中检测驾驶员的疲劳状态。通过任务约束的深度卷积网络，对TCDCN算法框架进行多任务训练，多任务训练可以准确快速地获得驾驶员面部的关键点以及面部特征，为后续的检测提供稳定的人脸图像。本章节最后对驾驶员面部标志及辅助任务、驾驶员疲劳识别特征和眼睛、嘴巴状态识别的突出特征进行了简单的阐述。河北工程大学硕士学位论文28第4章面部特征的标定与计算29第4章面部特征的标定与计算4.1基于dlib68个地标及面部特征标定Dlib[58]工具包中含有机器学习算法，它在很多机器学习领域都有应用，并协助人们处理实际问题。在Dlib工具包中，由面部68个地标对检测的面部标志进行对齐如图4-1所示。在人脸的基准上将左眼、右眼、嘴巴以及面部轮廓等区域标定出来，因此称之为人脸对齐，例如点37-40、43-46、49-68分别为左眼、右眼、嘴巴的基准关键点，获取当前检测图像对应位置的关键点。总之，Dlib的68个地标标记是根据左眼、右眼、嘴巴的动态变化，保证在面部五官不发生变形的基础上，使人脸与基准人脸重合。图4-168个人脸地标Fig.4-168humanfacelandmarks4.2EAR计算据统计，普通人每分钟眨眼15次左右，两次眨眼中间大约有4秒的间隔，嗜睡时眨眼的频率会加快。单位时间内累计的眨眼次数称之为眨眼频率，其计算如公式4-1所示：BlinkPNBN（4-1）河北工程大学硕士学位论文30公式4-1中，BN为时间段内累计的眨眼次数，由EAR来确定，视频帧数用PN表示。基于Dlib工具包获取的面部标志，采用六个定位点对每只眼睛进行标定如图4-2所示。可以根据眼睛的坐标位置计算出EAR的值，计算公式如4-2所示：263514||||||||2||||PPPPEARPP（4-2）其中，2P、3P、5P、6P关键点为纵坐标的值，1P、4P关键点为横坐标的值。EAR值取左眼EAR值和右眼EAR值得的平均值。公式4-2中2635PPPP表示上眼皮与下眼皮之间的垂直距离，即为眼睛的高度；142PP是计算左眼角到右眼角的水平距离，即为眼睛的宽度。因为垂直距离有两组数据，而水平距离只有一组数据，为了确保两组数据的权重一致性，所以水平距离要乘以2，依靠眼睛纵横比确定驾驶员是否眨眼。图4-2眼睛关键点标定Fig.4-2Eyekeycalibration一次完整闭眼过程的EAR变化曲线如图4-3所示，图中横纵坐标分别是视频的帧数和EAR值，设置EAR阈值为0.2，通过K最近邻（KNearNeighbor,KNN）分类器将EAR<0.2的值进行权值计算，权值比重为0.2。EAR值取左眼的EAR值（leftEAR）和右眼的EAR值（rightEAR）的平均值，模拟一组驾驶员眨眼或闭眼计算EAR变化曲线，如图4-4所示。从图4-4曲线图中可以看出有两次眨眼和三次闭眼过程，帧数在34到38帧和118到122帧为眨眼过程，171到184、300到315和449到462帧为闭眼过程与模拟过程中眨眼和闭眼的次数相同，分析结果如表4-1所示。从模拟驾驶员眨眼或闭眼以及实际疲劳检测过程得到的EAR曲线图，可以得出:系统在检测驾驶员眨眼或闭眼过程中，几乎不存在漏检或者误检，检测的实时性和计算的准确度达到了较高的水平。第4章面部特征的标定与计算31图4-3EAR曲线图Fig.4-3EARcurve图4-4EAR综合曲线图Fig.4-4EARcomprehensivecurve河北工程大学硕士学位论文32表4-1EAR曲线分析结果Table4-1Curveanalysisresults眼睛状态区段帧数眨眼34~385眨眼118~1225闭眼171~18414闭眼300~31516闭眼449~462144.3PERCLOS值计算卡内基梅隆研究院的研究员经过长时间的测试与验证，最终给出测量疲劳的物理量值PERCLOS，并得到了NHTSA的认可。PERCLOS定义为一定时间内眼睛的闭合度，PERCLOS是疲劳检测最有效的参数。由于眼睛的大小因人而异，眼睛的范围因受所在环境和头部运动的限制，眼睛的睁开程度是相比于自身的最大睁开状况而言的，因此可将时间转换为视频帧数进行计算[59]。随着持续时间的延长，当眼睑覆盖瞳孔面积超过一定的百分比时被定义为在一定时间内闭眼的时长，它是一个度量疲劳状态的有效手段。由于眼睛的大小因人而异，眼睛的大小受到环境因素和头部活动的限制而动态变化。时间可以转换为视频的帧数进行计算，驾驶员的嗜睡程度可以根据阈值进行判断[60]。目前，PERCLOS方法有三种判断眼睛闭合的不同准则，分别为EM准则、P70准则、P80准则。在实际研究中，研究者普遍认为最有效的是P80准则，P80准则表示一定时间内眼睑覆盖眼球的百分比超过80%的比例。PERCLOS原理曲线的横坐标表示时间t，单位为秒；纵坐标为瞳孔被眼睑覆盖的百分比。详细的测量方法是：用1080的摄像头获取驾驶员的面部图像，定位出每一帧图像的眼睛区域，对眼睛特征区域进行提取，通过计算在一定时间内眼睑覆盖眼球的百分比超过80%的帧数，确定闭眼所占总时间的比例。一周期的闭眼百分比如式4-3所示：320550(~)*100%ttPERCLOSEtttt（4-3）闭眼时长如式4-4所示：0532(~)Ttttt（4-4）同样，在一定时间内可以计算得到PERCLOS值和眼睛的平均闭眼时长，假设在一定时间T内包含有n个周期，kT为k个闭眼周期内眼睑覆盖眼球的比例超过80%所用的时间，当kn时可以获得公式4-5：第4章面部特征的标定与计算331nkkTPERCLOSET（4-5）平均闭眼时长如式4-6所示：1nnKkTt（4-6）通过连续5帧图像的EAR值，得出眼睑覆盖眼球的平均百分比为PERCLOS值，PERCLOS值的曲线图如图4-5所示，横坐标为视频的帧数，纵坐标为眼睛闭合的百分比，虚线代表的值为0.8。PERCLOS的计算过程是依次取EAR值的每一帧，每连续5帧循环一次计算，步长为1。图4-5PERCLOS曲线图Fig.4-5PERCLOScurve4.4MAR值计算驾驶员嘴巴状态通常有三种情况，分别为闭合、说话和打哈欠，在疲劳状态河北工程大学硕士学位论文34下，人会频繁的打哈欠。提取嘴巴区域的关键点类似于眼睛区域疲劳特征的提取，嘴巴区域的关键点分布如图4-6所示。嘴巴的高度H和宽度W计算公式如4-7所示：maxminmaxmin||||WxxHyy（4-7）H与W的比值MAR如式4-8所示：HMARW（4-8）公式4-8中的maxx、minx分别为68个面部关键点中的第54和第48关键点的位置，取两者之间差值的绝对值为嘴巴的宽度，记作W；68个面部关键点中第56个和第58关键点表示maxy以及第50个和第52个关键点表示miny，同样两者之间的差值的绝对值为嘴巴的高度。打哈欠的频率yawnf如式4-9所示：YyawnTNfN（4-9）YN表示在T时间段内累计打哈欠的次数，TN表示视频的帧数。图4-6嘴巴关键点标定Fig.4-6Mouthkeycalibration驾驶员嘴部状态的检测如图4-7所示，横坐标为视频帧数，纵坐标为MAR值。图4-7（a）为驾驶员嘴巴闭合状态，MAR值在0.350到0.375之间；图4-7第4章面部特征的标定与计算35（b）为驾驶员嘴巴说话状态，MAR值在0.375到0.60之间；图4-7（c）为驾驶员嘴巴打哈欠状态，MAR值在0.60以上。通过KNN分类器将MAR大于0.60的值进行权值计算，权值比重为0.1。图4-7嘴巴状态曲线图Fig.4-7Mouthstatecurve4.5疲劳状态在线检测技术路线如前所述，驾驶员疲劳状态是一个动态变化的过程，需要实时进行检测。本文提出一种判断驾驶员疲劳状态的方法，与传统的单一疲劳指标相比，该方法具有识别精度高和可靠性好的特点。驾驶员疲劳状态在线监测系统的流程图如图4-8所示，系统工作过程如下：河北工程大学硕士学位论文36图4-8在线监控工作流程图Fig.4-8Onlinemonitoringworkflowchart第4章面部特征的标定与计算37（1）摄像头采集驾驶状态下的视频输入系统中，系统对视频中的每一帧图像进行检测，筛选并过滤包含驾驶员面部的图像。（2）筛选出含有驾驶员面部的图像并进行ROI特征提取。（3）针对提取到的ROI面部区域特征，获取眼睛纵横比和嘴巴高宽比，计算EAR和MAR值。根据得到的EAR值，计算PERCLOS值，并将得到的EAR、PERCLOS和MAR值分别应用于KNN。（4）通过实验测试分析得出，将EAR的阈值设置为0.2，根据P80准则，将PERCLOS阈值为0.8。（5）当MAR值大于0.6时，表示驾驶员打哈欠，进而将MAR阈值设置为0.6。（6）将EAR和PERCLOS的权值分别设置为0.2和0.7，MAR值为辅助值，其权重设置为0.1。（7）通过实验测试分析可知，将M阈值设置为0.605较为合理。在这一部分中，N被记录为M>0.605的帧数，即驾驶员在一定时间内闭上眼睛累积的时长。（8）根据实验结果，将驾驶员状态划分为三种情况，当0<N<=25时为清醒状态；当20<N<=50时为疲劳状态；N>50为重度疲劳状态。4.5.1KNN分类器利用输入数据训练一个KNN分类器。KNN算法[61]广泛应用在数据的分类和处理领域中，同时是一种实用的分类器模型，其功能是能够划分和识别任何指定的样本数据点。完成这一目标需经历两个过程，一是收集完成目标所使用的数据集，二是设置K个分类阈值来求得样本数据参考点与其他任意数据点之间的距离。首先，在KNN分类器模型中，经常以数据集(,)Dxy与设置阈值K的样本参考点之间的计算距离当作被测数据的参照指标，从而可以更高效地处理样本点(,)xy之间的匹配优化问题。然后，再选择欧式距离(,)Dxy用作被测样本数据的衡量标准，欧式距离(,)Dxy的计算公式如4-10所示。22221122n1)()()...()()nniiiDxxyxyxyxy（,y（4-10）再选择K的数值。在KNN算法中，计算的精确度较大程度上取决于K值的选择。所以，一旦数据集分类器中的K值设置过大或过小，都有可能使分类结果出现错误，不利于分类模型的最优拟合结果。针对不同的数据集，并没有一定的具体应用数值（通常默认设置为5）。通常情形下，可以根据样本数据集参数的分河北工程大学硕士学位论文38布情况，选取较小的参数。另外，经过反复交叉迭代验证后，可得出一个适合的K值。K值的选择对算法结果有显著的影响。如果K值很小，则相当于在一个小的领域中使用训练示例进行预测。在极端情况下，1K，测试示例只与最近的样本相关，并且训练的误差非常小。若样本遇到噪声，那么预测是错误的，而且测试的错误非常大。当K的数值比较小时，会出现过拟合；当K数值比较大时，则相当于在一个更大的区域中使用训练示例加以检测。在特殊情况下nK，如果测试示例是训练数据集中实例数量最多的类，也会出现过拟合。本文一般选择较小的K值，K值为7。4.5.2面部多特征融合对识别出驾驶员面部的五个特征点，由ROI以特征点为中心对眼睛、嘴巴和鼻子的特征区域进行提取，这些特征区域经过TCDCN获取眼睛和嘴巴的特征地标，再由离线训练的AdaBoost分类器对睁眼、闭眼、张嘴和闭嘴状态进行分类，筛选出闭眼和张嘴的图像。闭眼和张嘴的图像根据特征地标计算出EAR和MAR值，在EAR值的基础上计算出PERCLOS的值；利用KNN对EAR、PERCLOS和MAR进行分类，筛选出EAR小于0.2、PERCLOS大于等于0.8和MAR大于0.6的值。对EAR、PERCLOS和MAR特征参数进行融合，融合得到的参数定义为M。M为EAR、PERCLOS和MAR按照各自不同的权值比重进行相加所得，其中EAR占20%、PERCLOS占70%和MAR占10%。将M>0.605的帧数N来判断驾驶员的疲劳程度，N<=25时标记为清醒状态；25<N<50时标记为疲劳；N>=50时为重度疲劳。4.6本章小结本章对dlib68个地标做简要的说明，这是面部特征点标定的主要依据。随后针对标定的眼睛和嘴巴区域获得眼睛纵横比和嘴巴高宽比，从而计算得到EAR值和MAR值。根据EAR和MAR值的大小判断驾驶员是否眨眼和打哈欠，从而检测驾驶员眼睛和嘴巴的状态。在EAR值的基础上计算PERCLOS值，通过P80准则来判断眼睑覆盖眼球的百分比，此值作为判断疲劳驾驶的重要参数。最后详细阐述疲劳检测系统的在线监控工作流程图、KNN分类器以及面部多特征融合。第5章实验数据和结果39第5章实验数据和结果在本章中，首先评估所提出的TCDCN在人脸检测FDDB数据集中的有效性，然后讨论EAR、MAR和PERCLOSE之间的相关性。通过疲劳主观问卷调查对采集的视频进行清醒、疲劳和重度疲劳状态进行标记，疲劳检测系统对标记的视频进行验证，确定检测系统的准确率。5.1环境和数据集该实验平台包括了一个具有X86架构的英特尔酷i7-10750（主频率：2.60GHz）、NVIDIAGeForceGTX1650、16GBDDR4、OpenCV3.4.11图像库、Dlib19.17.0工具包和深度学习框架Tensorflow2.5。在本文中，主要采用了以下两种类型的数据集进行训练。第一种为脸部检测及标准数据库（FaceDetectionDataSetandBenchmark,FDDB），它是一种专门用来进行无约束人脸检测的数据集，该数据集包括5171个面孔如图5-1所示。图5-1FDDB数据集Fig.5-1FDDBdataset河北工程大学硕士学位论文40第二种是志愿者在不同场景下采集的视频数据集，收集驾驶车辆的视频数据集，如图5-2所示。本文收集的视频数据集来自8名司机（20至45岁，4名女性司机和4名男性司机）和30次驾驶测试。所有的参与者都要求有驾驶证，每年开车里程超过2000公里，并且没有相关的疾病、吸毒、和饮酒记录。测试要求在测试前避免饮酒或喝含咖啡因的饮料，参与者在测试前需要熟悉车辆驾驶状态和驾驶路线，驾驶路线均在封闭安全路段。每次驾驶测试的持续时间分别为30分钟、60分钟和120分钟，驾驶路线是一个模拟双车道的高速公路，有一些平稳的转弯，参与者在测试期间要求连续行驶约40公里、80公里和160公里。图5-2不同场景下的数据集收集Fig.5-2Datacollectionindifferentscenarios5.2驾驶员疲劳驾驶检测系统第5章实验数据和结果41为了能够实现在实际环境中验证本文第四章在线监控系统的真实性，本文所采用的疲劳驾驶流程如图5-3所示。疲劳检测终端和后台分析系统两者构成一个完整的疲劳检测系统。疲劳检测终端主要分为摄像头（采集驾驶员的面部状态，传输给图像处理单元）、图像处理单元（对摄像头传输过来的图像进行分析处理，并将驾驶员的状态传输给后台分析系统）、语音警告设备（对驾驶员处于疲劳状态驾驶时，进行语音警告提醒：您已疲劳驾驶，请注意休息！）三个部分。其次，后台分析系统主要放在驾驶室内，主要对终端传输过来的驾驶员状态信息进行分析，并让语音警告设备对驾驶员做出警告提醒。图5-3疲劳驾驶检测系统结构Fig.5-3Structureoffatiguedrivingdetectionsystem5.2.1软件系统疲劳驾驶检测系统的软件部分也包括两部分：一是疲劳驾驶检测终端的软件部分；二是后台分析系统的硬件部分。下面将详细地对这两个部分基本构成加以说明。1、疲劳驾驶检测终端本文基于python脚本语言完成疲劳驾驶检测终端的软件部分，一方面，部分和终端以及后台分析系统的交互部分由python语言来实施算法进而得以实现，另一方面，整个python程序由脚本进行激活。对于整个算法的实现，疲劳驾驶检测系统采用tensorflow作为深度学习的框河北工程大学硕士学位论文42架，来编写程序从而实现卷积神经网络，采用Dlib68个地标对人脸特征点进行标定，采用opencv和opencv-contrib-python对图像进行处理。疲劳检测系统的摄像头负责采集图像，TCDCN网络负责人脸检测、眼睛和嘴巴定位，采用KNN分类器对眼睛状态和嘴巴状态进行分类。对图像的帧数进行统计，图像的帧数达到一定的帧数，则计算这段时间内的EAR、PERCLOS、MAR和M的值，其各自的阈值分别为0.2、0.8、0.6和0.605，各自的值与其各自的阈值进行比较，计算出M值。在一定帧数内连续累加M大于0.605的帧数得到N值，通过N的大小来判断驾驶员的状态。在脚本语言部分，为了实现能够自动启动运行算法部分，使疲劳检测系统能够完全实现自动实时监督驾驶员的驾驶车辆状态。本文采用bat批量处理脚本程序。2、后台分析系统本文的检测系统在python程序语言的基础上，设计一种专门用于疲劳驾驶检测系统的管理程序，使用图片服务器完成终端与后台之间的图像互传，并通过云服务完成终端与后台之间的文字、语音通讯。当发现驾驶员正处在疲劳驾车状况时，后台语音系统会向疲劳检测终端发出语音提醒驾驶员已经处于疲劳驾驶，请注意安全驾驶。为了验证上述驾驶员疲劳驾驶检测算法的准确性，本文分别进行了短期测试和长期测试进行验证。短期测试验证的目的主要是识别驾驶员昏昏欲睡驾驶的行为。长期测试主要验证是所提算法能否准确判断驾驶员在长期驾驶时是否打瞌睡，进而根据程度（清醒、疲劳和严重疲劳）进行分类。5.2.2硬件系统疲劳驾驶检测系统的硬件部分包括两个部分：一是疲劳驾驶检测终端的设备部分；二是后台分析系统的硬件部分。以下将详细地对这两个部分的构成进行说明。1、疲劳驾驶检测终端疲劳驾驶检测终端分为两种：驾驶模拟器采集终端和实车道路测试终端，实验中所用的实验设备如图5-4所示。驾驶模拟器采集终端主要由三块32英寸的显示屏组成，可以形成130度的驾驶视野，经过多次不同调整，将摄像头安装在显示屏上方。通过驾驶模拟器模拟不同的驾驶场景，采集驾驶员清醒、疲劳和重度疲劳三种状态下的样本。实车道路测试终端主要由摄像头、笔记本电脑、汽车三部分组成，摄像头安装在驾驶员面部正前方的中控台上，实时监控驾驶员的面部状态，将采集到的视频传输到笔记本终端。图5-3的语音设备由电脑笔记本上的第5章实验数据和结果43麦克风和扬声器所代替。图5-4实验设备Fig.5-4Experimentalequipment2、后台分析系统后台分析系统由一台联想Y7000笔记本电脑所代替，电脑安装Windows10的操作系统并插入无线网卡，笔记本电脑放在驾驶室内的副驾驶座椅上。笔记本电脑负责分析驾驶员的面部特征信息，实时的将驾驶员的驾驶状态显示在显示屏上。5.3短期实验为了验证所提出具有个体差异的疲劳驾驶检测算法的有效性，本文进行了一系列实验。首先，使用疲劳阈值0.605、EAR阈值0.2、PERCLOS阈值0.8和河北工程大学硕士学位论文44MAR阈值0.6分析疲劳检测的准确性。然后，通过未打哈欠和连续打哈欠的驾驶员驾驶车辆来确定疲劳识别算法中的核心阈值是否有效。短期实验将根据眼睛和嘴巴的不同状态进行实验，图5-5为未打哈欠状态下驾驶员的疲劳检测结果，图5-6为打哈欠状态下的驾驶员疲劳检测结果。图5-5和图5-6中的纵坐标分别为EAR、MAR、PERCLOS和M的值，横坐标为视频帧数（frames），灰色点划线为EAR、MAR、PERCLOS和M各自的阈值。图5-5中的EAR图为视频帧数在60帧附近左眼和右眼一次眨眼的过程，EAR值取leftEAR和rightEAR值的平均值。从图中可以看出眼睛在正常的情况下EAR值均大于0.2，发生闭眼的时EAR值迅速减小且小于0.2，因此设置EAR的阈值为0.2，其权重比例为0.2。通过对图5-5和图5-6中的MAR曲线图的对比，人在打哈欠时MAR的值会迅速增加，未打哈欠的状态下MAR的值保持在0.33附近，通过多次实验验证设定MAR的阈值为0.6，考虑打哈欠的频率较低将其权重比例设置为0.1。根据P80准则设定PERCLOS的阈值为0.8，此值为判断是否疲劳驾驶的重要参数，其权重比例为0.7。图5-5和图5-6中M的曲线图分别是在闭眼过程中未打哈欠状态下和打哈欠状态下的曲线图，图中红色曲线部分为超过M的阈值0.605的部分。图5-5未打哈欠驾驶员的疲劳检测结果Fig.5-5Fatiguedetectionresultsfordriverswhodonotyawn第5章实验数据和结果45图5-6打哈欠驾驶员的疲劳检测结果Fig.5-6Fatiguedetectionresultsfordriverswhodonotyawn对于图5-5和图5-6的分析，当驾驶员未打哈欠时M大于0.605的帧数相对较少；驾驶员在打哈欠的情况下往往伴随着长时间闭眼，使M大于0.605的帧数相对增加。驾驶员开始打哈欠时身体会出现疲劳犯困的情况，从图5-6可以看出驾驶员长时间打哈欠时M大于0.605值的帧数在25到50之间，可以给予驾驶员警告提醒。图5-5和图5-6显示了在睡意时间内对眼睛、嘴巴和疲劳阈值M的识别结果。5.4长期实验在短期测试中，发现EAR、PERCLOS、MAR和疲劳警告阈值M与驾驶员疲劳状态之间的关系。为了直观的评估在不同驾驶条件下眼睛和嘴巴状态的变化，依据提前设计好的路线对驾驶员进行实车实验。长期实验检测的结果如图5-7和图5-8所示，表示男生驾驶员的驾驶状态。图5-7在整个驾驶期间EAR、MAR、PERCLOS和M的值都远远偏离它们的阈值，M值连续红线部分的帧数增加，超过一定范围表明驾驶员处于疲劳状态下；而图5-8出现一些红色部分的区域，是由于驾驶员偶尔眨眼或者打哈欠的情况下导致EAR、PERCLOS、MAR和M的值偏离了它们的阈值，表明驾驶员在长期驾驶期间是清醒的。河北工程大学硕士学位论文46图5-7男生疲劳驾驶状态Fig.5-7Fatiguedrivingstateofmen图5-8男生未疲劳驾驶状态Fig.5-8Drivingwithoutfatigueofmen第5章实验数据和结果47图5-7和图5-8中，M是判断驾驶员面部多特征融合的判断指数，用于判断驾驶员的疲劳程度，每个参数根据不同的权重进行融合和计算，既保证了检测的准确性，也避免了缺失检测或检测错误，此外还可以根据N的大小来判断驾驶员的疲劳状态。在图5-8的M曲线中疲劳检测系统识别出3段疲劳状态曲线和2段重度疲劳曲线，其余部分为清醒状态下的曲线，分析结果如表5-1所示。图5-7中M曲线的部分没有出现N大于25的部分，则表明没有出现疲劳状态。根据图5-7和图5-8中M曲线得出的结果显示，检测系统没有出现错误检测或缺失检测的情况，满足实际的检测要求。通过对图5-7和图5-8的视频帧数提取分析，设置驾驶员清醒、疲劳和重度疲劳三种状态，N为视频的帧数。N小于等于25帧时为清醒状态，N大于25小于等于50为疲劳状态，N大于50为重度疲劳状态。如表5-1所示中可以看出在视频帧数在N大于25小于等于50的有35~76、123~167和199~224，视频帧数分别为42、45和26为疲劳状态；N大于50的帧数有307~394和545~602，视频帧数为88和58为重度疲劳。依据短期实验和长期测试对本文提出的疲劳驾驶检测系统进行验证，为了进一步确认本文提出的检测系统在性别上是否存在差异性，本文对女生进行同样的测试，验证疲劳检测系统的稳定性及精确性。表5-1男生驾驶员状态分析Table5-1Driverstatusanalysisofmen状态条件区段帧数清醒N<=25疲劳25<N<=5035~7642疲劳25<N<=50123~16745疲劳25<N<=50199~22426重度疲劳N>50307~39488重度疲劳N>50545~602585.5不同性别对疲劳检测结果的影响在长期测试的基础上对相同年龄段男生和女生进行同样的测试，以验证本文提出基于面部特征融合的驾驶员疲劳检测系统的准确性以及实时性。同样在封闭且安全的道路情况下对年龄相同的男生和女生进行测试，为了确保测试的准确性，河北工程大学硕士学位论文48保持测试环境一致性，比如同一辆汽车和同样的驾驶路线。图5-7和图5-8是一位年龄在26岁左右男生测试的数据；同样找一位年龄在26岁左右的女生进行测试，驾驶路线、环境、时长与男生保持一致。通过疲劳检测系统获得女生的驾驶数据如图5-9和图5-10所示，两张曲线图中的四条线从上往下依次表示为EAR、PERCLOS、MAR和M的数据曲线图，横坐标为视频的帧数，纵坐标为EAR、PERCLOS、MAR和M的值。图5-9女生疲劳驾驶状态Fig.5-9Fatiguedrivingstateofgirls第5章实验数据和结果49图5-10女生未疲劳驾驶状态Fig.5-10Drivingwithoutfatigueofgirls图5-9中的M曲线中包含2段疲劳状态曲线和2段重度疲劳曲线，其余部分为清醒状态下的曲线。在图5-10中M曲线的部分也没有出现N大于25的部分，证明未曾出现疲劳状态。图5-9中的M曲线分析结果如表5-2所示，在表5-2中可以看出女生驾驶车辆视频帧数在N大于25小于等于50的有43~68和151~179，视频帧数分别为26和29为疲劳状态；N大于50的帧数有189~241和262~343，视频帧数为53和82为重度疲劳。表5-2女生驾驶员状态分析Table5-2Driverstatusanalysisofgirls状态条件区段帧数清醒N<=25疲劳25<N<=5043~6826疲劳25<N<=50151~17929重度疲劳N>50189~24153重度疲劳N>50262~34382通过图5-7、图5-9和图5-8、图5-10男生和女生分别在疲劳状态和清醒状态下的各个曲线对比结果显示，在同样的驾驶条件下，本系统可以精确识别不同性别的驾驶员疲劳状态。河北工程大学硕士学位论文505.6驾驶员疲劳验证为了验证本文提出的基于面部多特征融合的疲劳检测系统的准确性，对8名参加测试的测试者分别进行同样的测试，测试条件：48小时内不能喝酒、吃药、喝咖啡和吃刺激性食物等；测试工况：（1）驾驶模拟器，进行高速公路、绕城高速和城市道路测试；（2）封闭道路测试，进行长直线行驶、连续弯道行驶和绕圈行驶。实时记录8名测试者的驾驶状态数据和面部特征，做成相应的样本数据库。在8名测试者的样本数据库随机抽取480个样本数据，每名测试者抽取60个样本数据。通过主观问卷调查的方式，将样本数据进行标记，分别是清醒状态、疲劳状态和重度疲劳状态。将标记的样本数据输入到疲劳检测系统进行验证，确定本文疲劳检测系统的准确率。5.6.1疲劳主观问卷调查验证在这项研究中，当司机完成测试驾驶时，他们被要求根据回顾视频完成疲劳主观问卷调查，自我报告评估问卷调查的具体内容如表5-3所示。8名测试者根据自己的驾驶视频回顾自己的驾驶状态，在当时的驾驶状态回答问卷调查中的问题，回答问题时是否存在困难：（1）根本不存在困难；（2）有轻微困难；（3）有困难。通过主观问卷调查的得分将驾驶员的状态分为清醒状态、疲劳状态和重度疲劳状态。驾驶员主观问卷调查的得分值划分为三个区域：5-8分为清醒状态，9-12分为疲劳状态，13-15分为重度疲劳状态；8名测试者问卷调查得分的结果如表5-4所示。按照主观问卷调查的得分对每名测试者的样本数据进行标记，标记内容是：清醒、疲劳和重度疲劳；将标记好的样本数据传输到疲劳检测系统进行逐一验证。表5-3对疲劳程度的自我报告进行评估Table5-3Assessself-reportoffatiguelevel项目描述分值1你感到昏昏欲睡吗？1~32你对清晰的思考有困难吗？1~33你马上回答一个问题有困难吗？1~34你对控制自己的肌肉有困难吗？1~35你对注意力集中有困难吗？1~3注：1表示根本不存在困难；2表示有轻微困难；3表示有困难。第5章实验数据和结果51表5-4问卷调查得分表Table5-4QuestionnaireScoreTable测试驾驶员12345678清醒55556555疲劳111211121291210重度疲劳13151315141315155.6.2疲劳检测系统验证对疲劳检测系统进行验证，将识别系统的准确率作为评价标准[62]，识别准确率的计算公式如5-1所示：n,1,2,3nnqPnQ（5-1）公式中，n表示驾驶员的疲劳程度：1代表清醒，2代表疲劳，3代表重度疲劳；np表示模型对第n种疲劳程度识别的正确率；nq表示第n种的疲劳驾驶测试样本数据种被准确识别出的样本个数；nQ表示第n种的疲劳驾驶测试样本总数。疲劳检测系统将对标记的480个样本数据进行验证，识别每名测试者清醒、疲劳、重度疲劳状态的准确率，准确率如表5-5所示。表5-5疲劳驾驶检测系统对不同驾驶员的识别结果Table5-5IdentificationResultsofFatigueDrivingDetectionSystemforDifferentDrivers测试驾驶员12345678平均值清醒P197.296.695.798.397.193.692.898.296.2疲劳P292.893.491.794.892.690.891.295.892.9重度疲劳P392.294.992.695.293.790.191.896.793.4注：单位为百分比。由表5-5中的数据可以得出，本文基于驾驶员面部多特征融合的疲劳检测系统对不同驾驶员的检测的准确率有所不同，8名测试者清醒状态的平均准确率为96.2%，疲劳状态的平均准确率为92.9%，重度疲劳状态的平均准确率为93.4%。疲劳和重度疲劳的准确率的平均值为疲劳检测系统的准确率。因此基于驾驶员面部多特征融合的疲劳检测系统的准确率为93.1%，满足实际情况下的疲劳检测系统的要求。5.7本章小结本章主要对实验的数据和结果进行详细阐述，验证本文所提疲劳驾驶疲劳检测系统的合理性。本章首先对实验环境和数据集的收集以及对疲劳检测设备的软件系统和硬件系统进行了阐述。然后进行了两次实验，一次是短期实验，另一次河北工程大学硕士学位论文52是长期实验。通过主观问卷调查的方式将测试者的驾驶视频标记为清醒、疲劳和重度疲劳，疲劳检测系统对标记的视频进行逐一识别验证。实验结果表明本文采用基于面部多特征融合的疲劳检测系统的准确率达到了93.1%，满足工业应用驾驶员疲劳实时检测系统的要求。结论53结论驾驶员疲劳驾驶检测技术的研究减少因疲劳引起交通事故的首要任务，对交通安全以及公共安全具有重要意义。因此，研究驾驶员疲劳驾驶具有重要意义。针对驾驶员长时间驾驶引起的疲劳驾驶问题，本文在现有研究成果的基础上，提出了一种基于驾驶员面部多特征融合的疲劳检测系统。该方法不仅结合TCDCN网络对驾驶员面部进行准确识别，而且通过驾驶员面部多个特征的融合对疲劳状态进行检测，实现了对驾驶员疲劳驾驶的预警。本文工作总结如下：（1）针对国内外对驾驶员疲劳检测技术的相关研究，总结了现今疲劳检测系统的优缺点以及存在的关键技术问题。通过对国内外的研究现状的分析，本文提出了基于驾驶员面部多特征融合的疲劳检测系统。（2）深入研究了疲劳驾驶系统中驾驶员面部检测的关键技术，对面部检测网络MTCNN、DCNN和TCDCN进行了深入分析。（3）为了能够准确且快速的实现对驾驶员疲劳状态的识别和检测的实时性，本文提出了离线训练模块和在线测试模块。离线训练模块通过利用多任务学习的优点，对驾驶员的面部疲劳特征进行训练；在线测试模块是为了能够从摄像头中实时的检测驾驶员的驾驶状态。（4）对眼睛、嘴巴的特征进行了单独检测分析。通过对眼睛睁眼、闭眼以及眨眼的过程进行检测，得到的EAR数据使用Python中的Matplotlib画出眨眼过程的曲线图，确定EAR的阈值为0.2，EAR小于0.2即为闭眼状态；在EAR数据的基础上通过公式计算得到PERCLOS的值，画出PERCLOS的曲线图，确定了PERCLOS的阈值为0.8，PERCLOS大于0.8连续帧数为闭眼时长。对嘴巴张开、闭合、打哈欠和说话的状态逐一进行了检测分析，通过对比嘴巴在不同状态下的MAR曲线图，确定MAR的阈值为0.6，MAR大于0.6为打哈欠状态。为了使疲劳检测的精度更加准确，本文将眼睛、嘴巴的特征进行融合，既是将EAR、PERCLOS和MAR参数融合成一个新参数M，利用M>0.605的帧数N来判断驾驶员的疲劳程度，驾驶员驾驶状态分为为清醒状态、疲劳状态、重度疲劳状态三种情况。（5）为了保证实验数据的可靠性以及检测系统的准确性，搭建了疲劳检测平台。在该平台上分别进行了两种实验验证：短期实验和长期实验。短期测试验证的目的主要是识别驾驶员昏昏欲睡驾驶的典型迹象；长期测试主要验证是所提算法能否准确判断驾驶员在长期驾驶时是否打瞌睡，并且根据疲劳程度（清醒、河北工程大学硕士学位论文54疲劳和严重疲劳）进行分类。通过实验表明疲劳检测系统的准确率为93.1%，达到了目前工业应用检测的准确率，可用于驾驶员疲劳驾驶检测。本文的创新点如下：（1）本文利用了驾驶员面部特征（眼睛、嘴巴等）来融合视觉信息，有效的提高了驾驶员疲劳检测的准确性。（2）通过TCDCN网络对人脸的关键点进行定位，提取出眼睛和嘴巴的区域，从而计算的EAR、PERCLOS和MAR是一个更简洁的解决方案。（3）本文提出新的参数M和N。M值为EAR、PERCLOS和MAR的加权值，即眼睛和嘴巴综合状态；N为判断驾驶员疲劳状态的参数值，依据N值的累加次数判断驾驶员是否疲劳。本文结合了深度学习、机器视觉、多特征融合等技术，提出了基于驾驶员面部多特征融合的疲劳检测方法，同时将其与现有的方案进行了对比，实验结果显示本文检测系统表现出良好的技术优势。但本文提出的方案还是有许多欠缺的地方，需要进一步改进，在未来的研究工作中，将关注以下几个方面：（1）上述研究将安装在车辆上，进一步验证夜间驾驶条件的疲劳检测的识别效果。（2）添加其他视觉特征来融合信息，如驾驶员头部姿势。参考文献55参考文献[1]AmodioA,ErmidoroM,MaggiD,etal.AutomaticdetectionofdriverImpairentbasedonpupillarylightreflex[J].IEEETransactionsonIntelligentTransportationSystems,2019,20(8):3038-3048.[2]LiYY,ZhangGN.Theeffectoffatiguedrivingoninjuryseverityconsideringtheendogeneity[J].JournalofSafetyResearch,2018,64:11-19.[3]ZhangGN,YanKKW,ZhangX,etal.Trafficaccidentsinvolvingfatiguedrivingandtheirextentofcasualties[J].AccidentAnalysisandPrevention,2016,87(2):34-42.[4]GanderpH,MarshallLNS,JamesI,etal.Investigatingdriverfatiguebasedonvehicleoperatingdata[J].JournalofTransportationSystemsEngineeringandInformationTechnology,2020,20(4):77-82.[5]HigginsJS,MichaelJ,AustinR,etal.Asleepatthewheel—Theroadtoaddressingdrowsydriving[J].JournalofSleep,2017,40(2):1-9.[6]PeiZ.StudySituationandDevelopingTrendonDetectingandEvaluatingTechniquesofMotorDriverFatigue[J].JournalofChinaAgriculturalUniversity,2001,6(6):01–105.[7]LiangY,ReyesML,LeeJD.Real-TimeDetectionofDriverCognitiveDistractionUsingSupportVectorMachines[J].IEEETransactionsonIntelligentTransportationSystems,2007,8(2):340-350.[8]LiuCC,HoskingSG,LenneMG.Predictingdriverdrowsinessusingvehiclemeasures:Recentinsightsandfuturechallenges[J].JournalofSafetyResearch,2009,40(4):239-245.[9]ForsmanPM,VilaBJ,ShortRA,etal.Efficientdriverdrowsinessdetectionatmoderatelevelsofdrowsiness[J].AccidentAnalysisandPrevention,2013,50:341–350.[10]WangJ,ZhuS,GongY.DrivingSafetyMonitoringUsingSemi-supervisedLearningonTimeSeriesData[J].IEEETransactionsonIntelligentTransportationSystems,2010,11(3):728-737.[11]Wu,BF,Chen,etal.Reasoning-BasedFrameworkforDrivingSafetyMonitoringUsingDrivingEventRecognition[J].IEEETransactionsonIntelligentTransportationSystems,2013,14(3):1231-1241.[12]郭建生,陈士伟,白菁等.军事飞行人员疲劳状况调查与分析(英文)[J].解放军医学杂志,2012,37(03):249-252.[13]李晅,詹皓,郭华等.空军飞行人员飞行疲劳状况调查与分析[J].人民军医,2014,57(11):1167-1169.[14]万斌,贾爱芳.矿工心理疲劳状况调查分析[J].能源技术与管理,2022,47(05):52-55.[15]田春雨,邵维阳,崔蓓等.海军舰员远航条件下视疲劳状况调查分析[J].国际眼科杂志,2021,21(09):1657-1660.河北工程大学硕士学位论文56[16]LeeS,KimJK.Factorscontributingtotheriskofairlinepilotfatigue[J].Journalofairtransportmanagement,2018,67:197-207.[17]FiltnessAJ,NaweedA.Causes,consequencesandcountermeasurestodriverfatigueintherailindustry:Thetraindriverperspective[J].Appliedergonomics,2017,60:12-21.[18]MvmyA,XlaB,KsB,etal.CanSVMbeusedforautomaticEEGdetectionofdrowsinessduringcardriving[J].SafetyScience,2009,47(1):115-124.[19]LinCT,ChangCJ,LinBS,etal.AReal-TimeWirelessBrain–ComputerInterfaceSystemforDrowsinessDetection[J].IEEETransactionsonBiomedicalCircuits&Systems,2010,4(4):214-222.[20]LinFC,KoLW,ChuangCH,etal.GeneralizedEEG-BasedDrowsinessPredictionSystembyUsingaSelf-OrganizingNeuralFuzzySystem[J].IEEETransactionsonCircuits&SystemsIRegularPapers,2012,59(9):2044-2055.[21]DongN,LiY,GaoZ,etal.AWPCA-BasedMethodforDetectingFatigueDrivingFromEEG-BasedInternetofVehiclesSystem[J].IEEETransactionsonIntelligentTransportationSystems,2019,7:124702–124711.[22]JungSJ,ShinHS,ChungWY.Driverfatigueanddrowsinessmonitoringsystemwithembeddedelectrocardiogramsensoronsteeringwheel[J].IetIntelligentTransportSystems,2014,8(1):43-50.[23]LeiJ,LiuF,HanQ,etal.StudyonDrivingFatigueEvaluationSystemBasedonShortTimePeriodECGSignal[C]//2018IEEEInternationalConferenceonIntelligentTransportationSystems(ITSC).IEEE,2018:2466-2470.[24]ZhangYF,GaoXY,ZhuJY,etal.AnovelapproachtodrivingfatiguedetectionusingforeheadEOG[J].InternationalIEEE/EMBSConferenceonNeuralEngineering,2015,7:707-710.[25]BullingA,WardJA,GellersenH,etal.EyeMovementAnalysisforActivityRecognitionUsingElectrooculography[J].IEEETransactionsonPatternAnalysis&MachineIntelligence,2011,33(4):741-753.[26]HuS,ZhengG.DriverdrowsinessdetectionwitheyelidrelatedparametersbySupportVectorMachine[J].ExpertSystemswithApplications,2009,36(4):7651-7658.[27]Koichi,Fujiwara,Erika,etal.HeartRateVariability-basedDriverDrowsinessDetectionanditsValidationwithEEG[J].IEEEtransactionsonbio-medicalengineering,2018,66(6):1769-1778.[28]SangtaeA,ThienN,HyojungJ,etal.ExploringNeuro-PhysiologicalCorrelatesofDrivers'MentalFatigueCausedbySleepDeprivationUsingSimultaneousEEG,ECG,andfNIRSData[J].FrontiersinHumanNeuroscience,2016,10(848):1-14.[29]WangH,WuC,LiT,etal.DrivingFatigueClassificationBasedonFusionEntropyAnalysisCombiningEOGandEEG[J].IEEEAccess,2019,7:61975–61986.[30]王磊.面向脑疲劳检测的EEG信号通道选择与多特征融合研究[D].武汉:武汉理工大学,2参考文献57020.[31]杨硕,李润泽,丁建清,徐桂芝.基于EMD去趋势波动的脑疲劳模糊熵分析[J].中国生物医学工程学报,2020,39(01):33-39.[32]张春翠.体疲劳对脑疲劳影响的脑电信息分析与处理[D].天津:天津大学,2014.[33]李增勇,焦昆,陈铭,王成焘.汽车驾驶员驾驶过程中的心率变异性功率谱分析[J].中国生物医学工程学报,2003(06):574-576.[34]王琳,张陈,尹晓伟,付荣荣,王宏.一种基于驾驶员生理信号的非接触式驾驶疲劳检测技术[J].汽车工程,2018,40(03):333-341.[35]王福旺,王宏,罗旭.基于EEG与EOG信号的疲劳驾驶状态综合分析[J].东北大学学报(自然科学版),2014,35(02):175-178.[36]RieraL,OzcanK,MerickelJ.Detectingandtrackingunsafelanedepartureeventsforpredictingdriversafetyinchallengingnaturalisticdrivingdata[C]//2020IEEEIntelligentVehiclesSymposium(IV).IEEE,2020:238-245.[37]TakeiY,FurukawaY.Estimateofdriver'sfatiguethroughsteeringmotion[C]//Systems,ManandCybernetics,2005IEEEInternationalConferenceon.IEEE,2005(2):1765-1770.[38]SandbergD,WahdeM.Particleswarmoptimizationoffeedforwardneuralnetworksforthedetectionofdrowsydriving[C]//IEEEInternationalJointConferenceonNeuralNetworks.IEEE,2008:788-793.[39]蔡素贤,杜超坎,周思毅,王雅斐.基于车辆运行数据的疲劳驾驶状态检测[J].交通运输系统工程与信息,2020,20(04):77-82.[40]张驰.高危车辆驾驶员疲劳驾驶监测及预警系统研究[D].锦州:辽宁工业大学,2018.[41]屈肖蕾.基于转向操作和车辆状态的疲劳驾驶检测方法研究[D].北京:清华大学,2012.[42]黄皓.基于驾驶操作及车辆状态的疲劳驾驶行为检测研究[D].南京:东南大学,2016.[43]XuepengZ,ChunningM,MingkuiF,etal.EyeFeaturePointDetectionBasedonSingleConvolutionalNeuralNetwork[J].IetComputerVision,2017,12(4):453-457.[44]FangZ,SuJ,LeiG,etal.DriverFatigueDetectionBasedonEyeStateRecognition[C]//2017InternationalConferenceonMachineVisionandInformationTechnology(CMVIT).IEEE,2017:105–110.[45]Liu,Z,Peng,Y,HuW.DriverFatigueDetectionbasedonDeeply-LearnedFacialExpressionRepresentation[J].JournalofVisualCommunicationandImageRepresentation,2020,71(6):12.[46]廖明明,赵波.基于面部特征融合的驾驶员疲劳检测[J].智能计算机与应用,2021,11(10):77-81.[47]苏锦瑾,霍春宝,王特特.基于面部特征的驾驶员疲劳检测系统[J].信息技术与信息化,2021(09):114-116.[48]刘炜煌,钱锦浩,姚增伟,焦新涛,潘家辉.基于多面部特征融合的驾驶员疲劳检测算法[J].计算机系统应用,2018,27(10):177-182.河北工程大学硕士学位论文58[49]周云鹏,朱青,王耀南,卢笑,凌志刚.面部多特征融合的驾驶员疲劳检测方法[J].电子测量与仪器学报,2014,28(10):1140-1148.[50]FreundY.ADecision-TheoreticGeneralizationofOn-LineLearningandanApplicationtoBoosting[C].J.Comput.Syst.Sci.1997,55,119–139.[51]YiS,WangX,TangX.DeepConvolutionalNetworkCascadeforFacialPointDetection[C]//ComputerVisionandPatternRecognition(CVPR),2013IEEEConferenceon.IEEE,2013:3476-3483.[52]ZhangZ,LuoP,Loy.FacialLandmarkDetectionbyDeepMulti-taskLearning[J].LectureNotesinComputerScience,2014,8694:94-108.[53]LinM,ChenQ,YanS.NetworkinNetwork[J].arXive-prints,2013.[54]FreundY,SchapireRE.ADecision-TheoreticGeneralizationofOn-LineLearningandanApplicationtoBoosting[C].ConferenceonLearningTheory.AcademicPress,1997,55:119–139.[55]ChengR,ZhaoY,DaiY,etal.AnOn-boardEmbeddedDriverFatigueWarningSystemBasedonAdaboostMethod[J].BeijingDaxueXuebaoZiranKexueBanActaScientiarumNaturaliumUniversitatisPekinensis,2012,48(5):719-726.[56]MinJ,XiongC,ZhangY,etal.DriverfatiguedetectionbasedonprefrontalEEGusingmulti-entropymeasuresandhybridmodel[J].BiomedicalSignalProcessingandControl,2021,69(3):102857–102865.[57]Shi-RuQU,PengJC.Designofmulti-featurefusiondriverfatiguestatedetectionsystembasedonFPGA[J].TransducerandMicrosystemTechnologies,2013,32(5):86-105.[58]KingDE.Dlib-ml:AMachineLearningToolkit[J].JournalofMachineLearningResearch,2009,10(3):1755-1758.[59]BRange,KWest,NAir,etal.U.S.DepartmentofTransportation,FederalHighwayAdministration[J].SeismicRetrofittngManualforHighwayBridges,1995,15(5):766-770.[60]HengQ,WangW,JiangX,etal.AssessmentofDriverMentalFatigueUsingFacialLandmarks[J].IEEEAccess,2019,7:150423–150434.[61]NonisF,DagnesN,MarcolinF,etal.3DApproachesandChallengesinFacialExpressionRecognitionAlgorithms—ALiteratureReview[J].JournalofAppliedSciences,2019,9:3904.[62]郭孜政,谭永刚,马国忠,等.基于BP神经网络的驾驶精神疲劳识别方法[J].哈尔滨工业大学学报,2014,46(08):118-121.