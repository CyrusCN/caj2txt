 分类号             密级       U D C              编号            硕士学位论文      基于面部多特征融合的疲劳驾驶检测系统设计与实现     学位申请人姓名: 韩一民 申请学位学生类别: 全日制硕士 申请学位学科专业: 电子信息 指导教师姓名: 瞿少成 教授    硕士学位论文   基于面部多特征融合的疲劳驾驶检测系统设计与实现   论文作者：韩一民 指导教师：瞿少成 教授 学科专业：电子信息 研究方向：物联网与人工智能  华中师范大学物理科学与技术学院 2023年5月     Design and Implementation of Fatigue Driving Detection System Based on Facial Multi-Feature Fusion  A Thesis Submitted in Partial Fulfillment of the Requirement For the Master Degree in Electronic Information  By Han Yimin Postgraduate Program College of Physical Science and Technology Central China Normal University  Supervisor:Shaocheng Qu Academic Title: Professor                 Signature ___________ Approved May,2023   I 摘  要 随着我国汽车保有量和驾驶员数量的不断增长，因交通事故引起的人员伤亡及财产损失也在逐年上升，而疲劳驾驶已经成为导致交通事故的主要原因之一。因此，研究一种安全可靠的疲劳驾驶检测系统具有重要的理论和实际意义。目前常用的疲劳驾驶检测方法中，基于驾驶员生理特征的检测方法和基于车辆行驶特征的检测方法存在着设备昂贵、易受环境影响、检测指标单一等问题。鉴于以上问题，本文提出了一种基于面部多特征融合的疲劳驾驶检测方法，并结合Orange Pi 5嵌入式开发板构建了一个疲劳驾驶检测及监管系统，该系统能够准确分析驾驶员的疲劳状态，提高驾驶安全性。具体研究内容如下： （1）设计了一种基于SimAM无参注意力机制的轻量化YOLOv5s人脸检测网络。为了让模型能够将更多的注意力聚焦于有效信息区域，进而提高模型的检测精度，在YOLOv5s主干网络尾部添加SimAM无参注意力机制。改进后的网络在Wider Face数据集上的实验结果表明，相较于原始YOLOv5s，在参数量不变的情况下，检测精度提高了2.1%，能够较好的完成人脸检测任务。 （2）改进了一种带有辅助子网络的PFLD人脸关键点检测模型。首先，构建了多尺度特征融合模块，以进一步加强模型对大目标及小目标的检测能力。其次，使用Ghost Bottleneck优化了模型主干网络，进一步降低模型参数量，提高检测速度。在WFLW数据集上的实验结果表明，改进后的模型检测速度更快、精度更高，能够满足实际使用需求。 （3）研究了一种基于面部多特征融合的疲劳判定方法。首先，通过人脸关键点计算EAR、MAR值，进而提取驾驶员眼部和嘴部特征。同时，采用PFLD辅助子网络获取头部特征。然后，根据上述提取的3类面部特征对眨眼、打哈欠、瞌睡点头等状态进行疲劳阈值设定。最后，得出一种基于面部多特征融合的疲劳判定方法。在YawDD数据集上的实验结果表明，该方法的检测准确率达到了95.3%。 （4）设计并实现了疲劳驾驶检测及监管系统。搭建了基于Orange Pi 5嵌入式开发板的疲劳驾驶检测系统，并使用Spring Boot开发了后台监管系统。经过测试实验，本文所设计的疲劳驾驶检测及监管系统能够相对准确地判定疲劳驾驶行为。  关键词：疲劳驾驶检测；YOLOv5s；SimAM；PFLD；Orange Pi 5   II Abstract With the continuous increase in the number of vehicles and drivers in China, the number of casualties and property losses caused by traffic accidents has been rising year by year, and fatigue driving has become one of the main causes of traffic accidents. Therefore, researching a safe and reliable fatigue driving detection system has important theoretical and practical significance. Among the commonly used methods for detecting fatigue driving, those based on driver's physiological characteristics and those based on vehicle driving characteristics have problems such as expensive equipment, susceptibility to environmental influences, and single detection indicators. In view of these problems, this paper proposes a fatigue driving detection method based on multiple facial features fusion and constructs a fatigue driving detection and supervision system based on Orange Pi 5 embedded development board, which can accurately analyze the driver's fatigue state and improve driving safety. The specific research contents are as follows: (1) A lightweight YOLOv5s face detection network based on the SimAM non-parametric attention mechanism is designed. In order to enable the model to focus more attention on effective information areas and improve the detection accuracy of the model, the SimAM non-parametric attention mechanism is added to the tail of the YOLOv5s backbone network. The experimental results on the Wider Face dataset show that compared with the original YOLOv5s, the improved network increases the detection accuracy by 2.1% under the same parameter volume and can better complete the face detection task. (2) A PFLD face key point detection model with an auxiliary sub-network is improved. Firstly, a multi-scale feature fusion module is constructed to further enhance the model's detection ability for large and small targets. Secondly, the Ghost Bottleneck is used to optimize the model's backbone network, further reducing the model's parameter volume and improving the detection speed. The experimental results on the WFLW dataset show that the improved model has faster detection speed and higher accuracy, which can meet practical use requirements. (3) A fatigue judgment method based on multiple facial features fusion is studied. Firstly, the EAR and MAR values are calculated through facial key points to extract the driver's eye and mouth features. At the same time, the PFLD auxiliary sub-network is used to obtain head features. Then, based on the above three types of facial features, fatigue  III thresholds are set for blink, yawn, nodding and other states. Finally, a fatigue judgment method based on multiple facial features fusion is obtained. The experimental results on the YawDD dataset show that the detection accuracy of this method reaches 95.3%. (4) A fatigue driving detection and supervision system is designed and implemented. A fatigue driving detection system based on the Orange Pi 5 embedded development board is built, and a background supervision system is developed using Spring Boot. Through testing experiments, the fatigue driving detection and supervision system designed in this paper can relatively accurately judge fatigue driving behavior.  Key words：Fatigue driving detection; YOLOv5s; SimAM; PFLD; OrangePi5  目  录 第一章 绪论............................................................................................... 1 1.1 选题背景与研究意义 ....................................................................... 1 1.2 疲劳驾驶检测国内外研究现状 ...................................................... 2 1.2.1 基于驾驶员生理特征的检测方法 ............................................. 2 1.2.2 基于车辆行驶特征的检测方法 ................................................. 3 1.2.3 基于驾驶员面部特征的检测方法 ............................................. 4 1.3 主要研究内容及论文组织结构 ...................................................... 5 1.3.1 主要研究内容 ............................................................................. 5 1.3.2 论文组织结构 ............................................................................. 6 第二章 一种改进 YOLOv5s 的人脸检测模型 ...................................... 7 2.1 人脸检测方法分析 ........................................................................... 7 2.2 基于改进 YOLOv5s 的人脸检测模型设计 .................................. 8 2.2.1 YOLOv5s 模型结构分析 ........................................................... 8 2.2.2 融合 SimAM 注意力机制的 YOLOv5s 人脸检测模型 ..... 12 2.3 实验及结果分析 ............................................................................. 15 2.3.1 数据集准备 ............................................................................... 15 2.3.2 评估指标 ................................................................................... 16 2.3.3 模型训练与结果分析 ............................................................... 17 2.4 本章小结 ......................................................................................... 19 第三章 一种改进PFLD的人脸关键点检测模型 ................................ 20 3.1 人脸关键点检测方法分析 ............................................................ 20 3.2 基于改进PFLD的人脸关键点检测模型设计 ............................. 21 3.2.1 PFLD模型结构分析 ................................................................. 21 3.2.2 多尺度特征融合模块 ............................................................... 24 3.2.3 轻量化PFLD网络结构 ........................................................... 26 3.3 实验及结果分析 ............................................................................. 30 3.3.1 数据集选取及预处理 ............................................................... 30 3.3.2 评估指标 ................................................................................... 34 3.3.3 模型训练与结果分析 ............................................................... 34 3.4 本章小结 ......................................................................................... 36 第四章 基于面部多特征融合的疲劳状态判定方法 ............................ 38 4.1 眼部特征提取与疲劳判定 ............................................................ 38 4.1.1 PERCLOS 准则 ........................................................................ 38 4.1.2 眼部纵横比(EAR)计算 ............................................................ 39 4.1.3 眼部纵横比阈值设定实验 ....................................................... 40 4.1.4 基于眼部特征的疲劳判定 ....................................................... 42 4.2 嘴部特征提取与疲劳判定 ............................................................ 44 4.2.1 嘴部纵横比(MAR)计算 ........................................................... 44 4.2.2 嘴部纵横比阈值设定实验 ....................................................... 45 4.2.3 基于嘴部特征的疲劳判定 ....................................................... 47 4.3 头部姿态特征提取与疲劳判定 .................................................... 49 4.3.1 获取头部姿态角 ....................................................................... 49 4.3.2 基于头部姿态的疲劳判定 ....................................................... 50 4.4 疲劳状态综合判定及测试 ............................................................ 51 4.5 本章小结 ......................................................................................... 53 第五章 疲劳驾驶检测及监管系统的设计与实现 ................................ 54 5.1 系统需求分析 ................................................................................. 54 5.1.1 功能需求 ................................................................................... 54 5.1.2 非功能需求 ............................................................................... 56 5.2 系统总体设计 ................................................................................. 57 5.2.1 总体架构设计 ........................................................................... 57 5.2.2 数据库设计 ............................................................................... 58 5.2.3 功能模块设计 ........................................................................... 60 5.3 疲劳驾驶检测系统实现................................................................. 61 5.3.1 硬件设备选型 ........................................................................... 61 5.3.2 模型NPU部署 ......................................................................... 63 5.3.3 系统功能实现 ........................................................................... 66 5.4 后台监管系统实现 ......................................................................... 68 5.4.1 开发环境配置 ........................................................................... 68 5.4.2 系统功能实现 ........................................................................... 69 5.5 系统测试与分析 ............................................................................. 72 5.5.1 测试环境 ................................................................................... 72 5.5.2 功能测试 ................................................................................... 73 5.5.3 非功能测试 ............................................................................... 75 5.6 本章小结 ......................................................................................... 77 第六章 总结与展望 ................................................................................ 78 6.1 论文总结 ......................................................................................... 78 6.2 未来展望 ......................................................................................... 79 参考文献 ................................................................................................... 80  1 第一章  绪论 1.1 选题背景与研究意义 根据我国公安部发布的数据显示，自2017年以来，我国汽车保有量呈现出逐年增长的趋势。截至2022年11月底，我国汽车保有量已经突破3.18亿辆，机动车驾驶人总数也已超过5亿人。当前，我国机动车保有量及驾驶人口规模均位列全球第一。图1.1为我国2017至2022年汽车保有量统计图。  图1.1 2017至2022年国家汽车保有量统计图 汽车作为现代交通工具，给人们出行带来了极大的便利，然而与此同时，交通事故的发生也日益显著。在中国，由交通事故引起的死亡人数已连续11年高居全球首位。尽管交通事故的原因种类繁多，包括路况环境、极端天气等客观因素，但其中绝大部分源自于人的主观因素，例如不规范的行车行为、酒驾、疲劳驾驶等[1]。 疲劳驾驶是指在驾驶过程中，驾驶员因长时间连续驾车或者睡眠不足等原因出现疲劳状态，从而影响其注意力、反应能力以及判断能力的一种危险驾驶行为。疲劳驾驶不仅增加了交通事故的风险，对人民生命和财产安全构成了极大威胁。由此00.511.522.533.52017201820192020202120222.172.42.62.813.023.18汽车保有量(亿辆) 2 可以看出，疲劳驾驶已经成为严重影响行车安全的因素之一。 每年因疲劳驾驶造成的交通事故数量巨大，造成的人员伤亡和经济损失也非常严重，因此研究检测驾驶员疲劳状态的技术对于公共交通安全来说十分重要。如果设计一种能够及时检测驾驶员疲劳状态并对其进行警告的系统，可以有效预防交通事故的发生，有助于提高道路交通安全。 传统的疲劳驾驶检测系统采用多种方法来检测驾驶员的疲劳状态，常用的有生理信号检测以及车辆运动分析等[2]。通过这些方法检测驾驶员疲劳状态大大降低了交通事故的发生率。但是传统的疲劳驾驶检测系统也存在一些局限性，例如往往需要驾驶员佩戴特殊的设备，使用起来不够方便而且准确性受环境影响较大，在恶劣天气条件下，疲劳检测的准确性会大大降低。 为了克服传统疲劳驾驶检测系统的局限性，出现了基于深度学习的疲劳驾驶检测技术，该方法相比传统的疲劳驾驶检测方法具有许多优点：首先它使用起来更加方便，只需要使用摄像头或其他传感器即可；其次，它可以更加准确地检测驾驶员的疲劳状态，并且不受环境影响，因此该技术已经成为近年来该领域的研究热点，同样也是未来发展的新趋势[3]。 综上所述，本文采用深度学习技术实现了一种基于面部多特征融合的疲劳驾驶检测系统。使用摄像头检测驾驶员的眼部、嘴部及头部姿态，采用面部多特征融合判定驾驶员是否疲劳，该方法与驾驶员无接触并且不受环境影响，克服了传统疲劳驾驶检测系统的弊端，且准确性与稳定性较好，具有一定的研究意义与应用价值。 1.2 疲劳驾驶检测国内外研究现状 近年来，随着人们对道路交通安全重视程度的提高，疲劳驾驶检测技术逐渐成为关注的热点。目前，疲劳驾驶检测技术主要分为三类：一是基于驾驶员生理特征的疲劳检测技术；二是基于车辆行驶特征的疲劳检测技术；三是基于驾驶员面部特征的疲劳检测技术。 1.2.1 基于驾驶员生理特征的检测方法 基于驾驶员生理特征的检测方法是指通过使用各种医学仪器测量驾驶员与疲劳相关的生理指标，以判断驾驶员是否出现疲劳的一种接触式检测方法。其中包括脑电信号检测、眼电信号检测、肌电信号检测、心电信号检测等方法[4]。 在Katsis等人的研究中[5]，采用肌电信号检测法来评估驾驶员的疲劳状态。该方法通过在驾驶员的身体部位放置电极，测量肌肉的电活动并将信号发送到肌电图 3 记录仪进行分析。研究结果表明，随着驾驶员疲劳程度的增加，肌电图频率逐渐降低，而肌电图幅值则逐渐升高。 在Jung等人的研究中[6]，将内嵌式心电传感器放置于驾驶员车辆方向盘中，利用其获取驾驶员的心电信号，利用心率变异性的时域和频域变化来反映驾驶员的疲劳状态，具有较高的准确性。 Ma等人[7]利用眼电图信号开展了警觉性估计的研究，发现多个眼动特征与警觉状态存在关联性。他们进一步探究了眼动信号与疲劳之间的相关性，总结出了眼动信号与警觉状态之间的相关系数，并为多种基于眼动信号的疲劳检测系统开发提供了理论支持。 Zhang等人[8]设计了一种降速方法来控制车速，当发现驾驶员处于疲劳状态时主动降速，避免由于驾驶员疲劳而导致的交通安全事故的发生。 祝荣欣等人[9]采用了一种联合使用心电和肌电信号的方式进行疲劳驾驶检测的方法，检测准确率达到了91.75%。 基于驾驶员生理特征的疲劳检测技术具有高准确度的特点。但是因为检测设备成本较高且直接和驾驶员产生接触，影响驾驶员驾驶，这使得该方法在实际应用中存在诸多局限性，目前尚未得到广泛推广。 1.2.2 基于车辆行驶特征的检测方法 基于车辆行驶特征的检测方法是指通过对驾驶员所驾驶的车辆行为来分析当前驾驶员所处的状态。当驾驶员处于疲劳状态时，车速、方向盘转向角、行车轨迹、刹车板压力等相关数据会发生一定改变，通过分析这些数据信息会间接得到驾驶员所处的状态[8]。 Sandberg[11]在研究中采用时间序列分析的方法，通过测量如车速、方向盘转动角度等驾驶行为指标，再使用粒子群优化训练的人工神经网络算法对驾驶员的疲劳状态进行分类，实现了对驾驶员的疲劳检测。 华南农业大学王海林等学者[12]提出了一种基于车辆状态的疲劳检测方法，该方法通过分析车辆的加速度、换挡、刹车、转向等状态的变化来识别驾驶员的疲劳状态，通过对这些信息的分析，可以有效地进行疲劳驾驶检测。 Wang等人[13]采用随机森林算法，结合车辆侧向加速度、纵向加速度和方向盘转角等参数信息，预测驾驶员是否处于疲劳状态。 沃尔沃公司开发了一种集成多种传感器技术的驾驶员安全警告系统Driver Alert Control（DAC），集成了多种控制器、轨迹传感器、摄像头等装置，对驾驶员的驾驶行为进行多维度的分析和评估，实时检测驾驶员的驾驶状态。  4 AutoVue系统是由美国的I-teris公司开发的一种车辆安全辅助系统[14]，它通过在车辆面对公路的位置上安装摄像头，能够检测车辆是否偏离正常车道，并在必要时向驾驶员发出警告，提高驾驶安全性。 马永锋等学者[15]针对货车司机的疲劳状态进行了实验研究，利用行车速度等数据进行疲劳驾驶检测。研究结果表明，行车速度和加速度等数据可以作为评估货车司机疲劳状态的重要指标。当司机疲劳状态加剧时，车辆的平均速度、最大速度等数据均呈现增加趋势，加速度均值和车辆的稳定性逐渐降低。 基于车辆行驶特征的疲劳驾驶检测方法无需对驾驶员进行接触，避免了对驾驶员行车过程的干扰，但检测准确性受环境因素影响较大，具有一定局限性。 1.2.3 基于驾驶员面部特征的检测方法 基于驾驶员面部特征的疲劳驾驶检测方法主要是利用计算机视觉技术对驾驶员的面部特征进行实时监测和分析，例如对眼睛、嘴巴、头部姿态等变化进行分析，可以有效检测出驾驶员的疲劳状态[16]。 捷豹公司开发的“驾驶员监控系统”（Driver Monitor System）是一种内置于方向盘中的疲劳驾驶检测系统，它能够采集驾驶员的眼动信息和面部表情，并通过分析这些数据来判断驾驶员的注意力和疲劳程度。 比亚迪公司研发了一种疲劳检测系统，由电控单元和摄像头两大部分组成，通过获取驾驶员面部特征等数据推断驾驶员的状态，出现疲劳驾驶时进行相应的警示，为驾驶员提供可靠的安全保障。 1998年，卡内基梅隆大学驾驶研究中心研发出一种基于视网膜定位的疲劳驾驶检测系统[17]，该系统采用一种叫做PERCLOS的方法通过比较单位时间内闭眼时间所占的百分比来评估驾驶员的疲劳程度，具有高精度、实时性和非接触性的优点。 Owens等学者[18]在弗吉尼亚理工大学运用真实驾驶场景中采集的影像，深入研究了疲劳驾驶引发事故的共性，通过PERCLOS方法对驾驶员的疲劳状态进行分析。该方法能够精确的确定事故发生的时间，为评估驾驶员的行驶状态提供了一种客观的评价方式。 Zandi等人[19]借助驾驶模拟器对疲劳状态下驾驶员的眼部数据进行了采集，其中包括眼睛闭合度、瞳孔直径、眨眼数量等眼部信息。除此之外，为了提高判断准确性，该研究利用脑电信号数据辅助判断眼球动作参数在疲劳状态下的精确性，并使用随机森林算法对疲劳状态进行分析评估。研究结果表明，眼睛中所蕴含的信息可以较为准确地评价驾驶员的疲劳状态 基于驾驶员面部特征的疲劳驾驶检测方法具有简单直观、可实现非接触式测量 5 以及高精度检测等优点。但该方法在实际应用中还面临着一些问题，例如算法复杂、疲劳特征提取困难及疲劳判定标准缺乏统一等挑战。 表1.1总结了目前各类疲劳检测方法的优缺点。从表中可以看出，基于驾驶员面部特征的疲劳检测方法不需要接触驾驶员，具有良好的抗干扰能力，并且成本较低。因此，该方法已成为当前疲劳驾驶检测领域的研究热点。鉴于其优势，本研究采用该类疲劳驾驶检测方法进行研究。 表1.1疲劳检测方法对比 方法 优点 缺点 准确性 基于驾驶员生理特征 不受外界环境影响 与驾驶员接触且设备成本高 高 基于车辆行驶特征 与驾驶员无接触 受天气及路况等因素影响 一般 基于驾驶员面部特征 检测准确与驾驶员无接触 算法复杂度较高 较高 1.3 主要研究内容及论文组织结构 1.3.1 主要研究内容 本文的主要研究方向是疲劳驾驶检测，通过了解选题背景和意义，系统性地阅读了国内外相关文献，发现当前疲劳驾驶检测系统存在以下几个问题：1) 检测成本高；2) 易受环境影响；3) 检测指标单一。针对目前存在的几个问题，本文结合当前快速发展的深度学习和计算机视觉技术进行研究并实现基于驾驶员面部多特征融合的疲劳驾驶检测系统。具体的研究内容有以下几点： （1）训练人脸检测器。疲劳驾驶检测的首要任务是对驾驶员进行人脸检测。经过对主流的人脸检测方法进行研究，设计了一种基于SimAM无参注意力机制的轻量化YOLOv5s人脸检测网络，将模型的注意力更多聚焦于有效信息区域，提高模型的检测准确度。最后利用Wider Face人脸数据集对模型进行训练，分析训练结果。 （2）训练人脸关键点检测模型。获得驾驶员人脸区域后需要进行面部关键点检测和头部姿态估计，为同时满足人脸关键点检测及头部姿态估计任务，选用带有辅助子网络的PFLD人脸关键点检测模型作为基础模型并改进其网络结构：一方面为进一步加强模型的检测精度，增加了多尺度特征融合模块，另一方面，为降低模型参数量，提高检测速度，使用Ghost Bottleneck优化PFLD的主干网络。最后利用WFLW数据集验证模型改进的有效性。 （3）研究一种基于面部多特征融合的疲劳判定方法。完成对驾驶员面部的关键点定位后，根据关键点的位置利用EAR与MAR提取出眼部和嘴部的特征。同时， 6 利用PFLD的辅助子网络提取头部特征。最后，通过阈值设定实验得出面部不同部位的疲劳阈值，总结出疲劳判定方法。 （4）设计并实现疲劳驾驶检测及监管系统。首先，对系统需求进行详细分析后，设计了系统的总体架构及数据库表。其次，将疲劳驾驶检测模型部署至Orange Pi 5开发板并进行NPU加速、使疲劳驾驶检测系统实现实时的疲劳驾驶判定。最后，使用SpringBoot开发了后台监管系统，完成了驾驶员信息管理、疲劳数据管理、疲劳数据可视化等模块的实现。 1.3.2 论文组织结构 本文的正文部分包含六个章节，具体的组织结构安排如下： 第一章：绪论。首先，阐述选题的背景与研究意义，介绍疲劳驾驶检测的技术分类和近年来国内外疲劳驾驶检测领域的研究现状。然后，针对当前疲劳驾驶检测系统存在的问题进行总结，确定本文的研究目标。最后，简要阐述本文的研究内容和结构组成。 第二章：一种改进YOLOv5s的人脸检测模型。基于驾驶员面部特征的疲劳驾驶检测第一步就是获取驾驶员面部区域，本章首先对YOLOv5s目标检测模型的模型结构进行分析，提出引入SimAM注意力机制提高模型的检测精度，最后通过实验证明了模型改进的有效性。 第三章：一种改进PFLD的人脸关键点检测模型设计。获取驾驶员的面部区域后，需要进行关键点定位并获取头部姿态信息，本章首先对采用的人脸关键点模型PFLD进行简要介绍，并对模型从精度及大小方面进行优化，使用WFLW验证模型改进的有效性，为后文疲劳判定方法研究提供基础。 第四章：基于面部多特征的疲劳状态判定方法研究。本章主要内容是对驾驶员眼睛、嘴巴、头部姿态进行疲劳特征提取，并研究出判定疲劳的方法。首先确定各部位使用的特征提取方法，并根据实验得出提取面部疲劳特征所需要的参数阈值，确定每个部位的疲劳判定方法，最后提出了一种多指标融合的疲劳判定方法。 第五章：疲劳驾驶检测及监管系统的设计与实现。首先，对系统进行功能和非功能需求分析，设计了系统的总体架构及数据库表。然后，根据系统需求进行硬件平台选型及软件开发环境配置，完成系统的功能实现。最后，对实现的系统进行功能与非功能测试。 第六章：总结与展望。总结回顾了本文的研究成果，分析了本文所设计的疲劳驾驶检测系统的不足，并提出了改进方向。   7 第二章  一种改进YOLOv5s的人脸检测模型 基于驾驶员面部特征的疲劳驾驶检测流程主要包括人脸检测、人脸关键点检测、特征提取和疲劳判定。作为疲劳驾驶检测的第一步，人脸检测至关重要，本章重点介绍了人脸检测算法的选择与优化。对比分析多种人脸检测方法后选择使用单阶段目标检测器YOLOv5s作为疲劳驾驶检测中人脸检测模块的基础模型，为了提高YOLOv5s模型的检测精度，对其进行改进，并分析改进结果。 2.1 人脸检测方法分析 人脸检测是一种计算机视觉技术，用于识别和标记人脸。简而言之，人脸检测是指自动识别和定位图像或视频流中的人脸区域[20]。如图2.1所示，当输入数据中存在人脸时，人脸检测算法会输出一个或多个矩形框，用于精确定位人脸的位置。这种技术可用于安全监控、人脸识别、虚拟现实和视频分析等多种应用程序。  图2.1人脸检测示例图 人脸检测技术是研究疲劳驾驶检测的基础和关键。早期的人脸检测方法主要是从图像的局部区域提取特征，然后输入人脸分类器进行检测，例如Histogram of Oriented Gradient等方法[21]。然而，这些方法存在着诸多不足，其检测精度容易受到环境和样本的影响，无法获得高精度的结果。随着深度学习技术的不断发展，人 8 们开始将其引入到计算机视觉中的人脸检测领域，使得人脸检测技术得到了革命性的变化，并取得了显著进展。目前基于深度学习的人脸检测算法大致分为三类[21]：基于级联的人脸检测，两阶段的人脸检测以及单阶段的人脸检测。第一种基于级联的人脸检测算法通常具有较高的速度但检测性能一般，适合在算力有限和人脸数量较少的情况下使用，常见的算法例如Cascade CNN[22]和AdaBoost[23]等。第二种基于两阶段的人脸检测算法，首先产生一个候选区域，然后在第二个阶段进行回归，具有很高的识别精度。但其缺点是检测速度太慢，典型的有Face R-CNN[24]。最后一类为基于单阶段的人脸检测，该算法是以Anchor的分类与回归为基础进行检测，例如YOLO[25]、SSD[26]、Retina Net[27]等算法，其检测速度比两阶段法更快且检测性能优于级联法，是目前人脸检测算法中的一个重要研究方向。 2.2 基于改进YOLOv5s的人脸检测模型设计 在工业上，单阶段目标检测模型使用较多的是YOLOv5系列模型，目前共有四个版本，模型参数量由小到大依次为YOLOv5s、YOLOv5m、YOLOv5l、YOLOv5x，其中YOLOv5s模型参数量较小，检测速度快，常用于边缘设备部署。在疲劳驾驶检测中，根据应用场景，需要对驾驶员的面部进行实时的检测，对模型的检测速度有较高要求。考虑成本和能耗，以及检测设备的算力较弱，本文将使用YOLOv5s目标检测网络作为基础网络进行改进，并将改进后的模型应用在疲劳驾驶检测中。  图2.2 YOLOv5s模型结构图 2.2.1 YOLOv5s模型结构分析 如图2.2所示，YOLOv5s的网络结构主要包含Backbone（骨干网络）、Neck（颈CBL esUnitCSP1  CSP2  FocusSPPConvBNLea y eluCBLCBLAddCBL esUnit esUnit esUnitConvConvConcatCBLConvConvConcatCBLCBLCBLSliceSliceSliceSliceConcatMa PoolingConcatMa PoolingMa PoolingCBLCBLFocusCBLCSP1 1CBLCSP1 3CBLCSP1 3CBLSPPCSP2 1CBL上采样ConcatCSP2 1CBL上采样ConcatCSP2 1ConvCBLConcatConcatCBLCSP2 1CSP2 1ConvConv76 76 25538 38 2551  1  255608 608 3 个  组件 个  组件2  个CBLCBLBNLea y eluCBLBNLea y eluCBLCBL 9 部）、Head（头部）三个部分。首先，对输入图片进行预处理，包括数据增强等操作。然后，将其送入主干网络部分进行特征提取。接着，通过 Neck 部分对这些特征进行融合，得到融合不同尺寸的特征图。最后，将这些特征送入检测头部分进行检测，获取输出结果。 (1) Bac bone YOLOv5主干网络的创新主要有两部分：其一，增加了Focus模块[28]来进行切片操作，结合卷积核卷积操作，能够更好地保留图像细节特征；其二，为了增强Backbone和Neck部分的特征提取效果，提出了两种不同的CSP Net结构[29]，即CSP1_X结构和CSP2_X结构。 Focus模块可以在不增加计算量的情况下提高模型的感受野和特征提取能力。该模块通过将输入张量分割成多个子张量，并对每个子张量进行不同的卷积操作，最后将所有子张量的结果拼接起来得到最终输出。相比于传统的卷积操作，Focus模块可以更好地捕捉不同尺度的特征，从而提高模型的检测精度和鲁棒性。在Focus模块中，最重要的是切片（Slice）操作，通过切片操作可以降低图片大小，扩充图片维度。如图2.3所示，一个大小为4×4×3的图像，经过切片操作后，大小变为了2×2×12，极大地扩充了通道数。  图2.3切片操作效果图 在YOLOv5的Backbone中，包含了CSP Net结构。不同于YOLOv4中只将CSP Net结构应用于模型主干网络，在YOLOv5中，CSP Net结构包括两种设计：CSP1_X和CSP2_X。其中，CSP1_X结构应用于模型的主干网络，CSP2_X结构应用于模型的Neck中。图2.4和图2.5分别为CSP1_X，CSP2_X的具体结构。 1234132412123434111111111111111111111111111111111111111111111111Slice 10  图2.4 CSP1_X结构图  图2.5 CSP2_X结构图 (2) Nec  Neck部分是YOLOv5系列目标检测模型的加强特征提取网络部分，位于Backbone结构和Head结构之间，主要由SPP模块和FPN+PAN结构两部分组成。 空间金字塔池化SPP（Spatial Pyramid Pooling）思想由He等人提出[30]，SPP结构与传统的全局池化层不同，采用了多个尺寸的池化滑窗，将所有不同大小的池化结果拼接在一起形成一个固定长度的输出向量。由于在目标检测任务中，输入图片的大小和真实物体的大小 异很大，传统的池化方法会导致信息的丢失。为了避免这一问题，SPP可以自适应地将同一尺度不同大小的输入处理得到相同的输出大小，从而融合不同尺度特征图的信息。SPP的内部结构如图2.6（a）所示，可以看到SPP结构将特征张量拆分为四个分支，分别经过了1×1、5×5、9×9、13×13的Max Pooling，并将得到的四个特征张量做了拼接。图2.6（b）给出了具体的例子，输入13×13×512的特征张量，经过平行的四次最大池化，将特征张量进行拼接，得到13×13×2048的特征张量。  （a）SPP结构图 （b）SPP原理图 图2.6 SPP结构与原理图 CSP1  CBL esUnit esUnit esUnitConvConvConcat 个  组件 个  组件BNLea y eluCBLCSP2  CBLConvConvConcatCBLCBLCBL2  个CBLCBLBNLea y eluCBLSPPMa PoolingConcatMa PoolingMa PoolingCBLCBL1 113 13 512   13 135 5 11 在网络结构设计中，YOLOv5还借鉴了FPN[31]和PAN[32]的结构，在FPN主干网络后拼接PAN结构，结合下采样和上采样生成特征金字塔，将低层的强定位特征信息传递到上层从而提升了特征融合的能力。YOLOv5中FPN、PAN结构如图2.7所示。  图2.7 FPN、PAN结构图 (3) Head YOLOv5的Head部分是网络的最后一部分，主要负责对特征图进行分类和定位，并生成边界框和类别预测。Head部分包含了三个不同尺度的检测器，每个检测器都会对一个特定尺度的特征图进行检测，最终输出的是各检测器输出结果的融合。对于本文使用的YOLOv5s网络结构，它的输出分别是80×80，40×40，20×20大小的特征图。 YOLOv5s的损失函数主要包括三个部分：分类损失，边界框损失和置信度损失[33]。三类损失的加权和构成了YOLOv5s总体的损失函数。其中，分类损失是指网络对目标物体的分类预测与真实类别之间的误 ；置信度损失是指网络对目标物体是否存在的置信度预测结果与真实结果之间的误 ；这两种损失均采用二元交叉熵函数进行计算。边界框损失是指网络对目标物体边界框预测值与真实边界框之间的误 ，在YOLOv5s中，使用的是CIOU_Loss损失函数，具体公式如式（2-1）所示。  ()222_211_1LossDistanceCIOUCIOUIOUDistanceCIOU=−=−−−−+ （2-1）  224gtpgtpwwvarctanarctanhh=− （2-2） 19x19PANFPN38x3876x76608x60819x1938x3876x7619x1938x3876x76上采样下采样 12 其中，IOU为真实框与预测框的交并比；_2Distance为预测框中心点和真实框中心点的欧氏距离，_DistanceC为预测框和真实框最小外接矩阵的对角线距离；v为长宽比影响因子；gtw与pw分别代表真实框与预测框的宽度，gth与ph分别表示真实框、预测框的高度。 2.2.2 融合SimAM注意力机制的YOLOv5s人脸检测模型 虽然YOLOv5s在工业上是一种非常流行的目标检测模型，但是它也存在一些问题。例如，在本文人脸检测任务中，希望模型尽可能的关注图像中的人脸信息而忽略其他次要的信息，而没有添加注意力机制的YOLOv5s目标检测模型可能会将注意力分散到整个图像中的所有区域，而不是集中在最重要的人脸区域上。这可能会导致模型在处理具有复杂背景或小目标的图像时出现误判，因此本文希望在YOLOv5s目标检测模型上添加注意力机制，使其更加关注图像中的关键信息，提高目标检测精度。 在日常生活中，注意力机制是指人类大脑在认知过程中通过调控自身的感知与关注，将精力集中地投射到特定的信息、事物或任务上，以实现高效、准确地处理信息，并对其做出反应的过程。注意力机制作用的过程可以被视为一个特征动态选择的过程，即根据输入特征的重要性自适应加权特征，以此来细化特征映射[34]。 然而，现有的注意力模块存在两个问题：首先，它们只能在通道或空间维度上细化特征，这种限制使得它们难以灵活地学习不同通道和空间位置中的注意力权重。下图呈现了两种常见的注意力模块：（1）通道注意力模块。如图2.8（a）所示，该模块对相同通道的不同位置的特征同等对待，而区别对待不同通道的特征。（2）空间注意力模块[35]。如图2.8（b）所示，该模块对某一位置的不同通道同等对待，而对同一通道上的不同位置区别对待。然而，人类的注意力应当是空间、通道两种注意力协同工作，即每个神经元都应当区别对待。  （a）通道注意力模块                         （b）空间注意力模块 图2.8 常见注意力机制示意图 此外，目前注意力机制的结构通常是由一系列复杂的单元组合而成，常见的注 HC C H eneration  pansionFusion HC  eneration  pansionFusionC H 13 意力机制往往采用额外的子网络生成注意力权值，如SENet（Squeeze and Excitation Networks）[36]，其结构由全局平均池化、全连接层、Sigmoid模块、ReLu激活函数等单元组成，这些网络结构复杂，其中包含大量全连接、池化等操作，众多单元的计算需要大量算力。表2.1展示了目前几个主流的注意力模块额外引入的参数量，从中可以看出，SimAM注意力机制[37]是一种无参注意力机制，相比其他注意力机制，SimAM在增加模型特征提取能力的同时，不会带来模型参数量的增加。 表2.1 不同注意力机制对比表 注意力机制 参数量 SE 22/Cr CBAM 222/2Crk+ GC 22/CrC+ ECA k SRM 6C SimAM 0 在神经科学领域，最丰富的信息处理神经元会与周围神经元不同，它往往会更加活跃，这些高度活跃的度神经元可能会抑制周围神经元的活动。这种现象被称为“空间抑制”[38]。SimAM的作者认为，具有空间抑制效应的神经元在视觉处理任务中应当给予更高的优先级。而在寻找这种神经元的过程中，最简单的方法是通过度量目标神经元及其他神经元之间的线性可分性来实现。SimAM的作者在此基础上，利用空间抑制现象设计了一个能量函数，用以评估每个神经元的重要性，并成功实现了三维的注意力机制。该能量函数的数学公式如式（2-3）下：  ()()()1220111ˆ,,ˆ,MtttitiiebyxytyxM−==−+−− （2-3） 在上式中，M为每个通道中神经元个数，MHW=。t和tb分别指神经元线性变换时的权重与偏置。0y和ty为二值标签。t和ix分别代表输入特征CHWX=在同一通道内的目标神经元和其他神经元。而ˆt，ˆix为t，ix的线性变换，ˆtttb=+，ˆititxxb=+。 最小化上述公式，旨在训练同一通道内神经元t与其他神经元之间的线性可分性。为了优化模型，使二值标签1ty=，01y=−，并加入了正则项。据此，可得最终的能量函数定义如下：  14  ()()()()()122211,,,111MtttitittttiebyxxbbM−==−−++−++−（2-4）神能量函数的值越低时，目标经元 t 与周围神经元的 异越大，其重要性也就越高，每个神经元的最小能量函数的公式如式（2-6）得到。  ()()2*22ˆˆ4ˆ22tet+=−++ （2-5） 其中，11ˆMiixM==，2211ˆˆ()MiixM==− 。上述公式表明，当能量值较低时，同一通道内的神经元t与其他神经元之间的线性可分性更高，神经元越重要。因此神经元的重要性可以通过1𝑒𝑡∗来衡量。最后，经过SimAM注意力机制增强后的特征X的最终结果如式（2-7）所示。  1XsigmoidXE= （2-6） X为输入的特征图，𝑋̃为增强后的特征图。E为特征图中所有*te值的集合。⊙为点积运算。为防止E值出现过大的情况，通过sigmoid函数来限制E值。 为了加强YOLOv5s目标检测网络对人脸特征的提取能力，使模型更加关注人脸而忽略其他不重要的特征，本章在对比多种注意力机制后选择将SimAM无参注意力机制引入YOLOv5s目标检测模型，在不增加模型参数量的情况下，通过能量函数对输入特征图计算注意力权重，赋予重要信息更大的权重，以增强模型对重要特征的学习能力，改进后的网络结构如图2.9所示。  15  图2.9 融合SimAM注意力机制的YOLOv5s人脸检测模型结构图 2.3 实验及结果分析 2.3.1 数据集准备 本文使用的人脸检测数据集选用公开数据集Wider Face[39]，其制作者来自于香港中文大学，最早于2015年公开。该数据集的图像源于Wider数据集，由61个事件类别的图像组成。Wider Face共标注了393,703个人脸数据，这些人脸数据在大小、光照、姿态、表情和遮挡等方面呈现出多元的变化，该数据集为研究人脸检测技术提供了重要的可靠数据来源。图2.10显示了部分Wider Face数据集示例图。  图2.10 Wider Face数据集 Wider Face数据集的标注格式如图2.11所示。文件中第一行为图片文件名。第 16 二行为对应图片中的人脸框数目m，接下来m行表示图片中每一个人脸矩形框的位置和人脸属性。其中，人脸属性包含六个类别，分别为人脸的模糊程度，表情夸张程度，曝光程度，遮挡程度，是否为无效图片，是否是正常姿势。并使用数字0，1，2表示各人脸属性值。例如表情夸张程度分为两个级别，0表示正常级别，1表示一般模糊，2表示非常模糊。  图2.11 Wider Face数据标注文件图 由于Wider Face数据集只有训练集和验证集带有标注信息，测试集缺少标注框信息，因此在本研究中，选取已标注好人脸的12280张图片作为训练集，3226张图片作为验证集进行模型的训练。 2.3.2 评估指标 为了更加精确的评估目标检测模型的表现，在模型训练前，通常需要找到各类模型的通用评价标准。因此，在人脸目标检测中，一般使用以下几个概念来评估模型的性能： （1）精确率（Precision） 精确率（Precision）又叫做查准率。简单来说，它是指预测为正的样本中实际为正的样本所占的比例。Precision是二元分类模型中一个重要的性能指标，其计算公式如式（2-8）所示。  TPPrecisionTPFP=+ （2-7）  17 其中，TP为模型预测结果为真，实际结果也为真。FP为模型预测结果为真，实际结果为假。 （2）AP与mAP 一般情况下，AP（Average Precision）是用于二分类问题的一种性能评估指标。它通过模型输出的预测概率计算出每个样本的单个精度值，并对所有样本的精度值求平均值。mAP（Mean Average Precision）是多分类问题中各个类别AP的平均值。由于人脸检测只需要判断样本是否为人脸，因此本实验中只有一个类别，mAP与AP相等。AP的计算公式如式（2-9）所示。  ()10APPRdR= （2-8） 2.3.3 模型训练与结果分析 本次模型的训练使用Kaggle云端算力平台，具体配置如下表所示： 表2.2 Kaggle算力平台配置表 配置 参数 GPU Tesla P100 显存 16G 开发语言 Python 深度学习框架 Pytorch 训练前需要先将Wider Face数据集的标注文件格式通过脚本转换为YOLO系列模型可训练的格式；并在模型的配置文件中设置数据集的类别数量、目标名称、路径，下载YOLOv5s.pt文件作为模型的预训练权重，设置优化器为Adam优化器；实验中的其他超参数如表2.3所示。配置好后将数据集和模型文件上传至Kaggle算力平台，并在保持实验环境参数不变的情况下进行多次训练。 表2.3 实验参数设置表 参数 值 BatchSize 64 LearningRate 0.01 TrainSize 640 ValSize 640 Epoch 200 在目标检测算法中常用mAP（Mean Average Precision）衡量算法检测各个目标 18 的平均精度，但是在人脸检测任务中只有人脸一个目标，因此采用AP(Average Precision)衡量算法的精度。此外，模型大小和推理时间也是衡量模型性能的重要指标。 将原始YOLOv5s与改进后的YOLOv5s在相同数据集和参数设置下分别进行训练，将实验结果进行比较，如表2.4所示。 表2.4 训练结果对比表 模型 评价指标 Precision AP FPS 模型大小（MB） YOLOv5s 86.3% 71.6% 95 7.01 改进后的YOLOv5s 90.9% 73.7% 93 7.01 根据实验结果，改进后的YOLOv5s经过200轮训练后，人脸检测查准率达到90.9%，AP为73.7%，相比原始的YOLOv5s模型在查准率上提高了4.6%，在平均精度上提高了2.1%。此外，由于本文在YOLOv5s模型中引入的注意力机制为无参注意力机制，模型的参数量并没有增加，模型的检测速度也没有出现明显下降，改进后模型的大小为7.01MB，模型检测帧率可达95FPS。考虑到Wider Face数据集存在大量困难样本，因此该实验结果表明，改进后的YOLOv5s人脸检测模型在模型性能、模型大小、模型检测速度等指标上均已达到预期。  图2.12 Wider Face数据集上的定性分析 为了进一步验证该人脸检测模型的效果，选取Wider Face数据集中部分测试集 19 图片进行人脸检测，测试效果如图2.12所示，可以看到该人脸检测器不仅能够准确检测大目标，对于小目标和存在遮挡的目标也能够准确检测，模型的鲁棒性较好。 2.4 本章小结 本章提出了一种改进YOLOv5s的人脸检测模型。首先，对人脸检测概念及方法进行了详细介绍，对比分析目前常用的人脸检测方法后采用YOLOv5s目标检测模型作为训练人脸检测器的基础模型。随后，详细介绍YOLOv5s模型的结构，为了使YOLOv5s更加关注图像中的人脸信息而忽略其他次要的信息，提高目标检测精度，针对多种注意力机制进行对比分析，将无参注意力机制SimAM引入YOLOv5s模型。最后，对改进后的模型进行训练并测试。测试结果显示，添加了SimAM注意力机制的YOLOv5s人脸检测模型检测准确率较高，满足真实疲劳驾驶检测场景中人脸检测的要求。    20 第三章  一种改进PFLD的人脸关键点检测模型 驾驶员出现疲劳状态时会出现频繁地打哈欠，眨眼，瞌睡点头等行为，利用人脸关键点检测可以获得面部关键点的位置从而计算出相应的疲劳参数，进而判定驾驶员是否产生疲劳。因此，本章首先对人脸关键点检测方法进行介绍，分析目前常用的人脸关键点检测技术，选择采用带有辅助子网络的PFLD模型作为基础模型进行研究。随后，对本文采用的PFLD模型从主干网络、辅助子网络和损失函数三方面进行详细阐述，并针对PFLD模型存在的不足对其进行改进。最后，使用WFLW数据集训练模型，根据人脸关键点检测算法评价指标来分析实验结果。 3.1 人脸关键点检测方法分析 人脸关键点检测是将一张人脸图像输入到关键点检测模型中，该模型将输出一组具有特定语义的关键点坐标。根据不同的应用场景，需要采用不同数量的关键点模型，例如5点模型、21点模型、68点模型和98点模型[40]。图3.1显示了98点人脸关键点标注图。  图3.1 人脸98关键点标注图 人脸关键点检测方法可以分为参数化和非参数化两类。其中，参数化方法包括主动外观模型方法[41]、主动形状模型方法[42]、、基于局部[43]以及基于全局[44]的方法。而非参数化方法包括基于级联回归的方法[45]、基于图模型[46]的方法、以及基于深度学习的方法。近年来，深度学习技术取得了长足进步，基于深度学习的方法显著提高了人脸关键点检测精度，已成为当前的研究热点。  21 3.2 基于改进PFLD的人脸关键点检测模型设计 3.2.1 PFLD模型结构分析 2019年Guo等人[47]指出了目前人脸关键点检测技术存在的四个挑战： （1）在人脸关键点检测任务中，面部表情变化大、光照复杂等因素很容易导致关键点特征出现偏 或消失，尤其在遮挡或极端光照等情况下更为明显。 （2）由于人脸具有三维形态以及多样的姿态变化，因此成像质量参 不齐，关键点的定位很容易出现偏 ，甚至出现定位失败。 （3）在训练数据中，很多不同类别的图片数量分布不均，会造成数据类别不平衡的问题，模型不能获取到合理的图像特征，进而影响模型的泛化性能。 （4）对于嵌入式开发板或手机终端等计算资源有限的设备，需要在考虑模型检测速度及模型大小的前提下对人脸关键点检测模型进行优化，以便能够在实际应用中更加广泛地使用。 针对以上挑战，Guo等人提出了PFLD（A Practical Facial Landmark Detector）面部关键点检测模型。针对数据集中不同类别数据不平衡的问题，设计了新的损失函数，加大对小样本的惩罚力度，同时使用多尺度融合来提高感受野，精确定位人脸关键点。另外，PFLD模型带有一个辅助子网络，在模型训练中能够实时获得头部姿态角来影响损失函数，使其加大对头部偏转较大样本的损失，进而加强模型对困难样本的特征提取能力。  图3.2 PFLD网络结构图 考虑到后续疲劳判定需要获取头部姿态角，且嵌入式设备端的计算能力较低，PitchYaw ollConvolutionFull ConnectecdFull ConnectecdFull ConnectecdMobilenet- 2 Bloc  22 所以本文选择采用基于回归的神经网络模型PFLD进行人脸关键点检测。图3.2所示为PFLD的模型结构，由主干网络和辅助网络两部分所组成。橙色虚线包围的部分为PFLD的主干网络，用于预测人脸特征点的位置。绿色虚线包围的部分为PFLD的辅助网络，用于在训练阶段估计人脸的头部姿态信息，并用角度损失对主分支的关键点损失函数加权，用于平衡样本的不均匀分布。 表3.1显示了PFLD主干网络的具体结构。 表3.1PFLD主干网络结构表 Input Operator t c n s 112×112×3 Conv3×3 - 64 1 2 56×56×64 Depthwise Conv3×3 - 64 1 1 56×56×64 Bottleneck 2 64 5 2 28×28×64 Bottleneck 2 128 1 2 14×14×128 Bottleneck 4 128 6 1 14×14×128 Bottleneck 2 16 1 1 (S1)14×14×16 Conv3×3 - 32 1 2 (S2)7×7×32 Conv7×7 - 128 1 1 (S3)1×1×128 - - 128 1 - S1,S2,S3 Full Connection - 196 1 - 表3.1中，t为通道扩张倍数、c为特征图输出通道数、n表示该层的重复次数、s则表示卷积操作步长。这些参数的选择以及层数的设置对网络的性能至关重要。表格中的Bottleneck是PFLD主干网络的核心，其来源于MobilenetV2[48]的倒  模块（Inverted Residual），具体结构如图3.3所示。该模块采用类似于图3.4中的  模块的升降维处理，能够有效地解决梯度爆炸和梯度消失等问题，其不同之处在于倒  模块是先升维再降维，而  模块是先降维后升维。此外，倒  模块最后卷积层的激活函数使用的是线性激活函数。 倒  模块之所以这样设计有两个原因：第一，在低维空间使用ReLu会损失较多信息，而在高维空间使用ReLu损失较少，因此先将输入通过1×1卷积层提高通道数再使用ReLu会最大限度减少信息丢失。第二，为了减少模型计算量，每个倒  模块内部都使用了DW（Depthwise separable convolution）卷积，DW卷积输入通道和输出通道相等，相比普通卷积可以大幅缩减模型参数量。但是，对于DW卷积来说，它本身不具备改变通道维度的能力，从输入得到的特征维度已经减少了， 23 如果再使用ReLu激活会造成更多的信息丢失。因此，倒  模块在最后进行降维操作时没有使用线性激活函数。  图3.3 倒  模块  图3.4   模块 PFLD模型的辅助分支是用来进行头部姿态估计的，分析PFLD模型代码可知，主干网络中最后一个倒  模块的输出即为辅助分支的输入，辅助分支仅在训练过程中被使用，用于修改损失函数，以更好地关注那些比较罕见的样本，在训练过程中提高模型对这些样本的学习能力，从而提高预测结果的鲁棒性和稳定性。由表3.2可知，辅助网络并没有采用像主干网络那样的MobileNetV2 Block倒  模块，而是采用4个卷积层及2个全连接层，尽可能保留更多的特征信息。 表3.2 PFLD辅助网络结构表 Input Operator c s 28×28×64 Conv3×3 128 2 14×14×128 Conv3×3 128 1 14×14×128 Conv3×3 32 2 7×7×32 Conv7×7 128 1 1×1×128 Full Connection 32 1 1×1×32 Full Connection 3 - 在人脸关键点检测任务中，导致检测精度低的主要原因有两个方面。第一，在 24 模型训练过程中，训练集中可能包含大量的正面人脸图像，而缺少侧面或其他非正面的样本，这会导致模型在处理非正面人脸时表现欠佳。第二，关键点检测的损失函数通常使用L1或L2距离来衡量预测值和真实值之间的 距，但由于图像只是3D物体在2D平面上的投影，因此不能完全准确地反映出关键点在三维世界中的位置，导致模型的检测精度不高。 为了应对这个问题，PFLD模型的设计者主张对数据集中数量较多的样本施加相对较小的惩罚，对数据集中数量较少的样本施加较大的惩罚。同时，将辅助分支中检测的头部姿态信息添加到损失函数中，使主干网络更加关注数据集中姿态角过大的样本，最后作者提出了一个新的损失函数，如公式（3-1）所示。该损失函数的设计很好地解决了数据集中样本类别分布不均匀的问题，增加了模型在不同人脸姿态下的检测精度。  ()221n11111cos||||MNCKckmnnnmckLdM=====− （3-1） 式中，M为样本总数，N为关键点的总数，其中()111cosCKcknnck==−代表样本权重；∥dnm∥是网络预测的98个关键点坐标和图像对应真实的关键点坐标的L2距离；C为不同的人脸属性；𝜔𝑛𝑐是在当前训练batch中，第c种属性的权重；K是指三种头部姿态角的类别，取值为1、2、3，分别对应偏航角（yaw），俯仰角（pitch）和翻滚角（roll）；𝜃𝑛𝑘是网络辅助分支输出的三个头部姿态角和真实图像中姿态角的 值，若某一样本与真实值之间的角度 值较大，cos𝜃值将变小，从而增加了该样本在模型训练中的权重。 虽然PFLD模型已经在人脸关键点检测方面取得了较高的精度和较小的模型体积，但是仍然存在一些可以优化的地方。例如，可以通过增加多尺度特征融合层的方式丰富特征信息，进一步提高模型的精度。其次，尽管PFLD模型已经进行了模型大小的优化，但对于一些资源受限的移动设备等场景，仍然存在着计算复杂度和存储空间需求较高的问题。因此，本文使用PFLD模型作为基础模型进一步探索更加高效和精确的人脸关键点检测模型，以满足在疲劳驾驶检测场景下的需求。 3.2.2 多尺度特征融合模块 在目标检测任务中，浅层大尺度特征图所含细节信息有助于小目标的检测与定位，而深层小尺度特征图则能够更好地挖掘目标语义上的信息，对于大目标的分类识别有很大的帮助[49]。因此，采用多尺度特征融合技术将大尺度特征和小尺度特征融合，可以提高目标检测的准确率。同时，PFLD作者也发现，将多尺度特征融合 25 嵌入到PFLD主干网络中可以更加准确地定位人脸关键点。根据图3.5中截取的部分PFLD模型代码可知，PFLD主干网络的输出采用多尺度融合的方式，将经过倒  结构获得的16维14×14的特征图进行两次卷积，分别获得32维7×7特征图和128维1×1特征图，然后将这三种尺度的特征图进行平均池化（Average Pooling），再将得到的196维特征向量输入全连接层得到98关键点信息。  图3.5 部分PFLD模型代码 本文为了在PFLD原始模型的基础上进一步优化，提高模型的检测精度，将全连接层输入由原来的3个增加至5个，增加了56×56及28×28尺度的特图，希望在保持检测速度不变的情况下提高模型的检测精度。具体优化后的网络结构如表3.3所示。 表3.3 优化后的PFLD主干网络结构 Input Operator t c n s 112×112×3 Conv3×3 - 64 1 2 56×56×64 Depthwise Conv3×3 - 64 1 1 56×56×64 Bottleneck 2 64 3 2 28×28×64 Bottleneck 3 96 3 2 14×14×96 Bottleneck 3 96 4 2 7×7×96 Bottleneck 2 16 1 1 7×7×16 Conv3×3 - 32 1 1 7×7×32 Conv7×7 - 128 1 1 (S1)56×56×64 (S2)28×28×64 (S3)14×14×96 (S4)7×7×96 (S5)1×1×128 AvgPool AvgPool AvgPool AvgPool - - - - - - 64 64 96 96 128 1 1 1 1 - - - - - - S1,S2,S3,S4,S5 Full Connection - 196 1 -  26  3.2.3 轻量化PFLD网络结构 卷积神经网络促进了图像识别、目标检测等多种计算机视觉工作的发展。但是，将深度学习模型应用到实时检测的嵌入式设备中，其难点在于模型太大，并且需要庞大的计算量。例如ResNet-50[50]，它的参数大约是25.6M，若采用32位存储，则需要102.4MB的内存空间，这对于一般嵌入式设备是无法接受的。所以目前深度学习的发展方向是开发一种可移植的、高效的网络结构，为移动设备提供可靠的性能。尽管PFLD网络中采用较小的卷积核来构建有效的Bottleneck模块，大大降低了模型参数量和计算成本，但其余的1×1卷积层仍会导致内存占用和浮点运算量的大量消耗。  图3.6 普通卷积层 图3.6展示了神经网络中任意卷积层的普通卷积操作，其可以将输入特征图𝑋转换为n个输出特征图𝑌。计算公式如式（3-2）所示。  *YXfb=+ （3-2） 其中，𝑓为卷积核，𝑏为偏置项，∗表示卷积运算。由于卷积核数量和维度较大，因此普通卷积操作所需的计算量非常巨大。 华为诺亚方舟实验室在CVPR2020会议上发布了名为GhostNet的最新研究成果[51]。该研究从减少冗余特征图的生成成本角度出发，提出了一种全新的神经网络基本单元Ghost Module，用以构建轻量级神经网络，该网络在精度接近SOTA方法的同时，大幅提高了计算速度和效率。 深度神经网络通常包含大量特征图以获取输入数据信息，这些特征图有时会存在冗余，但却是必要的。图3.7展示了一个ResNet-50网络中第一个  模块处理后的特征图可视化结果。观察到其中一些特征图非常相似，这些特征图被称为“幻影”特征图。GhostNet作者认为，不必通过卷积运算获得“幻影”，而是可以通过更Conv 27 “廉价”的操作由其他特征图转换获得。例如，使用简单的线性转换而非复杂的非线性变换。GhostNet的核心思想在于使用较低的计算成本生成冗余特征图。  图3.7 特征层中的冗余特征图 因此，作者提出了Ghost Module来替代普通卷积操作。Ghost Module通过三个部分组成：第一部分是普通卷积操作，生成Input特征图的浓缩特征；第二部分是分组卷积（DW卷积）操作，即我们上文提到的“廉价操作”，通过该操作生成Ghost特征图；第三部分是将第一部分生成的浓缩特征图和第二部分生成的Ghost特征图进行拼接。利用Ghost Module可以生成与普通卷积相同数量的特征图。因此Ghost Module可以替换普通卷积操作，集成到现有神经网络结构中，以减少计算成本。Ghost Module模块的详细原理如图3.8所示。  图3.8 Ghost Module原理图 （1）首先使用普通卷积生成浓缩特征图𝑌′∈ℝ𝑚×ℎ′×𝑤′（忽略偏置项𝑏），计算公式如式（3-3）所示。 ConvInputOutputIdentity 28  *YXf= （3-3） 其中，𝑓′∈ℝ𝑐×𝑘×𝑘×𝑚为卷积操作使用的卷积核，X为输入特征图。 （2）使用一系列低成本的线性运算𝛷𝑖,𝑗来对𝑌′中的原始特征图𝑦𝑖′进行操作，生成s个Ghost特征图𝑦𝑖,𝑗，其计算公式如式（3-4）所示。  ()',,Φ,1,,,1,,ijijiyyimjs=== （3-4） （3）通过将浓缩特征图和Ghost特征图进行拼接，可以得到神经网络模型的最终输出结果。 在计算机视觉任务中，通常使用浮点运算量（FLOPs）来衡量模型的计算量和速度，具体公式如下  outinFLOPSChwCkk= （3-5） 根据该公式可计算出将普通卷积替换为Ghost Module后的加速比：  ()1snhckkrnnhckkshddss=−+      111ckkscssscckkddss=−+−+ （3-6） 式中，分母为Ghost Module总的复杂度，nhckks为第一步进行特征浓缩产生的浮点运算量。()1nshdds−为第二步逐层线性运算及特征层拼接产生的浮点运算量，分子为普通卷积的复杂度。 公式（3-6）表明，当需要生成具有相同深度的特征图时，与普通卷积相比，Ghost Module能够显著减少参数量和计算量。作者利用Ghost Module的优势，提出了Ghost Bottleneck，其结构如图3.9所示。它是为小型卷积神经网络设计的一种特殊模块。Ghost Bottleneck与Res Net中的  模块类似，包含多个卷积层和shortcut。Ghost Bottleneck由两个堆叠的Ghost Module组成，第一个Ghost Module用于扩展通道数，第二个Ghost Module用于减少通道数以与shortcut路径匹配。然后，将这两个Ghost Module的输入和输出通过shortcut连接起来。Ghost Bottleneck能够在保证网络性能的前提下，显著提高小型卷积神经网络的计算效率，为移动端或嵌入式设备上的深度学习应用带来更多机会。  29  图3.9 Ghost Bottleneck结构图 因此，为了满足在真实驾驶环境下的疲劳驾驶检测时间要求，提高模型的检测速度而不影响检测精度，使用Ghost Bottleneck替换原始PFLD主干网络中的倒  模块来进一步减少模型的计算量，最终优化后的PFLD网络结构如表3.4所示。 表3.4优化后的PFLD人脸关键点检测网络 Input Operator t c n s 112×112×3 Conv3×3 - 64 1 2 56×56×64 Depthwise Conv3×3 - 64 1 1 56×56×64 Ghost Bottleneck 2 64 3 2 28×28×64 Ghost Bottleneck 3 96 3 2 14×14×96 Ghost Bottleneck 3 96 4 2 7×7×96 Ghost Bottleneck 2 16 1 1 7×7×16 Conv3×3 - 32 1 1 7×7×32 Conv7×7 - 128 1 1 (S1)56×56×64 (S2)28×28×64 (S3)14×14×96 (S4)7×7×96 (S5)1×1×128 AvgPool AvgPool AvgPool AvgPool - - - - - - 64 64 96 96 128 1 1 1 1 - - - - - - S1,S2,S3,S4,S5 Full Connection - 196 1 -  Add host ModuleStride 1 Bottlenec Stride 2 Bottlenec  host Module host ModuleD Conv Stride 2 host ModuleAddBNBNBN  eLUBNBN  eLU 30 3.3 实验及结果分析 3.3.1 数据集选取及预处理 近年来，随着深度学习技术的快速发展和精度要求的提高，在人脸关键点检测数据集中人脸关键点的标注数量经历了长足的发展，已经从最初的5个点发展到如今超过200个点[52]。目前，人脸关键点相关任务大多采用公开数据集，因为手动标注数据量巨大，耗时费力，而公开数据集通常来源广泛，包含不同年龄、性别、肤色、姿势等方面的不同类型的人脸图像，这些数据可以帮助模型更好地适应不同场景下的人脸。 如表3.5所示，常见的几种人脸关键点数据集有5关键点、21关键点、68关键点、98关键点等。在一些情况下，标注更多的关键点可能会使模型具有更好的精度和泛化能力，因为它可以捕捉到人脸的更多细节，但同时标注更多的关键点也会增加标注和训练的复杂性，并可能导致过度拟合。因此，应该根据具体的需求来选择具有不同关键点数量的数据集。 表3.5常见人脸关键点数据集 数据集名称 图片数量 标注点数量 CelebA 约20万张 5 VGG-Face 约262万张 6 AFLW 20000训练集+689测试集 21 LFPW 1432张 29 300W 3148训练集+689测试集 68 XM2VTS 2360张 68 WFLW 7500训练集+2500测试集 98 人脸关键点检测任务除与标记的关键点数量相关外，还会受到背景、光照、表情、遮挡、头部姿态等多种因素的影响，其中表情变化、遮挡、头部姿态等都是非常重要的问题。因此，在模型训练时，选择一个合适的数据集至关重要。目前，常用的人脸关键点数据集中，最具代表性的是300W（300 Faces In The Wild）数据集[53]。该数据集充分考虑到了不同环境下人脸关键点检测的情况，其中包含了大量的图像数据，主要来源于LFPW、AFW、HELEN、XM2VTS、IBUG等多个数据集，并且每一张图片上都标注有68个人脸关键点。 虽然300W数据集涵盖了多种场景下的数据，但是其所包含的表情变化、头部 31 姿态变化、面部遮挡等数据较为有限。WFLW是Wu等人在2018年提出并开源的数据集[54]，共包含了10000张人脸图像，划分为7500张的训练图片和2500张的测试图片，每张人脸图像上都标注了98个关键点。如图3.10中WFLW数据集部分样例所示，与其他人脸关键点检测数据集相比，WFLW数据集中的人脸具有更大的表情、姿态和遮挡方面的变化。此外，该数据集还详细标注了遮挡、姿态、妆容、光照、模糊和表情等多种属性信息，非常适合用于人脸关键点检测模型的训练。因此，本文对PFLD模型的训练使用WFLW（Wider Facial Landmarks In-The-Wild）数据集。  图3.10 WFLW数据集样例 通过官方下载的WFLW数据集包含训练图片，测试图片以及训练集和测试集的数据标注文件。标注文件详细的内容格式如图3.11所示，每一组标注数据分别为98个关键点的x、y坐标，人脸框位置，人脸的六种属性以及标注数据对应的图片名称。  图3.11 WFLW数据集标注文件 本文所采用的人脸关键点检测模型PFLD采用基于回归的检测方式，直接预测人脸特征点在整张图中的位置，预测值范围约束在0-1之间。然而，WFLW数据集中特征点的标注方式并不是采用0-1区间，在原始的WFLW标签文件中也没有包含 32 头部姿态角的信息，这将导致辅助网络无法进行训练。因此，需要对原始数据集进行数据预处理，数据集预处理主要分为以下四个部分： （1）归一化 关键点使用人脸边框尺寸进行归一化至0-1，计算公式如下：  ()()1.211.21maxminmaxminwxxhyy=−+=−+ （3-7）  iminNiminNxxxwyyyh−=−= （3-8） 式（3-7）和式（3-8）中𝑤,ℎ分别为人脸框的宽和高；（𝑥𝑚𝑎𝑥,𝑦𝑚𝑎𝑥）表示人脸框所在区域的右上角坐标；（𝑥𝑚𝑖𝑛,𝑦𝑚𝑖𝑛）表示人脸框所在区域的左下角坐标；（𝑥𝑁,𝑦𝑁）表示关键点归一化后的坐标；（𝑥𝑖,𝑦𝑖）表示标注点坐标。 （2）数据增强 为了增加深度学习模型训练数据的数量和多样性，数据增强技术被广泛应用于模型训练中。数据增强技术通过对现有数据进行旋转、缩放、平移、镜像等变换来实现。这些变换可以使模型更好地学习和泛化到新的数据，并降低过拟合的风险[55]。 在本文中，我们采用了对图片进行旋转和镜像操作的方法来制作随机样本，以提高数据集的样本数量，使模型更好地适应不同的场景和任务。 旋转操作是以得到的人脸框中心点为转动中心，按顺时针、逆时针方向随机生成转动角𝜃来转动图像，𝜃最大为±30度。计算公式如公式（3-9）和公式（3-10）所示。  1212maxmincenterminmaxmincenterminxxxxyyyy−+=+−+=+ （3-9）  ()()cossin1cossinsincossin1cosiiicentercenteriiicentercenterxxyxyyxyxy=++−−=−+++− （3-10） 式中，（𝑥𝑐𝑒𝑛𝑡𝑒𝑟,𝑦𝑐𝑒𝑛𝑡𝑒𝑟）表示人脸框中点的坐标，（𝑥𝑚𝑎𝑥,𝑦𝑚𝑎𝑥）表示人脸框所在区域的右上角坐标，（𝑥𝑚𝑖𝑛,𝑦𝑚𝑖𝑛）表示人脸框所在区域的左下角坐标。  33 为了扩充数据集样本数量，本文还采用了水平镜像反转的方法进行数据增强。即对每张人脸图像进行镜像翻转。该方法可以有效提高数据集的多样性和数量。镜像反转的计算公式如式（3-11）所示。  ()2iicenteriiixxxxyy=+−= （3-11） （3）数据裁剪 针对数据集中标注的人脸框，采用裁剪的方式将图像中人脸所在区域截取出来，并将其大小调整为112×112，以达到更好的训练效果。 （4）计算头部姿态角 根据WFLW数据集图片中标定的面部关键点，利用平均的头部模型求解2维关键点到3维关键点的对应关系，然后根据旋转矩阵求出头部姿态角，存入训练标签文件中供PFLD模型辅助分支网络训练时使用。 经过离线数据集预处理后数据集图片从7500张变为75000张，部分预处理后的图像和标注文件如图3.12、图3.13所示。  图3.12 预处理后部分图像效果  图3.13 WFLW预处理后的标注文件  34 3.3.2 评估指标 为了准确评价算法改进的有效性，需要采用客观的评价指标对其进行全面评估，目前人脸关键点检测算法中最常用的评估指标为归一化平均误 （NME）[56]，具体公式如式（3-12）所示：  *,,2111||||1100%MijijNjiippMeNd==−= （3-12） 其中，N代表样本数量；M表示每张人脸中关键点的数量；𝑝𝑖代表模型预测的关键点位置；𝑝𝑖∗表示第i个关键点的真实位置；d为两眼中心距离，如3.1节中图3.1所示，两眼中心距离为第96个关键点和第97个关键点之间的距离。 3.3.3 模型训练与结果分析 本文使用Pytorch框架搭建模型，并在PC端进行训练，PC端具体配置如表3.6所示。 表3.6训练环境配置表 配置 参数 GPU GTX 3070 显存 8G 内存 32G 硬盘 1T 集成开发工具 Pycharm 将裁剪为112×112尺寸的WFLW数据集图片作为PFLD模型的输入，采用Adam优化器进行训练，学习率与权值衰减系数分别设置为10−4和10−6，批处理数量（BathSize）设置为64，一共执行200轮训练。 图3.14展示了PFLD改进前后在测试集上的性能表现。蓝色曲线表示改进后的PFLD模型在测试集上的损失曲线，最优损失为0.172；黄色曲线表示原始PFLD模型的损失曲线，最优损失为0.193。从图3.14可以看出，改进后的模型收敛速度更快，且损失值更小。  35  图3.14 模型改进前后Loss图 本研究采用归一化平均误 （NME）、模型推理时间（Inference Time）和模型大小（Model Size）三项指标对基础模型和改进后的模型进行了评估。使用WFLW数据集对模型进行了训练，并在训练后的实验数据上进行了对比分析。具体结果如表3.7所示。 表3.7实验结果对比 Model NME 推理时间（MS） 模型大小（MB） PFLD 0.0598 37.57 7.03 改进后的PFLD 0.0562 23.08 4.76 根据表3.7可知，本文提出的基于改进PFLD的人脸关键点检测算法在检测速度和检测精度上相比原始PFLD模型均有不同程度的提高，且模型大小降低了32%，  由于在真实的驾驶环境下，检测环境复杂多变，为了进一步观察改进后的PFLD算法在人脸关键点检测上的效果，从测试集中挑选正面人脸、遮挡人脸、侧面人脸以及在不均匀光照下的人脸等多类人脸图片，观察模型是否能够准确识别关键点。具体测试结果如图3.15所示。  36  （a）正面检测效果图  （b）存在局部遮挡检测效果图  （c）光照分布不均匀检测效果图  （d）侧面检测效果图 图3.15 多种场景下的人脸关键点检测效果图 综上所述，改进后的PFLD模型精度高、检测速度快、模型体积小，适合部署在嵌入式设备，能够满足在疲劳驾驶检测场景下的人脸关键点检测需求。 3.4 本章小结 本章提出了一种改进PFLD的人脸关键点检测模型。首先，对目前常用的人脸关键点检测方法对比分析，选择采用带有辅助子网络的PFLD模型作为基础模型研究；同时，对PFLD模型从主干网络，辅助子网络，损失函数三方面进行详细介绍。 37 然后，针对PFLD模型存在的不足对其模型结构进行改进，在PFLD模型的基础上进一步增加多尺度特征融合，提高模型的检测精度；并使用Ghost Bottleneck替换PFLD模型中使用的倒  模块，减少网络的参数量，提高模型检测速度。最后，对比了多种人脸关键点检测数据集，采用WFLW数据集对模型进行了对比实验，实验结果验证了模型改进的有效性。    38 第四章  基于面部多特征融合的疲劳状态判定方法 前文详细介绍了人脸检测和关键点检测算法及其改进。在疲劳驾驶过程中，驾驶员面部会表现出多种疲劳特征，如闭眼时间延长、打哈欠和瞌睡点头等。本章主要针对上一章获取到的98个人脸关键点对驾驶员眼部、嘴部、以及头部特征进行提取，并研究出判定疲劳的方法。 4.1 眼部特征提取与疲劳判定 驾驶员在出现疲劳状态时，眼部的表现最为突出。通常来说，眼部状况的变化是反映驾驶员疲劳程度的最好标志。因此，本文将眼部疲劳特征列为判断驾驶员疲劳的重要标准。 4.1.1 P  CLOS准则 为了准确评估驾驶员的疲劳状态并识别正常的眨眼与疲劳时的眨眼，本文采用卡内基梅隆研究中心提出的PERCLOS（Percentage of Eyelid Closure）作为识别眨眼的方法[57]。PERCLOS具有三种分类标准：EM、P70和P80，分别以上眼睑覆盖瞳孔一半以上、70%以上和80%以上的面积为判断标准来评估眼睛闭合程度。针对以上三种分类标准，美国高速公路安全管理局（NHTSA）在此基础上做了相关研究，认为PERCLOS中P80标准是最能反映人体的疲劳程度的标准[58]。因此本文采用P80标准来识别正常眨眼与疲劳时的眨眼。  图4.1 眼睛开合度随时间变化图 图4.1展示了在PERCLOS的P80标准下驾驶员眼睛从睁开到闭合再到睁开的过程。其中，𝑡1表示第一次眼睛张开面积从最大达到80%的时间；𝑡2表示眼睛的张时间眼睑睁开程度            t1t2t3t4 39 开程度第二次到达80%所需的时间；𝑡3表示瞳孔从最大到最小再变为张开面积达到20%的时间；𝑡4表示瞳孔张开面积从最大变为张开面积达到20%的时间。 根据图4.1所得的𝑃80标准下的PERCLOS计算公式如式（4-1）所示。  348021100%ttPtt−=− （4-1） 在计算驾驶员闭眼时长时，一般采用视频帧数作为计量单位，因为基于面部信息的检测具有时间上的连续性。所以，PERCLOS值可以由一段时间内的闭眼帧数除以该段时间内的总帧数。计算公式如式（4-2）所示。  1100%NnneyePfN== （4-2） 其中，𝑃𝑒𝑦𝑒代表PERCLOS值，N代表时间段内的总帧数，𝑓𝑛代表在第n帧时的人脸图像是否是闭眼状态，若处于闭眼状态则为1，否则为0。 4.1.2 眼部纵横比（ A ）计算 为了获取驾驶员眼部特征，准确反映睁眼闭眼状态，Soukupova等人[59]在2017年提出了一种眼睛纵横比（Eye Aspect Ration，EAR）的概念，可以根据EAR的数值来评估眼部的状态。 一次完整的眨眼动作包括睁眼、闭眼、再睁眼三个阶段，下图4.2展示了相应的EAR值变化过程。根据EAR值的变化情况可以模拟出驾驶员进行眨眼的行为。当眼睛睁开时，眼睛的宽度及高度产生的波动很小，EAR值较为平稳。当出现闭眼行为时，眼睛的宽度不变，但是眼睛的高度迅速下降，EAR值也随之下降。  图4.2 EAR随时间变化曲线图 时间   睁眼睁眼   阈值 40 可以看到，EAR变化趋势图能够很好的反映驾驶员的睁眼闭眼状态的切换。因此，本文采用计算EAR值的方式来提取驾驶员眼部特征。获取EAR值流程如下： （1）计算EAR数值首先需要获得人脸特征点坐标，通过上一章节介绍的人脸关键点检测算法获得人脸的98个特征点信息。图4.3为该人脸关键点检测算法标定的左眼及右眼关键点分布图。        （a）左眼关键点坐标        （b）右眼关键点坐标 图4.3 人脸左右眼关键点分布 （2）对左右眼分别进行眼部纵横比的计算，其公式分别如式（4-3）和式（4-4）所示。  6167626663656460||||||||||||||3||leftPPPPPPEARPP−+−+−=− （4-3）  6975707471737268||||||||||||||3||rightPPPPPPEARPP−+−+−=− （4-4） 若眼睛附近某两个关键点坐标分别为(𝑥1,𝑦1)和(𝑥2,𝑦2)，则其二维空间的距离可以由式（4-5）表示。  ()()222121dxxyy=−+− （4-5） （3）正常情况下，人在眨眼时，左眼和右眼会一起发生眨眼动作。因此，将两只眼睛的EAR平均值作为最终的EAR值，计算公式如式（4-6）所示。  2leftrightaverageEAREAREAR+= （4-6） 4.1.3 眼部纵横比阈值设定实验 为了确定眼睛是否处于闭合状态，需要为眼部纵横比（EAR）设置一个阈值，当眼部纵横比低于此值时，就可以判断眼睛处于闭合状态。然后，在一段时间内统计闭眼状态的视频帧数就可以通过计算获得PERCLOS值，从而判断驾驶员是否出现疲劳状态。EAR阈值的选取至关重要，考虑到不同驾驶员的眼睛形状及大小不同，为了更加准确和合理地设定EAR阈值，本文进行了眼部纵横比阈值设定实验。具 41 体的实验过程如下：  （1）12名实验人员每人拍摄2段面部视频，共24段视频，每段视频为30秒。将24段视频分为A、B两组，A组16段视频进行阈值设定实验，B组8段视频对设定后的EAR阈值进行测试。 （2）将A组视频中每一帧图像输入第二章训练得到的人脸检测模型，并将获得的人脸预测框区域的图像截取后传入人脸关键点检测模型获取人脸面部的98个人脸关键点坐标。 （3）将驾驶员左眼及右眼对应的关键点坐标传入公式（4-6）中进行计算得到双眼的平均EAR值。将该EAR值作为视频中每一帧图像对应的EAR值，观察A组测试后的结果，设定EAR阈值。 （4）采用B组视频对设定的EAR阈值进行测试，观察该EAR阈值是否能够正确识别眨眼状态。 截取某一实验人员在30s内的眨眼过程，其眼睛纵横比随时间的变化关系见图4.4。其中，横坐标为30s内的图像总帧数，纵坐标表示眼部纵横比EAR值。当驾驶员的眼睛处于睁开状态时，EAR值在一定范围内波动；当驾驶员的眼睛出现眨眼时，EAR值迅速下降然后恢复到稳定范围内，在EAR变化曲线上会产生一个波谷。   图4.4 某一实验人员30s内EAR变化趋势图 根据A组16段视频的实验结果可知，在睁眼状态下，眼睛纵横比在0.30~0.40范围内波动。完全闭眼状态下，眼睛纵横比大部分落在0.10~0.20之间。因此，取均值0.15为完全闭眼时的EAR值，再根据𝑃80分类标准，当眼睑覆盖瞳孔达到80%以 42 上时，就认为眼睛处于闭合状态。因此，最终将EAR阈值设置为0.18，以此作为判断眼睛闭合状态的标准。 使用B组的8段视频对设定的EAR阈值进行测试，测试该阈值能否正确识别驾驶员眨眼，所得结果如表4.1所示。可以得出，本文设定0.18作为EAR阈值可以准确的识别驾驶员闭眼状态，为后续疲劳判定提供基础。 表4.1 EAR阈值测试结果 视频序号 总帧数 检测闭眼次数 实际闭眼次数 1 约700 7 7 2 约700 6 6 3 约700 9 9 4 约700 8 8 5 约700 6 6 6 约700 7 7 7 约700 6 6 8 约700 6 6 4.1.4 基于眼部特征的疲劳判定 通过阅读多篇与疲劳检测相关的论文，发现根据眼部特征判断疲劳的方法主要有以下几种： （1）通过计算一定时间段内眨眼频率判定驾驶员是否疲劳。李庆臣等人采用EAR值的变化过程作为眨眼检测的方法，并通过一段时间内眨眼频率的变化进行疲劳驾驶检测。当每分钟的眨眼次数大于20次时，认定此时驾驶员进入疲劳驾驶状态，并通过增大眨眼频率的动作减轻疲劳程度[60]。 （2）通过驾驶员的连续闭眼时长判定是否疲劳。根据不同的数据和研究，驾驶过程中，眼睛离开前方时间多于2秒，会大大增加交通事故的风险。在我国科目三考试中，考生在行驶中眼睛脱离行驶方向超过2秒钟被视为盲开，属于危险驾驶行为，按考试标准当判不及格。因此，很多疲劳驾驶检测系统将闭眼时长作为疲劳判定标准。 （3）通过单位时间内眼睛闭合时间所占的百分比（PERCLOS）来评估驾驶员疲劳程度。PERCLOS的提出者明确提出了两个阈值0.075和0.15[61]。根据这两个阈值，可以将驾驶员的状态可以分为三种：清醒、疲劳和困倦。总之，PERCLOS是一个非常有效的评估驾驶员疲劳程度的指标。 综合以上疲劳判定方法，本文通过闭眼时间及PERCLOS值来判定疲劳。将驾 43 驶员最长连续闭眼时长设定为2秒，超过2秒即判定为疲劳。此外，计算60s内的PERCLOS值，若超过0.15则认为出现疲劳状态。整体的基于眼部特征的疲劳判定流程如图4.5所示。  图4.5 基于眼部特征的疲劳驾驶检测判定流程图。 图4.6展示了基于眼部特征的疲劳判定检测结果。其中，图4.6（a）为正常状态下的检测效果；图4.6（b）和图4.6（c）分别为连续闭眼时间和PERCLOS值达到疲劳阈值时的检测效果。测试结果表明，本文所设定的眼部疲劳阈值可较准确地判定驾驶员是否处于疲劳状态。 开始读取视频帧是否读取结束人脸检测人脸关键点检测计算闭眼时间计算PERCLOSPERCLOS 0.15预警YESNO结束获取EAR闭眼时长  2sYESNOYESNO 44  （a）正常状态 （b）连续闭眼时间过长 （c）PERCLOS值过高 图4.6 基于眼部特征的疲劳驾驶判定结果图 4.2 嘴部特征提取与疲劳判定 驾驶员在疲劳状态下，特别是轻度或中度疲劳时，打哈欠是一个常见的表现，因此，嘴部特征也是判定疲劳驾驶的重要指标。 4.2.1 嘴部纵横比（MA ）计算 驾驶员的嘴部状态大致分为三种。分别为打哈欠，轻微张开，闭合。其中，可以将打哈欠归为疲劳状态，轻微张开和闭合归为正常状态。 打哈欠是人们感到疲劳时的一种生理反应，表现为嘴巴由紧闭到微微张开，再由张开到最终回到紧闭状态的一个过程，持续时间较长。因此，通过判断嘴巴的张合程度是区分疲劳状态与正常状态的有效方法。 为了判断驾驶员是否打哈欠，本文使用嘴部纵横比（Mouth Aspect Ratio，MAR）来检测嘴部状态。其原理类似于用于检测眼部状态的EAR。为了降低嘴唇厚度的影响，本文使用嘴部内轮廓的关键点计算MAR。如图4.7所示，嘴部内轮廓关键点下标为88~95。  图4.7 嘴部关键点标注图 嘴部纵横比MAR的计算公式如式（4-7）所示。  8995909491939288||||||||||||||3||PPPPPPMARPP−+−+−=− （4-7）  45 4.2.2 嘴部纵横比阈值设定实验 疲劳驾驶会影响驾驶员的嘴部表现，其中打哈欠是较为典型的状态之一。本文采用嘴部纵横比（MAR）来识别打哈欠现象。MAR值在嘴巴闭合时最小，当嘴巴逐渐张开时，MAR值随之增大，因此可以根据设定MAR阈值来判断驾驶员是否出现打哈欠状态。 为了使MAR阈值设置更加合理准确，本文使用公开数据集YawDD[62]进行MAR阈值设定实验。YawDD数据集是在真实驾驶环境下收集的数据集，其中包括较多驾驶员模拟疲劳状态下打哈欠的视频，非常适合本文MAR阈值设定实验。如图4.8所示，本文选用仪表盘上方镜头采集的29个视频进行MAR阈值设定实验。  图4.8 MAR阈值设定实验的YawDD视频 具体实验过程如下： （1）下载公开数据集YawDD，选取其中仪表盘上方镜头采集的视频，共29个视频。其中，16个男性驾驶员视频，13个女性驾驶员视频，将其分为A、B两组。A组包括11个男性驾驶员视频和8个女性驾驶员视频，B组包括5个男性驾驶员视频和5个女性驾驶员视频。 （2）将A组中每个视频输入人脸检测模型和人脸关键点检测模型分别进行人脸检测和人脸关键点定位，获得98个人脸关键点坐标。 （3）使用公式（4-7）计算A组视频中每一帧图像的MAR值，分析实验结果，设定MAR阈值。  46 （4）使用B组视频测试MAR阈值设定是否合理，能否准确区分驾驶员打哈欠和正常状态。 图4.9中显示了一段测试视频中的哈欠片段对应的嘴部纵横比（MAR）变化曲线。其中，横坐标表示视频片段的帧数，纵坐标表示MAR值。  图4.9 测试视频片段中EAR变化图 从图4.9中可以看出，嘴部闭合状态对应测试视频的前200帧左右，此时，EAR值在0.10左右持续波动；在200帧到300帧左右的时间段内，MAR值形成了一个峰值约为0.90的大幅波峰，持续约100帧左右，该波峰对应着视频中第一次打哈欠的动作；300帧到500帧时，MAR值又回到0.1以内波动，对应视频片段中驾驶员嘴巴闭合；500帧左右MAR又出现一个持续的波峰，峰值也在0.90左右，对应视频片段中第二次打哈欠动作。 统计A组实验中所有视频样本打哈欠形成波峰的个数，计算半峰高度并对其取平均值，即可得到MAR阈值约为0.45。根据相关研究[63]，一次打哈欠时长大约为2~5秒。因此，为了区分驾驶员嘴巴正常张开与打哈欠，本文设定当MAR值超过0.45且哈欠持续时长超过2s则认为驾驶员出现疲劳状态。 使用B组的10段视频对设定的MAR阈值进行测试，测试该阈值能否正确识别驾驶员打哈欠。所得检测结果如表4.2所示。   47 表4.2哈欠次数检测结果统计表 视频序号 视频时长（s） 检测次数 实际次数 1 85 3 3 2 78 3 3 3 69 3 4 4 66 3 3 5 79 3 3 6 71 2 3 7 91 3 3 8 40 3 3 9 101 4 4 10 49 2 2 由表4.2可以看出，对于视频6，实际打哈欠为3次，但检测结果为2次，出现一次漏检。同样，视频3中也出现一次漏检。检查原视频后可知，根据图4.10（a）所示，视频6中的测试者在打哈欠状态时，用手遮挡嘴部，人脸关键点检测模型无法精确反映嘴部关键点位置的变化，因此出现漏检情况。如图4.10（b）所示，视频3中的漏检是由于测试者打哈欠时间较快，没有持续超过2秒。综合上述分析，在排除某些干扰因素影响下，本文中所设定的MAR阈值可以有效地检测哈欠，并能够满足多种驾驶场景下的检测需求。  （a）视频6漏检原因 （b）视频3漏检原因 图4.10 漏检原因示意图 4.2.3 基于嘴部特征的疲劳判定 打哈欠是判别疲劳驾驶的重要特征，根据相关研究[64]，本文设定60s内检测到2次哈欠次数即判定驾驶员处于疲劳状态。基于嘴部特征的疲劳判定流程如图4.11 48 所示。  图4.11基于嘴部状态的疲劳判定流程图 图4.12为基于嘴部特征进行疲劳判定的检测效果。其中，图4.12（a）为正常状态下的表现，图4.12（b）为模拟疲劳状态下的检测结果。系统能够及时发现驾驶员的打哈欠行为，并通过警告信息提醒驾驶员。  （a）正常状态下检测效果 （b）模拟疲劳状态下检测效果 图4.12 基于眼部特征的疲劳驾驶判定结果图 开始读取视频帧是否读取结束人脸检测人脸关键点检测计算60s内哈欠次数预警YESNO结束获取MAR哈欠次数  2YESNO 49 4.3 头部姿态特征提取与疲劳判定 在驾驶机动车过程中，驾驶员的头部通常是保持着直视前方的姿态，除此之外，驾驶员在看后视镜时，头部会出现短暂的晃动，但这种晃动不会短时间多次出现。而当驾驶员处于疲劳状态时，会出现反复点头等行为，因此我们可以对驾驶员在单位时间内的点头次数进行统计，就可以反映出驾驶员的疲劳状态。 4.3.1 获取头部姿态角 头部姿态估计是一种基于计算机视觉技术的方法，用于获取面部图像中头部的姿态角度。在头部姿态检测过程中，计算机首先会提取2D面部关键点的坐标，并将其转换到3D空间中，通过计算旋转矩阵得到头部姿态角，最终得到真实环境下的头部姿态信息[65]。如图4.13所示，姿态角可以用三个欧拉角来表示：俯仰角（pitch）、偏航角（pitch）以及滚转角（pitch）。其中，俯仰角指的是围绕x轴旋转的角，表示头部的上下运动；偏航角指的是围绕y轴旋转的角，表示头部向左向右旋转；滚转角指的是围绕z轴旋转的角，表示头部的左右倾斜。在疲劳状态下，头部主要表现为上下点头或者向一侧倾斜，因此，俯仰角和滚转角会发生较大变化，而偏航角变化较小。  图4.13头部姿态角示意图 根据第三章中PFLD模型结构图及模型代码可知，辅助分支只在PFLD模型训练的过程中使用，用来预测头部姿态角。目的是增大对样本中姿态角过大或数量较少样本的惩罚，使得人脸关键点网络预测出的关键点位置更加鲁棒。但是，在面部 50 多特征融合的疲劳驾驶检测过程中，不仅需要使用到人脸关键点定位坐标，还需要获得头部姿态角，通过姿态角变化来反映驾驶员的疲劳状态。 因此，在模型推理时需要加入PFLD的辅助子网络，将辅助分支预测的头部姿态角直接输出，使该神经网络模型能够在完成人脸关键点检测任务的同时估计出头部姿态角，为后续根据驾驶员头部姿态进行疲劳判定的工作提供基础。 当头部姿态发生变化时，对应的头部姿态角会有明显变化，图4.14所示为通过PFLD辅助分支获取的头部姿态角效果。  图4.14 PFLD输出头部姿态角效果图 4.3.2 基于头部姿态的疲劳判定 根据相关文献[66]，驾驶时正常姿态下俯仰角的变化范围大致为5°~7°。驾驶员头部姿态因疲劳产生变化时，头部偏航角变化较小，主要变化集中在俯仰角和滚转角的改变。因此，需要设置俯仰角及滚转角的阈值，即当俯仰角和滚转角的角度变化超过该阈值时就表明驾驶员可能出现疲劳驾驶的情况。但是，驾驶员在驾驶过程中也会偶尔出现调整头部姿势的情况。与疲劳状态不同的是，驾驶员清醒状态下主动调整头部姿势的速度较快，俯仰角和滚转角不会长时间处于超过阈值的状态。因此，可以通过设置低头、偏头的持续时间与疲劳状态进行区分，避免出现错判、漏判。 车辆行驶过程中，若驾驶员的视线离开前方持续2秒以上，会大大增加导致交通事故的风险[67]。因此，本文设定当|𝑃𝑖𝑡𝑐ℎ|≧20∘或者|𝑅𝑜𝑙𝑙|≧20∘且持续时间超过2s时，就认为驾驶员处于打瞌睡的状态。 图4.15为基于驾驶员头部姿态特征的疲劳判定流程。  51  图4.15 基于驾驶员头部特征的疲劳判定流程 4.4 疲劳状态综合判定及测试 将以上基于眼部、嘴部、头部三类特征的疲劳判定方法融合后的疲劳判定流程如图4.16所示。 为了测试本文疲劳驾驶检测方法的准确性，使用YawDD数据集中Mirror部分的视频片段进行验证。该部分有320段视频，共分为三类，分别为驾驶员正常状态、交谈状态、疲劳状态时的视频片段。将YawDD数据集中Mirror部分的320段视频剔除掉其中驾驶员戴有墨镜的视频，最后得到298段包含有不同性别、年龄的志愿开始读取视频帧是否读取结束人脸检测人脸关键点检测预警NO结束获取头部姿态角YESNONOYESYESYES Pitch   20°或 Roll   20°偏离时间  2s 52 者在模拟驾驶场景下的视频。 图4.16 基于面部多特征融合的疲劳判定流程图 对筛选出来的298段视频采用本文设计的基于面部多特征融合的疲劳驾驶检测方法进行测试，得到的结果如表4.3所示。 表4.3 YawDD数据集下检测结果 视频类别 视频数量 疲劳数量 检测数量 准确率 平均准确率 帧率 正常状态 87 0 2 97.7% 95.3% 22FPS 交谈 86 0 6 93.0% 疲劳 125 125 131 95.2% 根据表4.3可知，虽然本文采用的疲劳驾驶检测方法在某些情况下存在将驾驶员正常状态或交谈状态判断为疲劳状态的问题。但总体上，本文所采用的疲劳驾驶检测方法准确率较高，且检测实时性较好。 图4.17为测试过程中驾驶员正常状态和模拟疲劳状态时的测试结果。从图中可开始读取视频帧是否读取结束人脸检测人脸关键点检测预警YESNO结束获取EAR、MAR、头部姿态角眼部疲劳判定嘴部疲劳判定头部疲劳判定面部多特征融合判定是否疲劳YESNO 53 以看出，当驾驶员出现疲劳状态时，本文提出的方法可以准确识别并进行预警。  （a）正常状态 （b）疲劳状态 图4.17 疲劳判定结果图 4.5 本章小结 本章主要研究了一种基于面部多特征融合的疲劳判定方法。首先通过计算EAR、MAR的方式获取眼部及嘴部特征，并采用PFLD辅助子网络获取头部特征。其次，进行了面部各部位的疲劳阈值设定实验，获得合适的疲劳阈值。最后，将各部位的疲劳判定方法进行融合，提出了一种基于面部多特征融合的疲劳驾驶检测方法。通过公开数据集YawDD的测试，该方法的疲劳驾驶检测准确率为95.3%，具备一定的研究意义与应用价值。    54 第五章  疲劳驾驶检测及监管系统的设计与实现 随着汽车数量的持续增加，道路交通事故的数量也呈逐年上升趋势。在造成道路交通事故的多种因素中，疲劳驾驶是主要原因之一。因此，疲劳驾驶检测系统的开发与应用对于保障道路交通安全至关重要。基于此，本章设计并实现了疲劳驾驶检测及监管系统。其中，疲劳驾驶检测系统能够及时的检测驾驶员的状态，若出现疲劳驾驶能够进行实时提醒。而后台监管系统能够对驾驶员的驾驶数据进行统计分析，及时发现长期存在疲劳驾驶现象的驾驶员并对其进行提醒，从而有效减少疲劳驾驶的发生。 5.1 系统需求分析 5.1.1 功能需求 疲劳驾驶检测系统需要实时的检测驾驶员的驾驶状态，对出现疲劳状态的驾驶员进行提醒，并且将驾驶数据及时上传至后台监管系统，因此疲劳驾驶检测系统的功能需求如图5.1所示。  图5.1疲劳驾驶检测系统功能需求分析图 （1）用户登录功能 用户登录功能是疲劳驾驶检测系统的必备功能之一，监管人员在后台监管系统中添加驾驶员的个人信息及授权码后该驾驶员才能正常登录使用。疲劳驾驶检测系统要求驾驶员提供ID和授权码，以确保用户的身份安全。 （2）面部图像采集功能 该功能要求系统能够通过高清摄像头实时采集驾驶员的面部图像，以便后续对注 登 图像采集疲劳判定标准选择疲劳状态分析疲劳预警驾驶数据上传驾驶员 55 驾驶员面部特征进行实时分析。此外，该功能还需要支持多种光线环境下的面部图像采集，以保证系统在不同时间和场景下都能准确采集面部图像。 （3）疲劳判定标准选择功能 疲劳驾驶检测系统能够提供多种疲劳判定标准供用户选择，以便用户根据自己的需求选择合适的标准。不同的驾驶员可能适合不同的判定标准，因此系统需要能够清晰地展示每个标准的具体内容和适用范围，以便用户做出正确的选择。 （4）疲劳状态分析功能 疲劳状态分析功能是疲劳驾驶检测系统的核心功能。该功能要求系统能够根据采集到的面部图像进行疲劳状态的分析，以便及时发现疲劳驾驶的风险。该功能需要结合选择的疲劳判定标准进行分析，并通过算法对面部图像中的眼睛、嘴巴、头部等特征进行识别和分析，得出疲劳状态的分析结果。 （5）疲劳状态预警功能 该功能要求系统能够在发现驾驶员出现疲劳状态时及时预警，以提醒驾驶员注意安全。该功能可通过声音、及图像等方式进行预警，以便不同情况下的驾驶员能够及时注意到预警信息。 （6）驾驶数据上传功能 驾驶数据上传功能是疲劳驾驶检测系统的重要功能之一，该功能要求系统能够将疲劳驾驶检测数据及时上传到服务器数据库，以便监管人员进行数据分析和处理。该功能需要支持多种上传方式，如Wi-Fi、4G、5G等，以便在不同的网络环境下都能够完成上传。此外，上传的数据还需要进行加密和安全存储，以保证数据的安全性和隐私性。 后台监管系统可以使监管员对驾驶员的驾驶数据进行动态管理与分析，以便监管人员对长期出现疲劳驾驶状态的司机进行提醒，避免驾驶员因疲劳驾驶行为造成生命及财产损失，后台监管系统的功能需求如图5.2所示。  图5.2后台监管系统功能需求分析图 注 登 疲劳数据可视化疲劳数据管理驾驶员信息管理监管员 56 （1）注 功能 只有在注 界面注 的监管人员才能使用后台监管系统，监管人员注 后，服务器后台会向监管人员的注 邮箱发送激活邮件，监管人员点击激活邮件中的激活链接后才能使用账号进行正常登录。 （2）登录功能 注 并激活后，进入登录界面，在表单中输入信息后点击登录，在服务器后台会将监管员输入的邮箱及加密后的密码与保存在服务器数据库中的监管人员账号信息进行比对，通过后登录成功，只有登录成功的账号才可进行后台监管系统的使用。 （3）疲劳数据管理功能 系统可以通过读取服务器数据库中疲劳驾驶检测系统上传的驾驶员驾驶数据，通过数据对驾驶员的驾驶状态进行分析评估。 （4）驾驶员信息管理功能 监管人员需要通过该功能添加驾驶员信息，给驾驶员生成ID及授权码，驾驶员可通过ID及授权码登录疲劳驾驶检测系统进行疲劳驾驶检测。 （5）疲劳数据可视化功能 疲劳数据可视化功能将驾驶员的疲劳数据通过图表或其他直观的方式展示出来，以便监管员能够更好地理解和分析数据。 5.1.2 非功能需求 （1）系统可靠性 系统在使用过程中能够保持稳定，不出现意外故障，确保系统始终能够正常运行。该系统直接影响驾驶员的行车安全，因此该系统可靠性非常关键。 （2）系统实时性 疲劳驾驶检测系统需要具有实时性，即能够快速地响应驾驶员的状态变化，并及时预警。由于驾驶员的疲劳状态可能会随时出现，所以系统的实时性非常关键。为了提高系统的实时性，需要优化系统架构和算法设计，尽可能减少系统的响应时间，此外还需要采用高性能的硬件设备和稳定的网络传输，确保数据传输的实时性。 （3）系统易用性 该系统的用户是广泛的车辆驾驶员，因此系统的易用性至关重要。驾驶员操作该系统，应当不需要太多的培训和指导，为了提高系统的易用性，可以设计简单明了的用户界面，减少操作步骤和复杂度。 （4）系统可扩展性  57 考虑到技术的不断发展和用户的需求变化，系统需要具有良好的可扩展性，即能够支持多种设备的接入和多种算法的集成，以适应未来的技术发展和用户需求。为了提高系统的可扩展性，可以设计模块化的系统架构，采用更加标准化的接口和协议。 5.2 系统总体设计 5.2.1 总体架构设计 根据系统需求分析，设计了系统总体架构图，如图5.3所示。该系统架构图包含三个部分，即数据存储层、业务层、展示层。  图5.3 系统总体架构图 MysqlRedis登录疲劳标准设定驾驶状态实时显示疲劳预警疲劳驾驶检测系统后台监管系统技术展示层业务层数据存储层驾驶数据可视化驾驶员信息展示注 登录CSSPyqt5EchartsHTMLJavaScript登录服务疲劳判定验证驾驶员信息驾驶员信息管理驾驶数据管理PytorchSpringBootMybatisOpencvRknn Toolkit2 Lite保存驾驶数据注 、登录服务 58 （1）数据存储层：该层主要使用服务器中的Mysql数据库存储数据，数据库中保存的是驾驶员在行驶过程中出现的一些疲劳信息。例如眨眼次数、哈欠次数、点头次数等信息，同时还保存了驾驶员的个人信息及驾驶行为的历史记录。 （2）业务层：该层实现了各功能的业务代码，使用SpringBoot开发了后台监管系统，并使用持久层框架Mybatis实现数据的存取。使用Pytorch框架构建模型，并采用Opencv计算机视觉库及RKNN Toolkit2 Lite 框架在Orange Pi 5嵌入式开发板端进行模型推理。 （3）展示层：该层分为两部分，一部分为疲劳驾驶检测系统展示页面，由驾驶员使用，能够实时的输出驾驶员的疲劳状态，当驾驶员出现疲劳状态后能够对其进行提醒。另一部分为后台监管系统展示页面，由监管人员使用，系统能够展示驾驶员的个人信息和最近一段时间的驾驶数据。 5.2.2 数据库设计 数据库设计是确保数据一致性、安全性、可访问性、可扩展性和可维护性的重要手段，是构建高质量、高可靠和高可用的数据系统的必要步骤。驾驶员每次使用疲劳驾驶检测系统后，驾驶数据就会上传到服务器中的数据库中保存，数据采用关系型数据库Mysql存储。 通过5.1.1及5.1.2节的分析，以实体和属性间的关系为基础，进行数据库设计，并利用实体-关系图（E-R图）予以描述，系统整体E-R图具备如图5.4所示的特征。  图5.4 系统整体E-R图 结合数据库概念设计和实际的数据需求，设计了本系统数据库所需的数据库表，驾驶员  姓名性别年龄激活码授权码车 号手机号姓名驾驶员注 邮箱密码监管员  是否激活眨眼次数加密    监管员驾驶数据开始时间数据编号姓名结束时间驾驶员  哈欠次数点头次数持续驾驶时长疲劳驾驶次数管理产生查看驾驶员  姓名手机号nmnn1m 59 其中包含驾驶员信息、监管人员信息及驾驶数据信息三张数据库表。各数据库表详细内容如下： （1）驾驶员信息表：该表存储了驾驶员的基本信息，包含驾驶员id、姓名、性别、年龄、电话、车 号等信息，具体信息如表5.1所示。 表5.1 驾驶员信息表 序号 字段名 属性 数据类型 长度 非空 备注 1 driver_id 驾驶员id int 255 √ 主键、自增 2 name 姓名 varchar 20 √  3 gender 性别 tinyint 2 √  4 age 年龄 tinyint 3 √  5 phone_number 手机号 varchar 11 √  6 car_number 车 号 varchar 10 √  7 password 授权码 varchar 10 √  （2）监管人员信息表：该表存储了监管人员的基本信息，包含监管员id、姓名、注 邮箱、登录密码、密码加密salt、是否激活、激活码等信息，具体信息如表5.2所示。 表5.2 监管员信息表 序号 字段名 属性 数据类型 长度 非空 备注 1 admin_id 监管员id int 255 √ 主键、自增 2 name 姓名 varchar 255 √  3 email 注 邮箱 varchar 255 √  4 password 密码 varchar 20 √  5 salt 密码加密 varchar 255 √  6 valid 是否激活 tinyint 1   7 cdkey 激活码 varchar 255   （3）驾驶数据信息表：该表存储了驾驶员在使用疲劳驾驶检测系统期间产生的驾驶数据，包含驾驶员id、驾驶员姓名、驾驶开始时间、结束时间、眨眼次数、哈欠次数、点头次数、持续驾驶时长、疲劳驾驶次数等信息，具体信息如表5.3所示。 表5.3 驾驶数据信息表 序号 字段名 属性 数据类型 长度 非空 备注 1 id 数据编号 int 255 √ 主键、自增  60 2 driver_id 驾驶员id varchar 255 √  3 name 姓名 varchar 20 √  4 start_time 开始时间 datetime 0 √  5 end_time 结束时间 datetime 0 √  6 eye 眨眼次数 int 5   7 yawn 哈欠次数 int 5   8 nod 点头次数 int 5   9 duration 持续驾驶时长 time 0 √  10 fatigue_times 疲劳驾驶次数 int 5   5.2.3 功能模块设计 经过对第5.1.1节关于驾驶员疲劳驾驶检测系统的需求分析，可将该系统划分为五个模块：（1）登录模块，只有登录成功的驾驶员才可使用该系统，登录时驾驶员需要输入手机号及监管人员赋予的授权码登录；（2）图像采集模块，通过该模块采集驾驶员实时的驾驶状态；（3）疲劳参数设置模块，针对不同驾驶员可随时调整疲劳参数更改疲劳判定标准；（4）驾驶状态实时显示模块，将驾驶员的状态及产生的动作实时的显示在窗口；（5）疲劳预警模块，当驾驶员出现疲劳状态时，该模块可以及时提醒驾驶员休息，避免发生事故。具体的功能模块设计如图5.5所示。  图5.5 疲劳驾驶检测系统功能模块图 另外，通过对监管员使用的后台监管系统功能需求的分析，可将其分为四个模块：（1）注 登录模块，用于监管人员注 和登录；（2）驾驶员信息管理模块，该模块用于管理和存档驾驶员和车辆的相关信息；（3）驾驶员疲劳数据管理模块，该模块用于收集、管理和分析驾驶员疲劳数据；（4）疲劳数据可视化模块，能够使监疲劳驾驶检测系统图像采集模块疲劳参数设定模块疲劳预警模块驾驶状态实时显示模块登录模块 61 管人员更加直观的查看和分析驾驶员的驾驶数据。具体的模块设计如图5.6所示。  图5.6 后台监管系统功能模块图 5.3 疲劳驾驶检测系统实现 5.3.1 硬件设备选型 为了满足疲劳驾驶检测的实际应用场景，需要尽可能的使用小型设备对驾驶员进行检测。此外，疲劳驾驶检测系统主要采用实时视频帧作为输入信号，其中使用到的人脸检测模型以及疲劳特征提取模型包含相当复杂的算法计算，需要处理器具备优秀的并行计算能力。 表5.4 常用嵌入式开发板对比表 开发板 CPU 算力（INT8） 价格（元） Raspberry Pi 4B ARM Cortex-A72 1.5TOPS 1200 NVIDIA Jetson Nano ARM Cortex-A57 MPCore 7TOPS 1500 英特尔神经计算棒2代 Intel Movidius Myriad X VPU 4TOPS 1100 地平线旭日x3派 ARM Cortex-A53 5TOPS 500 Orange Pi 5 Rockchip RK3588s 6TOPS 600 针对以上问题，在实际应用中可以将检测模型部署在边缘计算平台进行检测，表5.4列举了目前几种常用的嵌入式开发板。 Orange Pi 5采用了瑞芯微RK3588s新一代八核64位处理器，拥有四核A76加后台监管系统驾驶员信息管理模块驾驶数据管理模块疲劳数据可视化模块登录模块 62 四核A55，采用了8nm工艺设计，主频最高可达2.4GHz，集成ARM Mali G610 MP4 GPU，内嵌高性能3D和2D图像加速模块，同时内置高达6Tops算力的AI加速器NPU，并可使用瑞芯微开发的推理框架RKNN来提高模型的推理速度。图5.7所示为Orange Pi 5核心板的外观。  图5.7 Orange Pi 5核心板外观 检测驾驶员疲劳状态需要实时的获取驾驶员的面部特征，因此，图像采集设备的选取至关重要。如图5.8所示，本系统采用了罗技c270高清摄像头作为图像采集设备，该摄像头使用CMOS感光元件，像素高达300万，支持最高1280×720分辨率拍照、720p/30fps录像。该摄像头工作温度范围为-30~85℃，可适应于各种常规环境下的应用。此外，该摄像头还采用了罗技Right Light技术，在昏暗光线下具有自动对焦，自动光线矫正等功能，在光照较弱的情况下依然有较好的成像效果，符合多场景下的疲劳驾驶检测。   图5.8 罗技c270摄像头外观 确定疲劳驾驶检测系统所使用的嵌入式开发板和摄像头之后，搭建了硬件测试平台，用于完成系统测试。如图5.9所示，该硬件平台由Orange Pi 5嵌入式开发板、 63 摄像头、显示器等设备构成。  图5.9 疲劳驾驶检测硬件平台 5.3.2 模型NPU部署 NPU是神经处理单元（Neural Processing Unit）的缩写。它是一种专门为执行神经网络计算而设计的芯片或处理器，可以大大提高神经网络的训练和推理速度，同时减少功耗和资源占用。传统的中央处理器（CPU）和图形处理器（GPU）通常并不擅长进行神经网络计算，因为这些计算需要大量的矩阵运算和向量运算，而这些任务并不是传统处理器的强项。相比之下，NPU专门为这些计算任务而设计，因此它们可以提供更高效的性能和更低的能耗。NPU被广泛用于各种机器学习和人工智能应用中，如图像识别、自然语言处理、语音识别等。本文使用的硬件设备Orange Pi 5采用的是瑞芯微RK3588s芯片，内置了高达6Tops算力的AI加速器NPU，可以满足绝大多数终端设备的边缘计算需求。 RKNN是瑞芯微NPU平台使用的模型类型，该类模型文件一般以.rknn为后缀。对于Tensorflow，Keras，Pytorch等框架构建的模型，想要直接在RK3588s平台运行，需要先进行模型转换，将其转换为RKNN模型。可以在搭载Ubuntu18.04以及以上版本的PC上使用RKNN-Toolkit2工具将模型转化为RKNN格式，再将其部署到开发板上，模型转换流程如图5.10所示，先将训练后生成的模型文件转为ONNX模型再通过RKNN-Toolkit2工具转为RKNN模型。  64  图5.10 模型转换流程图 RKNN-Toolkit2工具支持Caffe、Tensorflow、Tensorflow Lite、ONNX、DarkNet、Pytorch等模型转化为RKNN模型，并支持RKNN模型导入导出。根据瑞芯微的开发文档可知，RKNN-Toolkit2目前版本适用ubuntu18.04及以上，且该工具只能安装在X86架构的PC上，暂不支持Windows、MacOS、Debian等操作系统。因此，本文使用VMware虚拟机构建一个ubuntu20.04系统，在该系统上进行RKNN-Toolkit2工具的安装，具体安装流程如下： （1）在命令行中输入命令从Github拉取包含RKNN-Toolkit2的文件包。其文件夹内容如图5.11所示。   图5.11 RKNN-Toolkit文件夹内容 （2）安装RKNN-Toolkit2依赖。在VMware虚拟机中的ubuntu系统上安装Anaconda并建立一个python3.6的环境，在该环境中安装RKNN-Toolkit2所需要的依赖。 （3）进入packages文件夹下安装RKNN-Toolkit2。 （4）导入RKNN检查安装是否成功。如图5.12所示，导入RKNN后，若命令行无报错即为安装成功。    模型     模型     模型 65  图5.12 RKNN安装成功示意图 RKNN-Toolkit2安装完成后，就可以将ONNX模型转为RKNN模型，具体转换代码如下：  1. from rknn.api import RKNN  2. import os  3. if __name__    '__main__':  4.     platform   'rk3588'  5.   6.     '''第一步：创建r nn对象'''  7.     rknn   RKNN()  8.   9.     '''第二步: 加载onn 模型''' 10.     rknn.config(target_platform 'rk3588') 11.     print('--  Loading model') 12.     ret   rknn.load_onnx('actor_simple.onnx') 13.     if ret !  0: 14.         print('load model failed') 15.         exit(ret) 16.     print('done') 17.  18.     '''第三步: 建立模型''' 19.     print('-- Building model') 20.     ret   rknn.build(do_quantization False) 21.     if ret !  0: 22.         print('build model failed') 23.         exit() 24.     print('done') 25.  26.     '''第四步: 导出并保存r nn模型'''  66 27.     OUT_DIR   'rknn_models' 28.     RKNN_MODEL_PATH   './{}/actor_simple.rknn'.format(OUT_DIR) 29.     if not os.path.exists(OUT_DIR): 30.         os.mkdir(OUT_DIR) 31.     print('--  Export RKNN model: {}'.format(RKNN_MODEL_PATH)) 32.     ret   rknn.export_rknn(RKNN_MODEL_PATH) 33.     if ret !  0: 34.         print('Export rknn model failed.') 35.         exit(ret) 36.     print('done') 37. 38.     '''第五步: r nn对象释放''' 39.     rknn.release() 将ONNX模型转换得到RKNN模型后，即可在开发板进行推理使用，RKNN模型的推理需要在开发板上安装RKNN-Toolkit2-Lite工具，再使用Python脚本在开发板端进行推理，图5.13所示为在Orange Pi 5开发板上的推理效果图。  图5.13 Orange Pi 5硬件平台推理效果图 5.3.3 系统功能实现 疲劳驾驶检测系统需要的开发环境及工具如表5.5所示。  67 表5.5 疲劳驾驶检测系统开发环境及工具表 名称 参数 操作系统 Ubuntu20.04 处理器 Rockchip RK3588s 内存 8G 显卡 Mali G610 集成开发环境 VSCode 开发语言 Python 工具包 Opencv、Pyqt5、Numpy 本文在ubuntu20.04系统上安装VSCode进行开发，使用PyQt5构建了疲劳驾驶检测系统的图形用户界面。同时，借助OpenCV计算机视觉库进行图像处理，并使用人脸检测和人脸关键点检测模型获得驾驶员的面部特征。最后，根据疲劳判定方法对这些特征进行分析，以实现系统的疲劳检测功能。系统的具体功能实现如下： （1）登录界面 为了确保疲劳驾驶检测系统的数据信息的准确性和安全性，使监管人员能够获得对应驾驶员的驾驶信息，需要使用驾驶员登录模块。登录页面如图5.14所示。  图5.14 登录页面示意图 使用疲劳驾驶检测系统时需要先访问登录页面，登录表单包括两个字段：手机号码及授权码。服务器后台首先将检查该号码是否在后台监管系统中与某个驾驶员 68 相关联。其次，检查该驾驶员是否已被监管人员授予授权码，并验证驾驶员输入的授权码是否和疲劳监管系统中驾驶员的授权码一致，若不一致则登录失败。 （2）疲劳驾驶检测系统主界面 登录成功后会跳转到疲劳驾驶检测系统的主界面，如图5.15所示。  图5.15 疲劳驾驶检测系统主界面 界面左侧为视频输入源和驾驶员状态输出模块，右侧为参数设置模块。驾驶员可设置输入源为摄像头输入和视频输入，可点击按钮开始或暂停疲劳驾驶检测。此外，在界面右侧，疲劳驾驶检测系统的疲劳判定标准模块是一个重要组成部分，驾驶员可根据情况自行调整使用的判定标准。 5.4 后台监管系统实现 5.4.1 开发环境配置 开发后台监管系统可以帮助交通管理部门对驾驶员进行监督和管理。通过监管系统获取的驾驶员行驶数据可以对驾驶员的行车情况进行监管和评估，及时发现存在长期疲劳驾驶行为的驾驶员，并对其进行警告或限制其驾驶资格，从而保障驾驶员的生命安全。 考虑到应用场景，监管系统采用B/S架构[68]进行开发，具体的开发环境及工具如表5.6所示。  69 表5.6 后台监管系统开发环境及工具表 名称 参数 操作系统 Windows11 处理器 Intel i7 11800h 内存 32G 显卡 GTX3070 集成开发环境 IntelliJ IDEA 数据库 Mysql5.7.27 开发语言 Java 框架 Spring Boot、Mybatis 5.4.2 系统功能实现 后台监管系统不同于疲劳驾驶检测系统，该系统是由监管人员使用的，监管人员能够通过该系统查看驾驶员的驾驶数据，该系统包括注 模块、登录模块、驾驶员信息管理模块、驾驶员驾驶数据管理模块、疲劳数据可视化模块。 （1）注 模块 注 模块是后台监管系统的重要模块之一，图5.16为后台监管系统的注 页面。疲劳驾驶监管人员需要在该页面中注 后才能使用该系统，注 时需要填入监管员的邮箱及密码。Web后端中会先判断该邮箱是否已经注 ，若注 过则提示监管人员“邮箱已注 ”；若没有注 过则会向监管人员的邮箱发送一封激活邮件，如图5.17所示，监管人员点击邮箱中的链接激活后才能正常登录。  图5.16 后台监管系统注 界面  70  图5.17 激活邮件示意图 （2）登录模块 如图5.18所示，该界面是监管人员使用后台监管系统的入口。监管人员可以通过该模块登录系统并使用系统中的各种功能。登录模块也是确保系统安全性的重要一环，监管人员需要在Web前端输入登录邮箱和登录密码，在Web后端中会验证登录邮箱及后端加密后的密码是否与Mysql数据库中保存的一致，若一致则登录成功，否则登录失败。通过该模块，防止了非法用户恶意登录，保障了该监管系统的数据安全。  图5.18 后台监管系统登录界面 （3）驾驶员信息管理模块 如图5.19所示，监管人员可以通过该页面来管理驾驶员个人信息，所有被允许使用疲劳驾驶检测系统的驾驶员都需要将个人信息添加到该模块，监管人员可以在该模块中添加、删除、修改、查看驾驶员的信息。  71  图5.19 驾驶员信息管理界面 （4）驾驶数据管理模块 如图5.20所示，驾驶数据管理模块是监管人员用来查看驾驶员行驶数据的模块，包括驾驶员姓名、驾驶时间、驾驶员是否产生疲劳等信息。通过对这些数据的分析，疲劳监管人员可以判断长期出现疲劳驾驶的驾驶员，可根据该模块中的驾驶员ID在驾驶员信息管理模块进行查询，找到该驾驶员的联系方式对其进行警告提醒。  图5.20 驾驶数据管理界面 （5）疲劳数据可视化模块 疲劳数据可视化模块是用来将驾驶员的驾驶数据进行可视化的模块。如图5.21所示，该模块将驾驶员在驾驶过程中出现的疲劳次数转换为柱状图、折线图等图形化的表达方式，方便疲劳驾驶监管人员更直观地了解驾驶员的疲劳状态，更及时地采取措施保障交通安全。  72  图5.21 疲劳数据可视化界面 5.5 系统测试与分析 软件测试的目的是在软件正式投入使用之前尽可能地发现软件存在的问题，确保该软件能够在真实的场景下可靠地工作，因此本节内容主要是对疲劳驾驶检测系统和疲劳驾驶监管系统分别进行功能性测试及非功能性测试实验来验证其可靠性。 5.5.1 测试环境 测试前需要先将疲劳驾驶检测系统和疲劳驾驶监管系统分别部署到Orange Pi 5嵌入式开发板端及服务器端。对疲劳驾驶检测系统的测试在开发板端进行，对后台监管系统的测试在PC端进行，环境的软硬件配置如表5.7所示。 表5.7 测试环境配置表 测试环境 硬件配置 软件配置 Orange Pi 5开发板端 CPU：8核 内存：8GB 硬盘：512GB 操作系统：Ubuntu20.04 开发工具：VSCode PC端 CPU：8核 内存：32GB 硬盘：1T 操作系统：Windows11 浏览器：Chrome、Edge 测试工具：Postman 服务器端 CPU：2核 内存：4GB 硬盘：50GB 操作系统：CentOS7.6 数据库：MySQL8.0 Web服务器：Tomcat  73 5.5.2 功能测试 （1）疲劳驾驶检测系统的功能测试 疲劳驾驶检测系统主要包括6个功能需求，分别是：用户登录、面部图像采集、疲劳阈值设定、疲劳状态分析判定、状态实时输出、疲劳预警、驾驶数据上传。分别对以上功能进行测试，测试结果如表5.8所示。 表5.8 疲劳驾驶检测系统功能测试表 用户登录功能测试 操作描述 1. 在登录界面输入驾驶员手机号及授权码 2. 在系统主界面点击“登录” 3. 弹出登录成功提示框后，点击“确认” 预期输出 弹出登录成功提示框，点击确定后，跳转到系统主界面 实际输出 与预期结果相符 测试结果 测试通过 面部图像采集功能测试 操作描述 在系统主界面点击“开始检测” 预期输出 在窗口中能实时看到驾驶员脸部状态 实际输出 与预期结果相符 测试结果 测试通过 疲劳阈值设定功能测试 操作描述 1. 在系统主界面修改眼部疲劳阈值 2. 在系统主界面修改嘴部疲劳阈值 3. 在系统主界面修改头部疲劳阈值 4. 点击开始检测 预期输出 能够正常检测出疲劳状态 实际输出 与预期结果相符 测试结果 测试通过 驾驶状态实时显示、预警功能测试 操作描述 在系统主界面点击“开始检测” 预期输出 系统主界面左下方状态输出框能够正常输出驾驶状态 实际输出 与预期结果相符 测试结果 测试通过  74 驾驶状态实时显示、预警功能测试 操作描述 在系统主界面点击“开始检测” 预期输出 系统主界面左下方的状态输出框能够正常输出驾驶状态 实际输出 与预期结果相符 测试结果 测试通过 驾驶数据上传功能测试 操作描述 在系统主界面点击“结束”按钮 预期输出 点击结束按钮后会将驾驶员驾驶期间的眨眼次数、哈欠次数、点头次数、疲劳次数等信息上传服务器数据库，在数据库中能够看到新插入的一条数据 实际输出 与预期结果相符 测试结果 测试通过 通过上述对疲劳驾驶检测系统的功能测试结果可知，各个功能模块的功能均己达到预期效果。系统能够正确验证驾驶员，保障系统的数据安全，此外系统能够正常实现疲劳驾驶检测、疲劳预警及驾驶数据上传等功能，符合预期设计需求。 （2）后台监管系统的功能测试 后台监管系统主要包含5个功能需求，分别是：注 功能、登录功能、驾驶员信息管理功能、疲劳数据管理功能、疲劳数据可视化功能，分别对上述功能进行测试，测试结果如表5.9所示。 表5.9 后台监管系统功能测试表 注 功能测试 操作描述 1. 在注 界面输入注 邮箱及密码点击“注 ”按钮 2. 点击激活邮件中的激活链接 预期输出 注 账号后邮箱会收到一封激活邮件，点击激活链接后会跳转至激活成功页面 实际输出 与预期结果相符 测试结果 测试通过 登录功能测试 操作描述 进入登录页面，输入邮箱及密码，点击“登录” 预期输出 跳转至疲劳后台监管系统主界面 实际输出 与预期结果相符  75 测试结果 测试通过 驾驶员信息管理功能测试 操作描述 1. 在主界面左侧的菜单栏点击“驾驶员信息” 2. 驾驶员信息显示界面点击“Add”按钮，填入驾驶员信息并保存 3. 在驾驶员信息显示界面点击“删除” 4. 在驾驶员信息显示界面点击“修改”修改字段信息 预期输出 1. 在系统界面显示已获得授权码的驾驶员信息 2. 刷新页面后点击“驾驶员信息”能看到新添加的驾驶员信息 3. 刷新页面后点击“驾驶员信息”显示该条驾驶员信息已删除 4. 刷新页面后点击“驾驶员信息”能看到修改后的驾驶员信息 实际输出 与预期结果相符 测试结果 测试通过 疲劳数据管理功能测试 操作描述 在系统主界面左侧的菜单栏点击“驾驶数据管理” 预期输出 在系统界面显示不同驾驶员的驾驶数据 实际输出 与预期结果相符 测试结果 测试通过 疲劳数据可视化功能测试 操作描述 在主界面左侧的菜单栏点击“疲劳数据可视化” 预期输出 在系统界面显示条形图、折线图等疲劳数据图表 实际输出 与预期结果相符 测试结果 测试通过 5.5.3 非功能测试 与系统功能测试不同，系统非功能测试主要关注软件系统在使用过程中的安全性、可用性、兼容性等方面的效果。旨在帮助开发人员和测试人员充分了解软件系统的综合性能表现。为了确保本文的疲劳驾驶检测及监管系统能够在实际应用场景下稳定运行。分别对其进行可用性、安全性、兼容性等方面的测试。 （1）可用性测试 可用性测试是一种评估软件易用性和用户体验的测试方法，它的目的是确保软件能够满足用户需求并提供良好的用户体验。本文系统的可用性测试结果如表5.10所示。根据测试，本文中的疲劳驾驶检测系统及疲劳驾驶监管系统界面设计符合用 76 户习惯、操作流程直观、系统的反应时间较快，具有良好的可用性。 表5.10 系统可用性测试表 页面导航测试 测试方法 1. 观察导航菜单项是否明确、易于查找和操作 2. 观察导航菜单项的布局和结构是否合理且易于理解 3. 观察导航菜单项的标签和图标是否清晰且易于辨认 实际效果 系统导航功能醒目直观、易于理解和使用，符合用户需求 测试结果 测试通过 页面路由测试 测试方法 1. 观察页面路由的响应时间，是否能够快速跳转到预期页面 2. 观察页面路由跳转是否准确，是否出现错误或混乱情况 实际效果 系统的路由响应速度快，能够及时跳转到正确的页面 按钮控件测试 测试方法 1. 测试按钮的布局、字体、大小和颜色等视觉效果是否美观 2. 测试按钮的响应时间、点击后的反馈和跳转是否正常 实际效果 按钮图标大小适中，布局美观，响应速度快，能够正确跳转 测试结果 测试通过 可视化图表测试 测试方法 检查页面渲染的图表能否正常显示、是否出现比例或缩放不准确等问题 实际效果 图表数据显示准确，比例协调美观 测试结果 测试通过 （2）安全性测试 系统安全性测试可以提前发现软件中存在的潜在漏洞和安全风险，保障软件不受到黑客攻击和数据泄露，维护软件系统的稳定性和安全性。本文中针对系统安全性的测试结果如表5.11所示。根据测试，未发现软件中存在的明显漏洞及安全问题。 表5.11 系统安全性测试表 账号安全测试 测试方法 使用Navicat Premium 16在数据库中查看用户密码 实际效果 数据库中的密码为一串经MD5加密后的乱码。 测试结果 测试通过  77 访问权限测试 测试方法 1. 登录时输入空白或错误的账号和密码 2. 在监管系统中使用未激活的账号登录 实际效果 1. 系统提示登录失败 2. 提示“未激活账号无法正常登录” 3. 访问失败，无法通过URL直接访问 测试结果 测试通过 （3）系统兼容性测试 系统兼容性测试旨在验证系统在不同操作系统、浏览器、设备和网络环境下的性能表现和兼容性情况。通过这些测试，可以确保系统能够在各种环境和平台下正常运行和提供良好的用户体验，从而提高系统的使用效率和用户满意度。 经过对疲劳驾驶检测及监管系统的兼容性测试，系统在不同操作系统（如Windows，Ubuntu）下均能正常运行，在不同浏览器（如Chrome，Firefox，Edge）下也能良好运行，同时在多种网络环境（如4G、5G、WIFI）下也没有出现兼容性问题。测试结果表明，本文系统能够较好地适应多种平台，保证用户体验。 5.6 本章小结 本章设计并实现了疲劳驾驶检测及监管系统。首先，根据系统的需求分析，构建系统总体架构，并完成数据库表和各功能模块设计。其次，使用Orange Pi 5嵌入式开发板进行模型部署，并基于Pyqt5进行疲劳驾驶检测系统开发。随后，使用SpringBoot进行后台监管系统开发。最后，经过全面的系统功能性测试和非功能性测试，系统各功能模块均能够正常运行，在可靠性、安全性、兼容性等方面也表现良好，能够较好地满足用户需求。     78 第六章  总结与展望 6.1 论文总结 疲劳驾驶是一种普遍存在的危险驾驶行为，对人民的生命安全产生严重威胁，因此，疲劳驾驶检测技术的研究与应用具有重要意义，本文针对现有检测技术存在的问题提出了一种基于面部多特征融合的疲劳驾驶检测方法，并设计实现了疲劳驾驶检测系统，详细的工作总结如下： （1）首先介绍了疲劳驾驶检测的研究背景及意义，总结归纳了目前国内外普遍采用的三类疲劳驾驶检测方法，并针对这三类检测方法进行了对比分析，为了满足实际使用需求，最终选择采用无接触、精度高的基于驾驶员面部特征的检测方法展开研究。 （2）设计了一种YOLOv5s结合无参注意力机制的人脸检测模型。首先，对比了目前存在的人脸检测方法，综合分析模型大小及检测精度等指标后，采用目前工程领域常用的目标检测模型YOLOv5s作为基础研究算法。然后，为了使模型更加关注关键信息，加强对人脸区域的特征提取，将无参注意力机制SimAM与YOLOv5s相结合，优化模型结构。最后，通过对比实验可得，改进后的模型在参数量不变的情况下，检测精度相较原模型提高了2.1%。 （3）设计了一种改进PFLD的人脸关键点检测模型。首先，对比目前常用的人脸关键点检测方法后，选择采用PFLD关键点检测模型作为基础研究模型。然后，在原始模型的基础上进一步增加了多尺度特征融合，提高模型的检测精度。同时，为了提升模型的推理速度，降低模型的参数量，将PFLD主干网络分支中采用的Bottleneck模块替换为更加轻量化的Ghost Bottleneck。最后，根据实验结果可知，改进后的PFLD人脸关键点检测模型精度及检测速度均有提升。 （4）研究了一种基于驾驶员面部多特征融合的疲劳驾驶检测方法。首先，通过阈值设定实验选择能够准确提取特征的阈值。然后，将眼部、嘴部、头部的疲劳判定方法融合，设计出一种基于面部多特征融合的疲劳判定方法。最后，对该方法使用YAWDD数据集进行了测试，测试结果表明该判定方法准确率高，具有一定的研究意义。 （5）设计并实现了疲劳驾驶检测及监管系统。为了能将基于面部多特征融合的疲劳驾驶检测方法应用到实际场景中，将训练后的算法模型部署至Orange Pi 5并进行NPU加速推理，在驾驶员端能够完成实时的疲劳驾驶检测。此外，使用Spring  79 Boot开发了后台监管系统，能够将驾驶数据可视化，直观的反映驾驶员的驾驶状态，对出现疲劳驾驶的驾驶员进行警告。 6.2 未来展望 本文通过对算法的深入研究，成功开发了一种基于面部多特征融合的疲劳驾驶检测系统，并在疲劳驾驶检测方面取得了显著效果。然而，鉴于本研究存在时间和资源等方面的限制，该系统仍然存在一些不足之处，需要进一步完善。主要的改进方向包括以下几个方面： （1）虽然本文采用的人脸检测模型属于轻量级模型，但是模型的主干网络部分参数量仍然较大。在后期的研究中，我们可以对模型的主干网络进行优化，进一步降低模型的参数量，提高模型的检测速度。 （2）在实际应用中，除疲劳驾驶检测外，还可加入驾驶员分心检测，更能降低交通事故的发生率。例如，可以利用车载摄像头和智能算法检测驾驶员的目光是否离开了道路，是否频繁地看手机等行为，及时提醒驾驶员，确保驾驶员的安全驾驶。 （3）本文开发的后台监管系统的功能较少，只包括了驾驶员信息管理，疲劳数据可视化，疲劳数据管理等功能。在后期的研究中，可以在后台监管系统中添加更多的功能，如行车记录、车辆状态监测等，使得后台监管系统更加全面、实用，提高车辆的安全性和可靠性。 （4）考虑到驾驶室的空间较小，本研究采用体积小的嵌入式开发板作为检测设备。后续可以将系统做成手机应用程序，驾驶员可以随时使用手机进行疲劳驾驶检测，不需要另外携带设备，提高了使用的便捷性，更加方便驾驶员操作。    80 参考文献 [1] 吕东缙. 造成交通事故的因素分析与预防措施[J]. 交通世界, 2015(10): 18-19 [2] 张瑞, 朱天军, 邹志亮, 等. 驾驶员疲劳驾驶检测方法研究综述[J]. 计算机工程与应用, 2022, 58(21): 53-66. [3] 郑伟成, 李学伟, 刘宏哲, 等. 基于深度学习的疲劳驾驶检测算法[J]. 计算机工程, 2020, 46(07): 21-29. [4] 范沁红, 江星辰, 杨刚俊, 等. 驾驶操作疲劳风险评价测试方法及趋势研究[J]. 太原理工大学学报, 2021, 52(04): 9. [5] Katsis C D, Exarchos T P, Papaloukas C, et al. A two-stage method for MUAP classification based on EMG decomposition[J]. Computers in Biology& Medicine, 2007, 37(9): 1232-1240. [6] Jung S, Shin H, Chung W. Driver fatigue and drowsiness monitoring system with embedded electrocardiogram sensor on steering wheel[J]. IET Intelligent Transport Systems, 2014, 8(1): 43-50 [7] Ma, Jia-Xin, Shi, et al. An EOG-based Vigilance Estimation Method Applied for Driver Fatigue Detection[J]. Neuro science & Biomedical Engineering, 2014, 2(1): 41-51. [8] Zhang Z, Luo D, Rasim Y, et al. A vehicle active safety model: vehicle speed control based on driver vigilance detection using wearable EEG and sparse representation[J]. Sensors, 2016, 16(2): 242. [9] 祝荣欣, 王金武. 基于心肌电的联合收获机驾驶人疲劳检测研究[J]. 农机化研究, 2020, 42(02): 8-14+43. [10] 蔡素贤, 杜超坎, 周思毅, 等. 基于车辆运行数据的疲劳驾驶状态检测[J]. 交通运输系统工程与信息, 2020, 20(04): 77-82. [11] Sandberg D, Wahde M. Particle swarm optimization of feedforward neural networks for the detection of drowsy driving[C]//2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence). IEEE, 2008: 788-793. [12] Wang, H L, et al. Fatigue Driving Detection System Design Based on Driving Behavior[C]//2010 International Conference on Optoelectronics & Image  81 Processing. IEEE, 2010:549-552. [13] Wang M S, et al. Drowsy behavior detection based on Driving information[J]. International Journal of Automotive Technology, 2016, 17(2016): 165-173. [14] Zhang F, Dai F. Research and Application of Fatigue Driving Monitoring and Emergency Treatment[C]//2017 International Conference on Computer Technology, Electronics and Communication. IEEE, 2017: 1072-1075 [15] Wang K, Ma Y, Huang J, et al. Driving Performance of Heavy-Duty Truck Drivers under Different Fatigue Levels at Signalized Intersections[C]//19th COTA International Conference of Transportation, 2019: 581-592. [16] 张峰, 郭刚, 严平, 等. 基于面部多特征融合的实时疲劳驾驶检测方法[C]//国际应用科学与工程协会. 2021第四届算法, 计算与人工智能国际会议. ACM, 2021: 2021-367. [17] Grace R, Byrne V E, Bierman D M, et al. A drowsy driver detection system for heavy vehicles[C]//17th Digital Avionics Systems Conference, IEEE, 1998. [18] Owens J, Dingus T, Guo F, et al. Estimating the Prevalence and Crash Risk of Drowsy Driving Using Data from a Large-Scale Naturalistic Driving Study[C]//Transportation Research Board 97th Annual Meeting, 2018. [19] Zandi A S, Quddus A, Prest L, et al. Non-Intrusive Detection of Drowsy Driving Based on Eye Tracking Data[J]. Transportation Research Board, 2019, 2673(6): 247-257. [20] 严陈. 人脸检测算法研究及验证[D]. 扬州大学, 2019. [21] Dadi H S, Pillutla G. Improved Face Recognition Rate Using HOG Features and SVM Classifier[J]. Iosr Journal of Electronics & Communication Engineering, 2016, 11(4):34-44. [22] Li H, Lin Z, Shen X, et al. A Convolutional Neural Network Cascade for Face Detection[C]//IEEE Conference on Computer Vision & Pattern Recognition. IEEE, 2015: 5325-5334. [23] Viola P A, Jones M J. Robust Real-Time Face Detection[J]. International Journal of Computer Vision, 2004, 57(2): 137-154. [24] Jiang H Z, Learned-Miller E. Face detection with the faster R-CNN[C]//12th IEEE International Conference on Automatic Face &amp; Gesture Recognition. IEEE, 2017: 650-657.  82 [25] Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-time Object Detection[C]//IEEE Conference on Computer Visionand Pattern Recognition. IEEE, 2016: 779-788. [26] Liu W, Anguelov D, Erhan D, et al. SSD: Single shot Multi Box detector[C]//14th European Conference on Computer Vision. Springer, 2016: 21-37. [27] Deng J, Guo J, Zhou Y, et al. RetinaFace: single-stage dense face localisation in the wild[J/OL]. arXiv: 1905.00641, 2019. [28] Mohiyuddin A, Basharat A, Ghani U, et al. Breast tumor detection and classification in mammogram images using modified YOLOv5 network[J]. Computational and Mathematical Methods in Medicine, 2022, 2022: 1-16. [29] Wang C, Liao H, et al. CSPNet: A New Backbone that can Enhance Learning Capability of CNN[C]//IEEE Conference on Computer Vision and Pattern Recognition Workshops. IEEE, 2020:390-391. [30] He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37(9): 1904-1916. [31] Lin T Y, Dollár P, Girshick R, et al. Feature Pyramid Networks for Object Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2117-2125. [32] Liu S, Qi L, Qin H, et al. Path Aggregation Network for Instance Segmentation[C]//IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8759-8768. [33] 李砚峰, 刘翠荣, 吴志生, 等. 基于深度学习One-stage方法的焊缝缺陷智能识别研究[J]. 广西大学学报(自然科学版), 2021, 46(02): 362-372. [34] Niu Z, Zhong G, Yu H. A review on the attention mechanism of deep learning[J]. Neurocomputing, 2021, 452: 48-62. [35] Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//European Conference on Computer Vision. 2018: 3-19. [36] Hu J, Shen L, Sun G. Squeeze-and-Excitation Networks[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2018: 7132-7141. [37] Yang L, Zhang R, Li L, et al. SimAM: A simple, Parameter-Free Attention Module for Convolutional Neural Networks[C]//International Conference on  83 Machine Learning. PMLR, 2021: 11863-11874. [38] 孙俊楠. 基于深度学习的阿尔茨海默症图像分类算法研究[D]. 江南大学, 2022. [39] Yang S, Luo P, Loy C, et al. WIDER FACE: A Face Detection Benchmark[C]// IEEE Conference on Computer Vision & Pattern Recognition. IEEE, 2016:5525-5533. [40] 李小平, 白超.基于深度学习的司机疲劳驾驶检测方法研究[J]. 铁道学报, 2021, 43(06): 78-87. [41] Edwards G J, et al. Face Recognition Using Active Appearance Models[C]//5th European Conference on Computer Vision. Springer 1998: 581-595. [42] Cootes T F, Taylor C J, Cooper D H, et al. Active shape models—their training and application[J]. Computer Vision and Image Understanding, 1995, 61(1): 38-59. [43] Saragih J M, Lucey S, Cohn J F. Deformable model fitting by regularized landmark mean-shift[J]. International Journal of Computer Vision, 2011, 91: 200-215. [44] Papandreou G, Maragos P. Adaptive and Constrained Algorithms for Inverse Compositional Active Appearance Model Fitting[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2008: 1-8. [45] Dollár P, Welinder P, Perona P. Cascaded Pose Regression[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010: 1078-1085. [46] Liang L, Wen F, et al. Accurate Face Alignment Using Shape Constrained Markov network[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2006, 1: 1313-1319. [47] Guo X, Li S, Yu J, et al. PFLD: A Practical Facial Landmark Detector[J/OL]. arXiv: 1902.10859, 2019. [48] Sandler M, Howard A, Zhu M, et al. Mobilenetv2: Inverted Residuals and Linear Bottlenecks[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2018: 4510-4520. [49] Zhu L, Geng X, Li Z, et al. Improving YOLOv5 with attention mechanism for detecting boulders from planetary images[J]. Remote Sensing, 2021, 13(18):  84 3776. [50] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2016: 770-778. [51] Han K, Wang Y, Tian Q, et al. Ghostnet: More features from cheap operations[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2020: 1580-1589. [52] Bulat A, Tzimiropoulos G. Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources[C]//IEEE International Conference on Computer Vision. IEEE, 2017: 3706-3714. [53] Sagonas C, Antonakos E, Tzimiropoulos G, et al. 300 faces in-the-wild challenge: Database and results[J]. Image and vision computing, 2016, 47: 3-18. [54] Wu W, Qian C, Yang S, et al. Look at boundary: A Boundary-aware Face Alignment Algorithm[C]//IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2129-2138. [55] Akbar S, Peikari M, Salama S, et al. The transition module: a method for preventing overfitting in convolutional neural networks[J]. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 2019, 7(3): 260-265. [56] 景芙蓉. 基于深度学习的疲劳驾驶检测研究与实现[D]. 电子科技大学,2021. [57] Yang S, Song X, Zhang L, et al. The anti-fatigue driving system design based on the eye blink detect[C]//Seventh International Conference on Electronics and Information Engineering. SPIE, 2017, 10322: 406-410. [58] Soares G, de Lima D, Neto A M. A mobile application for driver's drowsiness monitoring based on PERCLOS estimation[J]. IEEE Latin America Transactions, 2019, 17(02): 193-202. [59] Soukupova T, Cech J. Eye Blink Detection Using Facial Landmarks[C]//21st Computer Vision Winter Workshop. 2016: 2. [60] 李庆臣. 基于面部特征的疲劳驾驶检测系统设计[D]. 郑州大学, 2019. [61] Hanowski R. J, Bowman D, Alden A, et al. PERCLOS+: Development of a Robust Field Measure of Driver Drowsiness[C]//15th World Congress on Intelligent Transport Systems and ITS America’s 2008 Annual Meeting. 2008:1- 85 13 [62] Abtahi S, Omidyeganeh M, Shirmohammadi S, et al. YawDD: A Yawning Detection Dataset[C]//5th ACM multimedia systems conference. 2014: 24-28. [63] 张文影. 基于面部和心率特征融合的驾驶员疲劳状态识别方法研究[D]. 华南理工大学, 2020. [64] 杜永昂. 基于视频的疲劳驾驶检测方法研究[D]. 华北电力大学, 2021. [65] 闵秋莎, 刘能, 陈雅婷, 基于面部特征点定位的头部姿态估计[J]. 计算机工程, 2018, 44(06): 263-269. [66] 武君. 基于单目视觉的头部姿态估计算法的研究与实现[D]. 北方工业大学, 2017. [67] Horrey W J, Wickens C D. In-Vehicle Glance Duration: Distributions, Tails, and Model of Crash Risk[J]. Transportation Research Record, 2007, 2018(1): 22–28. [68] Dang G Q, Cheng X Y. The Research of Embedded Remote Monitoring System Based on B/S Framework[J]. Applied Mechanics & Materials, 2015, 713-715:508-511. 